import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r,o as m,c as h,d as p,a as e,b as s,e as a,w as n,f as l}from"./app-2a2d189a.js";const c="/assets/algorithm1-ea699695.png",u="/assets/vamana-91e258d6.png",d="/assets/algorithm2-88fb369e.png",g="/assets/algorithm3-b3dfb02a.png",f="/assets/figure1-1aa6d79e.png",w="/assets/figure2-185348ae.png",b="/assets/figure3-9f0a4549.png",v={},y=e("h1",{id:"diskann",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#diskann","aria-hidden":"true"},"#"),s(" DiskANN")],-1),x=e("p",null,"DiskANN",-1),S={class:"hint-container info"},k=e("p",{class:"hint-container-title"},"相关信息",-1),N=e("li",null,[e("p",null,"可能这里要有一些 关于 其它的对于这篇文章的解读")],-1),_=e("li",null,[e("p",null,"我们需要一些链接？")],-1),G=e("li",null,[e("p",null,"paper:")],-1),M={href:"https://github.com/microsoft/DiskANN",target:"_blank",rel:"noopener noreferrer"},D=e("li",null,[e("p",null,"blogs：")],-1),A={href:"https://www.zhihu.com/search?type=content&q=diskANN",target:"_blank",rel:"noopener noreferrer"},V={href:"https://blog.csdn.net/whenever5225/article/details/106863674",target:"_blank",rel:"noopener noreferrer"},z={href:"https://blog.csdn.net/weixin_44839084/article/details/119217569",target:"_blank",rel:"noopener noreferrer"},P={href:"https://blog.csdn.net/weixin_44839084/article/details/129679691",target:"_blank",rel:"noopener noreferrer"},q={class:"table-of-contents"},I=l('<h2 id="no-0-abstract" tabindex="-1"><a class="header-anchor" href="#no-0-abstract" aria-hidden="true">#</a> No.0 Abstract</h2><ul><li><mark>总结</mark></li><li>“64GB RAM”通常指的是 64GB 的随机存取存储器（Random Access Memory），也就是计算机的运行内存。运行内存用于暂时存储正在运行的程序和数据，以便 CPU 能够快速访问和处理。</li><li></li><li><mark>原文翻译</mark></li><li>当前最先进(state-of-the-art)的近似最近邻搜索（ANNS）算法生成的索引必须<mark>存储在主内存中</mark>，以便进行快速、高召回率的搜索。这使得它们开销高昂并限制了数据集的大小。我们提出了一个新的基于图的索引和搜索系统，名为DiskANN，它能够在一台只有64GB RAM和一块便宜的固态硬盘（SSD）的单个工作站上给十亿点的数据库建索引、存储和搜索。与当前的智慧相反，我们证明了DiskANN构建的基于SSD的索引能够满足大规模ANNS的全部三个愿望：高召回率、低查询延迟和高密度（每个节点索引的点）。在十亿点的SIFT1B bigann数据集上，DiskANN能够在一台16核机器上(QPS)每秒服务超过5000个查询，平均延迟&lt;3ms，并且保持95%以上的1-recall@1，而使用类似内存占用的最先进十亿点ANNS算法，如FAISS [18] 和 IVFOADC+G+P [8] 在1-recall@1的准确率约为50%。另外，在高召回率模式下，DiskANN可以比如HNSW [21] 和 NSG [13]这样的最先进图形基方法在每个节点上索引和服务5至10倍更多的点。最后，作为我们整体DiskANN系统的一部分，我们引入了Vamana，这是一种新的基于图的ANNS索引，即使对于内存中的索引来说，它也比现有的图索引更加通用。</li></ul><h2 id="no-1-introduction" tabindex="-1"><a class="header-anchor" href="#no-1-introduction" aria-hidden="true">#</a> No.1 Introduction</h2><ul><li><p><mark>原文翻译</mark></p></li><li><p>在最近邻搜索问题中，给出了一个特定空间中点的数据集P。目标是<mark>设计一个小尺寸的数据结构</mark>，这样，对于相同度量空间中的任何查询q和目标k，我们就可以快速地从数据集P中检索到q的k个最近邻。这是算法研究中的一个基本问题，也是在计算机视觉、文档检索和推荐系统等不同领域中常用的( sub-routine)子例程，等等。在这些应用中，实际的实体——图像、文档、用户档案——被嵌入到百维或千维空间中，以便所需的实体相似性概念被编码为它们嵌入的距离。</p></li><li><p>In the nearest neighbor search problem, we are given a dataset <em>P</em> of points in some space. The goal is to design a data structure of small size, such that, for any query <em>q</em> in the same metric space, and target <em>k</em>, we can retrieve the <em>k</em> nearest neighbors of <em>q</em> from the dataset <em>P</em> quickly.</p><ul><li>a small size data structure, for query q and retrieve knn quickly.</li></ul></li><li><p>This is a fundamental problem in algorithms research, and also a commonly used sub-routine in a diverse set of areas such as computer vision, document retrieval and recommendation systems, to name a few. In these applications, the actual entities — images, documents, user profiles — are <em>embedded</em> into a hundred or thousand dimensional space such that a desired notion of the entities’ similarity is encoded as distance between their embeddings.</p><ul><li>be embedded a hundred or thousand dimensional space;</li></ul></li><li><p>不幸的是，由于所谓的维度诅咒现象，通常不可能在不进行数据的线性扫描的情况下检索到精确的最近邻（例如，见[15, 23]）。因此，人们转而寻找近似最近邻（ANN），其目标是检索到接近最优的k个邻居。更正式地，假设有一个查询q，并且假设算法输出了一组k个候选近邻X，假设G是基础数据集中q的k个最近邻的真实集合(ground-truth)。那么，我们定义这个集合X的k-recall@k为|X∩G|/k。那么，ANN算法的目标就是在尽可能快速检索结果的同时最大化召回率，这就导致了召回率与延迟之间的权衡。</p></li><li><p>Unfortunately, it is often impossible to retrieve the exact nearest neighbors without essentially resorting to a linear scan of the data (see, e.g., [15, 23]) due to a phenomenon known as the <em>curse of</em> <em>dimensionality</em> [10].</p><ul><li>如果我们暴力的话 那么进行 linear scan，其 时间复杂度为O(n d);</li></ul></li><li><p>As a result, one resorts to finding the <em>approximate nearest neighbors</em> (ANN) where the goal is to retrieve <em>k</em> neighbors which are close to being optimal.</p><ul><li>ANNS：其目标是检索到接近最优的k个邻居</li></ul></li><li><p>More formally, consider a query <em>q</em>, and suppose the algorithm outputs a set <em>X</em> of <em>k</em> candidate near neighbors, and suppose <em>G</em> is the ground-truth set of the <em>k</em> closest neighbors to <em>q</em> from among the points of the base dataset.</p></li><li><p>Then, we define the <em>k</em>-recall@<em>k</em> of this set <em>X</em> to be |X∩G|/k. The goal of an ANN algorithm then is to maximize recall while retrieving the results as quickly as possible, which results in the recall-vs-latency tradeoff.（recall-vs-time tradeoff）</p></li><li><p><mark>原文翻译</mark></p></li><li><p>这个问题有许多算法，它们有着多样的索引构建方法，并在索引时间、召回率和查询时间等方面有一系列的权衡。例如，虽然k-d树生成的索引紧凑，且在低维度时搜索速度快，但当维度d超过大约20时它们通常会非常慢。另一方面，基于局部敏感哈希（LSH）的方法[2, 4]在最坏情况下提供了索引大小和搜索时间之间几乎最优的保证，但它们未能利用点的分布，并且在现实世界数据集上被更多近期的基于图的方法超越。最近对数据依赖的LSH方案的研究（例如[3]）还没有在大规模上被证明。截至目前，就搜索时间与召回率而言，现实世界数据集上最好的算法通常是基于图的算法，如HNSW[21]和NSG[13]，其中索引算法构造了一个<mark>可导航的图</mark>，覆盖了基础点，搜索过程是一个最优先遍历，它从一个选定的（或随机的）点开始，并沿着图的边走，同时在每一步都更接近查询点，直到它收敛到一个局部最小点。Li等人近期的工作[20]对ANN算法进行了优秀的调查和比较。</p></li><li><p>There are numerous algorithms for this problem with diverse index construction methodologies and a range of tradeoffs w.r.t indexing time, recall, and query time.</p><ul><li>其实这些算法就是： diverse index construction， tradeoff the indexing time, recall, and query time;</li></ul></li><li><p>For example, while k-d trees generate compact indices that are fast to search in low dimensions, they are typically very slow when dimension <em>d</em> exceeds about 20. On the other hand, Locality Sensitive Hashing based methods [2, 4] provide <em>near-optimal</em> guarantees on the tradeoff between index size and search time in the worst case, but they fail to exploit the distribution of the points and are outperformed by more recent graph-based methods on real-world datasets. Recent work on data-dependent LSH schemes (e.g. [3]) is yet to be proven at scale.</p><ul><li>讲述了 kd-tree的缺点维度问题 和 LSH的缺点没有充分利用点的分布：the distrubution of the points 空间特性，</li></ul></li><li><p>As of this writing, the best algorithms in terms of search time vs recall on real-world datasets are often graph-based algorithms such as HNSW [21] and NSG [13] where the indexing algorithm constructs a <em>navigable</em> graph over the base points, and the search procedure is a best-first traversal that starts at a chosen (or random) point, and walks along the edges of the graph, while getting closer to the query at each step until it converges to a local minimum. A recent work of Li et al. [20] has an excellent survey and comparison of ANN algorithms.</p><ul><li>讲述了在 search time vs recall 这两个的(not say index size) 有优势的基于图的算法：hnsw and nsg</li><li>离线构建阶段：constructs a navigable graph over the base points 可导航</li><li>在线搜索阶段：and the search procedure is a best-first traversal that starts at a chosen (or random) point, and walks along the edges of the graph, while getting closer to the query at each step until it converges to a local minimum.NN-expansion;</li></ul></li><li><p>许多应用程序需要在数十亿个欧几里得度量的点上进行快速准确的搜索。如今对大型数据集进行索引基本上有两种主要的高层方法。</p></li><li><p>Many applications require fast and accurate search on billions of points in Euclidean metrics. Today, there are essentially two high-level approaches to indexing large datasets.</p><ul><li>首先是 fast and accurate ，然后是 数据集大1b，</li></ul></li></ul><p>​</p><ul><li><p>第一种方法基于<mark>倒排索引+数据压缩( Inverted Index + Data Compression)</mark>，包括诸如 FAISS[18]和 IVFOADC+G+P[8]等方法。这些方法将数据集聚类为 M 个分区，并将查询 𝑞 最近的部分中的点做比较进行比较，比如，m &lt;&lt; M分区。m远小于M 个最接近查询的聚类部分中的点进行比较。此外，由于全精度向量无法装入主内存，所以使用诸如乘积量化[17]等量化方案对这些点进行压缩。虽然这些方案内存占用较小——在 128 维中存储数十亿个点的索引不到 64GB——并且使用 GPU 或其他硬件加速器可以在小于 5 毫秒内检索到结果，但它们的 1-recall@1 相当低（约为 0.5），因为数据压缩是有损的。这些方法对于较弱的 1-recall@100 的概念会报告更高的召回值——即真正的最近邻存在于 100 个输出候选列表中的可能性。然而，在许多应用中，这个度量可能不被接受。</p></li><li><p>footprint：占用内存</p></li><li><p>The first approach is based on <em>Inverted Index + Data Compression</em> and includes methods such as FAISS [18] and IVFOADC+G+P [8].</p><ul><li>这个感觉就是PQ 的论文 后面的介绍的那些即：IVF+PQ；</li></ul></li><li><p>These methods cluster the dataset into <em>M</em> partitions, and compare the query to only the points in a few, say, <em>m &lt;&lt; M</em> partitions closest to the query.</p><ul><li>第一步：聚类，然后建立IVF，然后query到m个 查询部分 进行 query比较(就是PQ讲述的后面的部分)</li><li>如果不进行PQ的话，那就是最普通的IVF了</li></ul></li><li><p>Moreover, since the full-precision vectors cannot fit in main memory, the points are compressed using a quantization scheme such as <em>Product Quantization</em> [17].</p><ul><li>上面先讲述一个简单的ivf，然后说不能全部放入到main memory，就进行PQ</li></ul></li><li><p>While these schemes have a small memory footprint – less than 64 GB for storing an index on billion points in 128 dimensions and can retrieve results in <em>&lt;</em> 5 ms using GPUs or other hardware accelerators, their 1-recall@1 is rather low (around 0*.*5) since the data compression is lossy.</p><ul><li>这样的话pq导致 内存占用小，然后又是在 main memory所以利用上硬件加速和GPUs 可以达到很低的检索时间；</li><li>但是呢由于 有损compression 导致recall is low</li></ul></li><li><p>These methods report higher recall values for a weaker notion of 1-recall@100 – the likelihood that the true nearest neighbor is present in a list of 100 output candidates. However, this measure may not be acceptable in many applications.</p><ul><li>这些方法 对于 1-recall@100的时候 结果会好；但是不适用；？？？？？？？？？？？？</li></ul></li><li><p>第二种方法是将数据集分割成不相交的分片，并为每个分片构建一个内存索引。然而，由于这些索引既存储索引又存储未压缩的数据点，所以它们的内存占用比第一种方法大。例如，对于 128 维的 1 亿个浮点向量的一个 NSG 索引，其内存占用大约为 75GB²。因此，要为数十亿个点提供索引服务，就需要多台机器来承载这些索引。据报道[13]，在阿里巴巴的电子商务平台淘宝中就采用了这样的方案，他们将包含 20 亿个 128 维点的数据集分成 32 个分片，并在不同的机器上为每个分片承载索引。查询被路由到所有分片，所有分片的结果被汇总。通过这种方法，他们报告在延迟约为 5 毫秒时 100-召回率@100 的值为 0.98。请注意，将其扩展到拥有数百亿个点的网络规模数据时，将需要数千台机器。查询时，将查询点同时路由到每一个部分，在各部分并行执行查询，最后，将各部分返回的结果整合到一块选取最近的点作为最终结果。这类方法由于将原始数据载入内存，因此有很大的内存占用，而且，当数据规模增大时，就需要更多的机器，优点是能够实现高召回低延迟。</p></li><li><p>The second approach is to divide the dataset into disjoint <em>shards</em>, and build an in-memory index for each shard.</p><ul><li>也是啊，比如SIFT100M的数据，有一个query，那么分别在10个10M个数据集上进行搜索最后结果rerank的话好像是跟在100M上进行搜索是一样的吧？？？？？？？？？</li></ul></li><li><p>However, since these indices store both the index and the uncompressed data points, they have a larger memory footprint than the first approach. For example, an NSG index for 100M floating-point vectors in 128 dimensions would have a memory footprint of around 75GB . Therefore, serving an index over a billion points would need several machines to host the indices.</p><ul><li>因为这个是没有进行压缩的，所以 SIFT100M的数据 占用75GB，因此存储1B，需要多台机器进行承载索引；</li></ul></li><li><p>Such a scheme is reportedly [13] in use in Taobao, Alibaba’s e-commerce platform, where they divide their dataset with 2 billion 128-dimensional points into 32 shards, and host the index for each shard on a different machine.</p><ul><li>20亿个数据 分为 32块不相交的；</li></ul></li><li><p>Queries are routed to all shards, and the results from all shards are aggregated. Using this approach, they report 100-recall@100 values of 0*.*98 with a latency of <em>∼</em> 5ms. Note that extending this to web scale data with <em>hundreds of billions</em> of points would require thousands of machines.</p><ul><li>效果感觉还是很好的；除了需要更多的机器；</li></ul></li><li><p>这两类算法的可扩展性都受到这样一个事实的限制，即<mark>它们构建的索引旨在从主内存中提供服务</mark>。将这些索引移动到磁盘，甚至是固态硬盘（SSDs）上，将会导致搜索延迟灾难性地上升以及相应的吞吐量下降。关于需要主内存的搜索的当前观点反映在 FAISS 的博客文章[11]中：“FAISS 仅支持从随机存取存储器（RAM）中进行搜索，因为磁盘数据库要慢好几个数量级。是的，即使是固态硬盘也是如此。”</p></li><li><p>The scalability of both these classes of algorithms is limited by the fact that they construct indices meant to be served from main memory.</p></li><li><p>Moving these indices to disks, even SSDs, would result in a catastrophic rise of search latency and a corresponding drop in throughput. The current wisdom on search requiring main memory is reflected in the blog post by FAISS [11]: <em>“Faiss supports searching</em> <em>only from RAM, as disk databases are orders of magnitude slower. Yes, even with SSDs.”</em></p><ul><li>disks and SSDs and the throughput？？？</li><li>如果将index放到这里面去的话，那么搜索的时间延迟就会很慢，就是说 如果 我们什么都不顾的把这些索引放到disk的话，那么我们的三部分中的 index size相当于在内存没有了；然后 recall 一样的方法应该是 不会降低的；但是最主要的是 时间上肯定降低了，甚至降低了几个数量级；也就是说如果我们想要去进行 挪移，那么就要想办法 去解决这个时间问题去 三赢；</li></ul></li></ul><h2 id="再重新细节看看看不懂" tabindex="-1"><a class="header-anchor" href="#再重新细节看看看不懂" aria-hidden="true">#</a> 再重新细节看看看不懂</h2><p>挑战是如何减少随机访问 SSD 的次数和减少发起 SSD 访问请求的数量？？？</p><ul><li><p>事实上，固态硬盘驻留索引的搜索吞吐量受到每个查询随机磁盘访问次数的限制，而延迟受到到磁盘往返次数（每次往返可能包含多次读取）的限制。一个廉价的零售级固态硬盘需要几百微秒来服务一次随机读取，并且每秒可以处理大约 30 万次随机读取。另一方面，具有多阶段管道的搜索应用程序（例如网络搜索）对于最近邻搜索需要几毫秒的平均延迟。因此，设计一个高性能的固态硬盘驻留索引的主要挑战在于减少（a）随机访问固态硬盘的次数到几十次，以及（b）到磁盘的往返请求次数到十次以下，最好是五次。天真地将传统内存中近似最近邻算法生成的索引映射到固态硬盘上，每个查询会产生数百次磁盘读取，这将导致不可接受的延迟。</p></li><li><p>a）<strong>随机访问固态硬盘的次数</strong>：这个描述关注的是SSD处理随机读取操作的能力。在这种情况下，&quot;次数&quot;通常指的是随机读取操作的频率，即SSD在单位时间内可以处理多少次随机读取请求。随机访问次数多，意味着SSD能够快速响应多个不连续的数据访问请求，这在需要处理大量小文件或数据库查询等场景中非常重要。SSD的随机读取性能通常用IOPS（每秒输入/输出操作数）来衡量。</p><p>b）<strong>到磁盘的往返请求次数</strong>：这个描述关注的是完成数据读取或写入操作所需的往返次数，即从主机发出请求到SSD完成操作并返回结果的整个过程。&quot;往返请求次数&quot;通常涉及到数据传输的延迟，包括SSD控制器处理请求、在闪存芯片中查找数据、数据传输到主机的时间。往返次数越少，意味着数据处理的延迟越低，这对于需要快速响应时间的应用（如在线事务处理系统）来说是非常关键的。</p><p>两者的区别主要在于：</p><ul><li><strong>随机访问次数</strong>强调的是SSD处理多个随机位置数据请求的能力，反映了SSD在高并发随机I/O操作下的性能。</li><li><strong>往返请求次数</strong>强调的是每次数据请求从发出到完成的效率，反映了SSD在单个I/O操作中的延迟。</li></ul></li><li><p>Indeed, the search throughput of an SSD-resident index is limited by the number of random disk accesses/query</p><ul><li>首先是SSD-index的search throughput即搜索吞吐量：是指 单位时间内能够处理的查询query的数量；因为每个查询点可能需要的 随机磁盘访问次数是不一样的；QPS</li></ul></li><li><p>and latency is limited by the the number of round-trips (each round-trip can consist of multiple reads) to the disk.</p></li><li><p>An inexpensive retail-grade SSD requires a few hundred microseconds to serve a random read and can service about <em>∼</em> 300K random reads per second. On the other hand, search applications (e.g. web search) with multi-stage pipelines require mean latencies of a few milliseconds for nearest neighbor search.</p></li><li><p>Therefore, the main challenges in designing a performant SSD-resident index lie in reducing (a) the number of random SSD accesses to a few dozen, and (b) the number of round trip requests to disk to under ten, preferably five. Naively mapping indices generated by traditional in-memory ANNS algorithms to SSDs would generate several hundreds of disk reads per query, which would result in unacceptable latencies.</p></li></ul><h3 id="_1-1-our-technical-contribution" tabindex="-1"><a class="header-anchor" href="#_1-1-our-technical-contribution" aria-hidden="true">#</a> 1.1 Our technical contribution</h3><ul><li><p><mark>原文翻译</mark></p></li><li><p>我们提出了DiskANN，这是一种基于我们新的基于图的索引算法Vamana的SSD驻留ANNS系统，它推翻了当前的智慧并证明了即使是商品级SSD也可以有效地支持大规模的ANNS。我们工作的一些有趣方面包括：</p><ul><li>DiskANN可以在一台配有64GB RAM的工作站上为高达数十亿个数据点的百维数据集建立索引并提供服务，提供95%以上的1-recall@1，并且延迟时间低于5毫秒。</li><li>一种名为Vamana的新算法能生成比NSG和HNSW更小直径的图索引，使DiskANN能够将连续硬盘读取的数量降到最低( sequential disk reads)。</li><li>由Vamana生成的图也可以在内存( in-memory)中使用，在那里它们的搜索性能达到或超过了如HNSW和NSG这样的最先进的内存中算法。</li><li>将数据集重叠划分为多个区域，在每个区域中建立小Vamana，然后再归并得到一个索引，它的性能与在整个数据集构建索引相当(不能完全载入内存的大规模数据)；</li><li>我们证明了Vamana可以结合现成的矢量压缩(vector compression)方案，如乘积量化(product quantization)来构建DiskANN系统。图索引和数据集的全精度向量一起存储在磁盘上，而压缩的向量则缓存在内存中。</li></ul></li><li><p>We present DiskANN, an SSD-resident ANNS system based on our new graph-based indexing algorithm called Vamana, that debunks current wisdom and establishes that even commodity SSDs can effectively support large-scale ANNS. Some interesting aspects of our work are: 这个是将大部分放在了SSD，然后并且一个新的可导航图Vamana,并且对SSD的性能要求没有太限制通过算法的设置，比如可能好的SSD的性能好，导致的search的时间消耗低，但是一个普通的商品级的SSD仍然是可以的；</p><ul><li>DiskANN can index and serve a billion point dataset in 100s of dimensions on a workstation with 64GB RAM, providing 95%+ 1-recall@1 with latencies of under 5 milliseconds.之前的第二个方法是SIFT100M 75GB ；这里说的效果好；</li><li>A new algorithm called Vamana which can generate graph indices with smaller diameter than NSG and HNSW, allowing DiskANN to minimize the number of sequential disk reads.因为每次查询要减少访问磁盘的次数，所以改进了目前的sota的建立图的算法进行改进以适应磁盘的；比NSG和HNSW有更小的直径（路径跳数），使DiskANN最小化顺序读取磁盘的次数；</li><li>The graphs generated by Vamana can be also be used in-memory, where their search performance matches or exceeds state-of-the-art in-memory algorithms such as HNSW and NSG.完全的内存的搜索性能也是比较好的(更改了选边策略)</li><li>Smaller Vamana indices for overlapping partitions of a large dataset can be easily merged into one index that provides nearly the same search performance as a single-shot index constructed for the entire dataset. This allows indexing of datasets that are otherwise too large to fit in memory.并且对于大规模数据不能导入到内存的；这个的话可不可以结合淘宝那个先划分不同的机器呢？？还有就是这里为什么 要进行 重叠呢？？就是说划分然后 合并 就是说现在的Vamana的仿照的这个功能相对于NSG是没有被丢掉的；</li><li>We show that Vamana can be combined with off-the-shelf vector compression schemes such as product quantization to build the DiskANN system. The graph index along with the full-precision vectors of the dataset are stored on the disk, while compressed vectors are cached in memory。通过压缩进一步的压缩内存，即在main memory的消耗；图索引和原始的数据在disk上面，然后 压缩的向量在 in memory；这也是为了更好的压缩 主内存；其实可能没有这个的结合，也是可以进行对比的；只不过如果使用了 这个 recall仍然没有下降的话那么就是更好的方案了；</li></ul></li></ul><h3 id="_1-2-notation" tabindex="-1"><a class="header-anchor" href="#_1-2-notation" aria-hidden="true">#</a> 1.2 Notation</h3>',12),W=e("ul",null,[e("li",null,[e("mark",null,"总结")]),e("li"),e("li",null,[e("mark",null,"原文翻译")]),e("li",null,"在本文的剩余部分，我们用P表示数据集，并且|P| = n。我们考虑顶点对应于P中的点，它们之间具有边的有向图。通过稍微重载表示法，我们用G = (P, E)来指这样的图，同时让P也代表顶点集。对于有向图中的一个点p ∈ P，我们用Nout(p)表示落在p上的出边集合。最后，我们用xp表示与点p对应的向量数据，并且用d(p, q) = ||xp − xq||表示两点p和q之间的度量距离。本文中展示的所有实验都使用了欧几里得度量( Euclidean metric)。"),e("li",null,[s("For the remainder of the paper, we let "),e("em",null,"P"),s(" denote the dataset with "),e("em",null,[s("|"),e("strong",null,"P"),s("|")]),s(" = "),e("em",null,"n"),s(". We consider directed graphs with vertices corresponding to points in "),e("em",null,"P"),s(", and edges between them. With slight notation overload, we refer to such graphs as "),e("em",null,"G"),s(" = ("),e("em",null,"P, E"),s(") by letting "),e("em",null,"P"),s(" also denote the vertex set. Given a point "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" in a directed graph, we let "),e("em",null,"N"),s("out("),e("em",null,"p"),s(") to denote the set of out-edges incident on "),e("em",null,"p"),s(". Finally, we let "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"x_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(" denote the vector data corresponding to "),e("em",null,"p"),s(", and let "),e("em",null,"d"),s("("),e("em",null,"p, q"),s(") = "),e("em",null,[s("||"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"x_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(" - "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"q")])]),e("annotation",{encoding:"application/x-tex"},"x_q")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s("||")]),s(" denote the metric distance between two points "),e("em",null,"p"),s(" and "),e("em",null,"q"),s(". All experiments presented in this paper used Euclidean metric. "),e("ul",null,[e("li",null,[s("这里就是说： "),e("em",null,"G"),s(" = ("),e("em",null,"P, E"),s(") 这个图，P是点集，E是边集，且是有向边；")]),e("li",null,[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"N"),e("mrow",null,[e("mi",null,"o"),e("mi",null,"u"),e("mi",null,"t")])]),e("mo",{stretchy:"false"},"("),e("mi",null,"p"),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},"N_{out} (p)")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"N"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2806em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.109em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mathnormal mtight"},"o"),e("span",{class:"mord mathnormal mtight"},"u"),e("span",{class:"mord mathnormal mtight"},"t")])])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])]),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal"},"p"),e("span",{class:"mclose"},")")])])]),s(" :表示 该点的 出度；")]),e("li",null,[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"x_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(": 表示 原始向量；")]),e("li",null,"d(p, q) :表示距离 在 欧几里得里面；")])])],-1),F=l('<h3 id="_1-3-paper-outline" tabindex="-1"><a class="header-anchor" href="#_1-3-paper-outline" aria-hidden="true">#</a> 1.3 Paper Outline</h3><ul><li><mark>原文翻译</mark></li><li>第2节介绍了我们的新图索引构建算法Vamana，第3节解释了DiskANN的整体系统设计。第4节提出了Vamana与HNSW和NSG在内存索引的实证比较，并展示了DiskANN在普通机器上针对十亿点数据集的搜索特性。</li><li>Section 2 presents Vamana our new graph index construction algorithm and Section 3 explains the overall system design of DiskANN. Section 4 presents an empirical comparison Vamana with HNSW and NSG for in-memory indices, and also demonstrates the search characteristics of DiskANN for billion point datasets on a commodity machine.</li></ul><h2 id="no-2-the-vamana-graph-construction-algorithm" tabindex="-1"><a class="header-anchor" href="#no-2-the-vamana-graph-construction-algorithm" aria-hidden="true">#</a> No.2 The Vamana Graph Construction Algorithm</h2><ul><li><mark>原文翻译</mark></li><li>在介绍Vamana的详细信息之前，我们首先简要概述基于图的ANNS算法，这些详细信息在算法3中给出了规范。</li><li>We begin with a brief overview of graph-based ANNS algorithms before presenting the details of Vamana, a specification which is given in Algorithm 3.</li></ul><h3 id="_2-1-relative-neighborhood-graphs-and-the-greedysearch-algorithm" tabindex="-1"><a class="header-anchor" href="#_2-1-relative-neighborhood-graphs-and-the-greedysearch-algorithm" aria-hidden="true">#</a> 2.1 Relative Neighborhood Graphs and the GreedySearch algorithm</h3>',5),L=e("ul",null,[e("li",null,[e("p",null,[e("mark",null,"总结")])]),e("li",null,[e("p",null,"Relative Neighborhood Graphs RNG；")]),e("li",null,[e("p",null,"以及 对于 贪婪搜索的时候 SNG的好处；以及SNG的构建和 规则；")]),e("li",null,[e("p",null,[e("mark",null,"原文翻译")])]),e("li",null,[e("p",null,"大多数基于图的ANNS算法的工作方式如下：在索引构建期间，它们根据数据集P的几何属性构建图G = (P, E)。在搜索时，对于一个查询向量xq，搜索使用如算法1中的自然贪婪(a natural greedy)或最佳优先遍历(best-first traversal)在G上进行。从某个指定的点s ∈ P开始，它们遍历图形以逐渐靠近xq。")]),e("li",null,[e("p",null,[s("Most graph-based ANNS algorithms work in the following manner: during index construction, they build a graph "),e("em",null,"G"),s(" = ("),e("em",null,"P, E"),s(") based on the geometric properties of the dataset "),e("em",null,"P"),s(". 离线构建阶段")])]),e("li",null,[e("p",null,[s("At search time, for a query vector "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"q")])]),e("annotation",{encoding:"application/x-tex"},"x_q")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(", search employs a natural greedy or best-first traversal, such as in Algorithm 1, on "),e("em",null,"G"),s(". Starting at some designated point "),e("em",null,"s"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(", they traverse the graph to get progressively closer to "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"q")])]),e("annotation",{encoding:"application/x-tex"},"x_q")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(".使用NN-expansion；")])]),e("li",null,[e("p",null,"while：得到局部最优前；L是候选队列；V仅仅是记录已经visited")]),e("li",null,[e("figure",null,[e("img",{src:c,alt:"algorithm1",tabindex:"0",loading:"lazy"}),e("figcaption",null,"algorithm1")])]),e("li",null,[e("p",null,"目前已有大量工作致力于了解如何构造稀疏图(sparse graphs)，以便GreedySearch(s, xq, k, L)能够快速收敛到任意查询的（近似）最近邻居。至少当查询接近数据集点时，发生这种情况的一个充分条件是所谓的稀疏邻域图（SNG），该概念在文献[5]中被介绍。在一个SNG中，每个点p的外部邻居的确定如下：初始化一个集合S = P \\ {p}。只要S ≠ ∅，就从p添加一条有向边到p* ，其中p*是S中距离p最近的点，并且从S中移除所有点p0，使得d(p, p0) > d(p**, p0)。然后很容易看出，GreedySearch(s, xp, 1, 1)从任何s ∈ P开始都将收敛到所有基点p ∈ P的点p。")]),e("li",null,[e("p",null,[s("There has been much work on understanding how to construct sparse graphs for which the GreedySearch("),e("em",null,"s,"),s(),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"q")])]),e("annotation",{encoding:"application/x-tex"},"x_q")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(", k, L) converges quickly to the (approximate) nearest neighbors for any query. 稀疏图是友好地；那么稀疏图的可能定义如下：")])]),e("li",null,[e("p",null,[s("A sufficient condition for this to happen, at least when the queries are close to the dataset points, is the so-called "),e("em",null,"sparse neighborhood graph"),s(" (SNG), which was introduced in [5] . 【This notion itself was inspired by a related property known as the "),e("em",null,"Relative Neighborhood Graph"),s(" (RNG) property, first defined in the 1960s】所以RNG 和 SNG的关系？？？")])]),e("li",null,[e("p",null,[s("In an SNG, the out-neighbors of each point "),e("em",null,"p"),s(" are determined as follows: initialize a set "),e("em",null,"S"),s(" = "),e("em",null,"P"),s(),e("em",null,"\\ {p}"),s(". As long as S ≠ ∅, add a directed edge from "),e("em",null,"p"),s(" to "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"p"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"p*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6597em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal"},"p"),e("span",{class:"mord"},"∗")])])]),s(" , where "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"p"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"p*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6597em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal"},"p"),e("span",{class:"mord"},"∗")])])]),s(" is the closest point to "),e("em",null,"p"),s(" from "),e("em",null,"S"),s(", and remove from "),e("em",null,"S"),s(" all points "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"p'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9463em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(" such that "),e("em",null,"d"),s("(p, "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"p'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9463em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(" ) "),e("em",null,"> d"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"p"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"p*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6597em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal"},"p"),e("span",{class:"mord"},"∗")])])]),s(" , "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"p'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9463em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(" ). It is then easy to see that GreedySearch("),e("em",null,"s,"),s(),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"x_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(" 1*,* 1) starting at any "),e("em",null,"s"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" would converge to "),e("em",null,"p"),s(" for all base points "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(".")]),e("ul",null,[e("li",null,"就是说 全部的数据集：S，"),e("li",null,[s("然后将接近的p 的 最近邻 放入到"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"p"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"p*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6597em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal"},"p"),e("span",{class:"mord"},"∗")])])]),s(" ；放入一个 就对剩下的S中的进行散化删除掉 不能当p的出度邻居；")]),e("li",null,"然后 不断的进行上一段的；这样的话，贪婪搜索； 就变成了 dfs"),e("li",null,"就是说 如果 进行 dfs的话，那么怎么样才能得到 真正的 快速收敛到这个呢？")])]),e("li",null,[e("p",null,"虽然这种构建在原则上是理想的，但对于即使是中等规模的数据集来构建这样的图也是不可行的，因为运行时间约为 O(n²)。基于这种直觉，已经有一系列的工作来设计更实际的算法，这些算法能生成对 SNG 的良好近似[21,13]。然而，由于它们基本上都试图近似 SNG 属性，所以在控制这些算法输出的图的直径和密度方面灵活性非常小。")]),e("li",null,[e("p",null,[s("While this construction is ideal in principle, it is infeasible to construct such graphs for even moderately sized datasets, as the running time is "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"n"),e("mn",null,"2")])]),e("annotation",{encoding:"application/x-tex"},"n^2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])])])])])])])])]),s(" ). 如果我们要建立一个根据上面的简单定义 的SNG，那么就是对每个点进行那样了")])]),e("li",null,[e("p",null,"Building on this intuition, there have been a series of works that design more practical algorithms that generate good approximations of SNGs [21, 13]. However, since they all essentially try to approximate the SNG property, there is very little flexibility in controlling the diameter and the density of the graphs output by these algorithms."),e("ul",null,[e("li",null,"就是说这个 diameter and the density of the graphs output /???这两个 很重要；")])])],-1),T=e("h3",{id:"_2-2-the-robust-pruning-procedure",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-2-the-robust-pruning-procedure","aria-hidden":"true"},"#"),s(),e("strong",null,"2.2 The Robust Pruning Procedure")],-1),R=e("ul",null,[e("li",null,[e("p",null,[e("mark",null,"总结")])]),e("li",null,[e("p",null,"SNG属性的图 这个。SNG也可以叫做 RNG；")]),e("li",null,[e("p",null,"圈内只有一个点；简单的陈述；")]),e("li",null,[e("p",null,"GreedySearch贪婪搜索")]),e("li",null,[e("p",null,[e("mark",null,"原文翻译")])]),e("li",null,[e("p",null,"如前面所提到的，满足SNG属性的图都是GreedySearch搜索过程的很好的候选图。然而，这些图的直径也有可能相当大。例如，如果点在一维线上线性排列，则每个点连接 其两个邻居（其中一个在端点）的O(n)直径线图就是满足SNG属性的图。搜索存储在磁盘中的这样的图会导致对磁盘进行多次连续的读取，以获取算法1中搜索路径上访问的顶点的邻居。")]),e("li",null,[e("p",null,"As mentioned earlier, graphs which satisfy the SNG property are all good candidates for the GreedySearch search procedure. However, it is possible that the diameter of the graphs can be quite large.")]),e("li",null,[e("p",null,[s("For example, if the points are linearly arranged on the real line in one dimension, the "),e("em",null,"O"),s("("),e("em",null,"n"),s(") diamater line graph, where each point connects to its two neighbors (one at the end), is the one that satisfies the SNG property. Searching such graphs stored in disks would incur many sequential reads to the disk at to fetch the neighbors of the vertices visited on the search path in Algorithm 1.")])]),e("li",null,[e("figure",null,[e("img",{src:u,alt:"vamana",tabindex:"0",loading:"lazy"}),e("figcaption",null,"vamana")])]),e("li",null,[e("p",null,"为了克服这一点，我们希望确保沿着搜索路径的每个节点到查询的距离以一个大于 1 的乘法因子α减小，而不仅仅是像在 SNG 属性中那样仅仅减小。考虑有向图，其中每个点 p 的出边邻居是由算法 2 中的稳健剪枝（p，V，α，R）过程确定的。注意，如果每个 p∈P 的出边邻居是由稳健剪枝（p，P{p}，α，n-1）确定的，那么从任意 s 开始的贪婪搜索（s，p，1，1），如果α＞1，将在对数级的许多步骤内收敛到 p∈P。然而，这将导致索引构建的运行时间约为 O(n²)。因此，基于[21,13]的想法，Vamana为一个精心选择的、节点数远远少于 n-1 的 V 调用稳健剪枝（p，V，α，R），以改进索引构建时间。")]),e("li",null,[e("p",null,[s("To overcome this, we would like to ensure that the distance to the query decreases by a multiplicative factor of "),e("em",null,"α >"),s(" 1 at every node along the search path, instead of merely decreasing as in the SNG property.")])]),e("li",null,[e("p",null,[s("Consider the directed graph where the out-neighbors of every point "),e("em",null,"p"),s(" are determined by the RobustPrune("),e("em",null,"p,"),s(),e("em",null,"V"),s(", α, R) procedure in Algorithm 2.")])]),e("li",null,[e("p",null,[s("Note that if the out-neighbors of every "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" are determined by RobustPrune("),e("em",null,"p, P"),s(),e("em",null,[s("\\ {"),e("strong",null,"p"),s("}, α, n")]),s(),e("em",null,"−"),s(" 1), then GreedySearch("),e("em",null,"s, p,"),s(" 1*,* 1), starting at any "),e("em",null,"s"),s(", would converge to "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" in logarithmically many steps, if "),e("em",null,"α >"),s(" 1.")])]),e("li",null,[e("p",null,[s("However, this would result in a running time of "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"n"),e("mn",null,"2")])]),e("annotation",{encoding:"application/x-tex"},"n^2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])])])])])])])])]),s(" ) for index construction. Hence, building on the ideas of [21, 13], Vamana invokes RobustPrune("),e("em",null,"p,"),s(),e("em",null,"V"),s(", α, R) for a carefully selected "),e("em",null,"V"),s(" with far fewer than "),e("em",null,"n"),s(),e("em",null,"−"),s(" 1 nodes, to improve index construction time.")])])],-1),H=e("figure",null,[e("img",{src:d,alt:"algorithm2",tabindex:"0",loading:"lazy"}),e("figcaption",null,"algorithm2")],-1),C=e("h3",{id:"_2-3-the-vamana-indexing-algorithm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-3-the-vamana-indexing-algorithm","aria-hidden":"true"},"#"),s(" 2.3 The Vamana Indexing Algorithm")],-1),B=e("ul",null,[e("li",null,[e("p",null,[e("mark",null,"总结")])]),e("li"),e("li",null,[e("p",null,[e("mark",null,"原文翻译")])]),e("li",null,[e("p",null,"Vamana以迭代的方式构建一个有向图 G。图 G 被初始化，使得每个顶点有 R 个随机选择的出边邻居。请注意，当 R＞logn 时，图是充分连接的，但随机连接并不能确保贪婪搜索算法收敛到好的结果。接下来，我们让 s 表示数据集 P 的质心，它将是搜索算法的起始节点。然后该算法以随机顺序遍历 P 中所有的点 p，并且在每一步中，更新图以使其更适合贪婪搜索（s，xp，1，L）收敛到 p。实际上，在对应于点 p 的迭代中，Vamana首先在当前图 G 上运行贪婪搜索（s，xp，1，L），并将 Vp 设置为贪婪搜索（s，xp，1，L）访问的所有点的集合。然后，该算法通过运行稳健剪枝（p，Vp，α，R）来更新 G 以确定 p 的新出边邻居。然后，Vamana通过为所有 p′∈Nout(p)添加反向边（p′，p）来更新图 G。这确保了在搜索路径上访问的顶点和 p 之间有连接，从而确保更新后的图将更适合贪婪搜索（s，xp，1，L）收敛到 p。")]),e("li",null,[e("p",null,[s("Vamana constructs a directed graph "),e("em",null,"G"),s(" in an iterative manner. The graph "),e("em",null,"G"),s(" is initialized so that each vertex has "),e("em",null,"R"),s(" randomly chosen out-neighbors. Note that while the graph is well connected when "),e("em",null,"R >"),s(" log "),e("em",null,"n"),s(", random connections do not ensure convergence of the GreedySearch algorithm to good results. 其实一个充分连接的图也不一定能得到好的结果？？")])]),e("li",null,[e("p",null,[s("Next, we let "),e("em",null,"s"),s(" denote the medoid of the dataset "),e("em",null,"P"),s(", which will be the starting node for the search algorithm. The algorithm then iterates through all the points in "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" in a random order, and in each step, updates the graph to make it more suitable for GreedySearch("),e("em",null,"s,"),s(" x"),e("em",null,"p,"),s(" 1*, L*) to converge to "),e("em",null,"p"),s(". 质心 s 然后对P中的每个点进行搜索算法，在进行GreedySearch的时候，然后更新图；")])]),e("li",null,[e("p",null,[s("Indeed, in the iteration corresponding to point "),e("em",null,"p"),s(", Vamana first runs GreedySearch("),e("em",null,"s,"),s(" x"),e("em",null,"p,"),s(" 1*, L*) on the current graph "),e("em",null,"G"),s(", and sets "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"V"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"V_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(" to the "),e("em",null,"set of all points visited"),s(" by GreedySearch("),e("em",null,"s,"),s(" x"),e("em",null,"p,"),s(" 1*, L*). Then, the algorithm updates "),e("em",null,"G"),s(" by running RobustPrune("),e("em",null,"p,"),s(),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"V"),e("mi",null,"p")])]),e("annotation",{encoding:"application/x-tex"},"V_p")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(" , α, R) to determine "),e("em",null,"p"),s("’s new out-neighbors.当对p点进行更新图的时候，首先 search然后更新图；")])]),e("li",null,[e("p",null,[s("Then, Vamana updates the graph "),e("em",null,"G"),s(" by adding backward edges ("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"p'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9463em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(" , p) for all "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"p'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9463em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(),e("em",null,"∈"),s(),e("em",null,"N"),s("out("),e("em",null,"p"),s("). This ensures that there are connections between the vertices visited on the search path and "),e("em",null,"p"),s(", thereby ensuring that the updated graph will be better suited for GreedySearch("),e("em",null,"s,"),s(" x"),e("em",null,"p,"),s(" 1*, L*) to converge to "),e("em",null,"p"),s(". 进行反向边的添加；")])]),e("li",null,[e("p",null,"然而，添加（p′，p）这种形式的反向边可能会导致对 p′的度的违反，因此每当任何顶点 p′的出度超过度阈值 R 时，通过运行稳健剪枝（p′，Nout(p′)，α，R）来修改图，其中 Nout(p′)是 p′现有的出边邻居的集合。随着算法的进行，该图对于贪婪搜索来说变得越来越好且越来越快。我们的整体算法对数据集进行两次遍历，第一次遍历α=1，第二次遍历使用用户定义的α≥1。我们观察到第二次遍历会产生更好的图，并且使用用户定义的α进行两次遍历会使索引算法变慢，因为第一次遍历计算出的图具有更高的平均度，这需要更长时间。")]),e("li",null,[e("p",null,[s("However, adding backward edges of the form (p′"),e("em",null,", p"),s(") might lead to a degree violation of p′ , and so whenever any vertex p′ has an out-degree which exceeds the degree threshold of "),e("em",null,"R"),s(", the graph is modified by running RobustPrune(p′，Nout(p′)，α，R) where "),e("em",null,"N"),s("out(p′ ) is the set of existing out neighbors of p′ . 当出度超R的时候，运行RobustPrune")])]),e("li",null,[e("p",null,[s("As the algorithm proceeds, the graph becomes consistently better and faster for GreedySearch. Our overall algorithm makes two passes over the dataset, the first pass with "),e("em",null,"α"),s(" = 1, and the second with a user-defined "),e("em",null,"α"),s(),e("em",null,"≥"),s(" 1. We observed that a second pass results in better graphs, and that running both passes with the user-defined "),e("em",null,"α"),s(" makes the indexing algorithm slower as the first pass computes a graph with higher average degree which takes longer.")])])],-1),O=l('<figure><img src="'+g+'" alt="algorithm3" tabindex="0" loading="lazy"><figcaption>algorithm3</figcaption></figure><figure><img src="'+f+'" alt="figure1" tabindex="0" loading="lazy"><figcaption>figure1</figcaption></figure><h3 id="_2-4-comparison-ofvamanawith-hnsw-21-and-nsg-13" tabindex="-1"><a class="header-anchor" href="#_2-4-comparison-ofvamanawith-hnsw-21-and-nsg-13" aria-hidden="true">#</a> 2.4 Comparison ofVamanawith HNSW [21] and NSG [13]</h3><ul><li>从高层次来看，Vamana与 HNSW 和 NSG 这两种非常流行的近似最近邻搜索算法相当相似。这三种算法都对数据集 P 进行迭代，并使用贪婪搜索GreedySearch（s，xp，1，L）和稳健剪枝RobustPrune（p，V，α，R）的结果来确定 p 的邻居。然而，这些算法之间存在一些重要差异。最关键的是，HNSW 和 NSG 都没有可调节的参数α，并且隐含地使用α=1。这是让Vamana在图的度和直径之间实现更好权衡的主要因素。接下来，虽然 HNSW 将剪枝过程的候选集 V 设置为贪婪搜索（s，p，1，L）输出的 L 个候选的最终结果集，而Vamana和 NSG 让 V 是贪婪搜索（s，p，1，L）访问的所有顶点集。直观地说，这个特性帮助Vamana和 NSG 添加远程边，而 HNSW 由于只向附近点添加局部边，因此有一个额外的步骤，即在数据集的嵌套样本序列上构建图的层次结构。下一个差异与初始图有关：虽然 NSG 将起始图设置为数据集上的近似 K 最近邻图，这是一个时间和内存密集型步骤，而 HNSW 和Vamana有更简单的初始化，前者从空图开始，Vamana从随机图开始。我们观察到，从随机图开始会比从空图开始产生质量更好的图。最后，Vamana对数据集进行两次遍历，而 HNSW 和 NSG 都只进行一次遍历，这是由我们的观察结果即第二次遍历会提高图的质量所推动的。</li><li>At a high level, Vamana is rather similar to HNSW and NSG, two very popular ANNS algorithms. All three algorithms iterate over the dataset <em>P</em>, and use the results of the GreedySearch(<em>s,</em> x<em>p,</em> 1*, L*) and RobustPrune(<em>p,</em> <em>V</em>, α, R*) to determine <em>p</em>’s neighbors.</li><li>However, there are some important differences between these algorithms. Most crucially, both HNSW and NSG have no tunable parameter <em>α</em> and implicitly use <em>α</em> = 1. This is the main factor which lets Vamana achieve a better trade-off between graph degree and diameter. 这个？？ graph degree and diameter？？</li><li>Next, while HNSW sets the candidate set <em>V</em> for the pruning procedure to be the <em>final result-set of</em> <em>L</em> <em>candidates</em> output by GreedySearch(<em>s, p,</em> 1*, L*), Vamana and NSG let <em>V</em> be the entire set of vertices visited by GreedySearch(<em>s, p,</em> 1*, L*). Intuitively, this feature helps Vamana and NSG add long-range edges, while HNSW, by virtue of adding only local edges to nearby points, has an additional step of constructing a hierarchy of graphs over a nested sequence of samples of the dataset.</li><li>The next difference pertains to the initial graph: while NSG sets the starting graph to be an approximate <em>K</em>-nearest neighbor graph over the dataset, which is a time and memory intensive step, HNSW and Vamana have simpler initializations, with the former beginning with an empty graph and Vamana beginning with a random graph. We have observed that starting with a random graph results in better quality graphs than beginning with the empty graph.</li><li>Finally, Vamana makes two passes over the dataset, whereas both HNSW and NSG make only one pass, motivated by our observation that the second pass improves the graph quality.</li></ul><h2 id="_3-diskann-constructing-ssd-resident-indices" tabindex="-1"><a class="header-anchor" href="#_3-diskann-constructing-ssd-resident-indices" aria-hidden="true">#</a> 3 DiskANN: Constructing SSD-Resident Indices</h2><ul><li>我们现在分两部分来整体呈现 DiskANN 的设计。在第一部分，我们解释索引构建算法，在第二部分，我们解释搜索算法。</li><li>We now present the design of the DiskANN overall in two parts. In the first part, we explain the index construction algorithm, and in the second part, we explain the search algorithm.</li></ul><h3 id="_3-1-thediskannindex-design" tabindex="-1"><a class="header-anchor" href="#_3-1-thediskannindex-design" aria-hidden="true">#</a> 3.1 TheDiskANNIndex Design</h3>',7),E=e("ul",null,[e("li",null,[e("p",null,"高层的想法很简单：在数据集 P 上运行Vamana并将得到的图存储在固态硬盘（SSD）上。在搜索时，每当算法 1 需要一个点 p 的出邻域时(就是需要一个点p的邻居的时候)，我们只需从 SSD 中获取(fetch)此信息。然而，==请注意，仅仅在 100 维中为十亿个点存储向量数据就会远远超出工作站的随机存取存储器（RAM）！==这就引出了两个问题：我们如何在十亿个点上构建一个图，以及如果我们甚至无法存储向量数据，那么在算法 1 的搜索的时候，我们如何在查询点和候选列表中的点之间进行距离比较？")]),e("li",null,[e("p",null,[s("The high-level idea is simple: run Vamana on a dataset "),e("em",null,"P"),s(" and store the resulting graph on an SSD. At search time, whenever Algorithm 1 requires the out-neighbors of a point "),e("em",null,"p"),s(", we simply fetch this information from the SSD. 仔细想下Vamana的过程，")])]),e("li",null,[e("p",null,[s("However, note that "),e("em",null,"just storing the vector data for a billion points in"),s(" 100 "),e("em",null,"dimensions would far exceed the RAM on a workstation!")])]),e("li",null,[e("p",null,"This raises two questions: how do we build a graph over a billion points, ？第一个问题？")]),e("li",null,[e("p",null,"and how do we do distance comparisons between the query point and points in our candidate list at search time in Algorithm 1, if we cannot even store the vector data?第二个问题；")]),e("li",null,[e("p",null,"解决第一个问题的一种方法是使用像 k-means 这样的聚类技术将数据分割成多个较小的分片，为每个分片构建一个单独的索引，并在搜索时仅将查询路由到少数几个分片。然而，因为查询需要被路由到多个分片。这样的方法会遭受搜索延迟增加和吞吐量降低的问题。")]),e("li",null,[e("p",null,[s("One way to address the first question would be to partition the data into "),e("em",null,"multiple smaller shards"),s(" using clustering techniques like "),e("em",null,"k"),s("-means, build a separate index for each shard, and route the query only to a few shards at search time. However, such an approach would suffer from increased search latency and reduced throughput since the query needs to be routed to several shards.")])]),e("li",null,[e("p",null,"事后看来，==我们的想法很简单：与其在搜索时将查询路由到多个分片，不如如果我们将每个基本点发送到多个附近的中心以获得重叠的簇呢？（这个要好好的理解以下）==实际上，我们首先使用 k-means 将十亿个点的数据集划分为 k 个簇（比如说 k = 40），然后将每个基本点(base point)分配给 ll- 个最接近的中心（通常 ll= 2 就足够了）。然后，我们为分配给每个簇的点构建Vamana索引（现在每个簇大约只有 N * ll / k 个点，因此可以在内存中进行索引），最后通过简单地合并边将所有不同的图合并成一个单一的图。根据经验，事实证明不同簇的重叠性质为贪婪搜索算法提供了足够的连接性，即使查询的最近邻实际上在多个分片中分裂。我们想指出的是，之前已经有早期的工作[9,22]通过合并几个较小的、重叠的索引来为大型数据集构建索引。然而，他们构建重叠簇的想法是不同的，需要对这些不同技术进行更详细的比较。")]),e("li",null,[e("p",null,[s("Our idea is simple in hindsight: "),e("em",null,"instead of routing the query to multiple shards at search time, what"),s(),e("em",null,"if we send each base point to multiple nearby centers to obtain overlapping clusters?")])]),e("li",null,[e("p",null,[s("Indeed, we first partition a billion point dataset into "),e("em",null,"k"),s(" clusters (with "),e("em",null,"k"),s(" = 40, say) using "),e("em",null,"k"),s("-means, and then assign each base point to the "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"l"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"l^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s("-closest centers (typically "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"l"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"l^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s(" = 2 suffices). We then build Vamana indices for the points assigned to each of the clusters (which would now only have about $ N l^* /k$ points and thus can be indexed in-memory), and finally merge all the different graphs into a single graph by taking a simple union of edges. 那么这个合并是在哪里进行的呢？？？")])]),e("li",null,[e("p",null,"Empirically, it turns out that the overlapping nature of the different clusters provides sufficient connectivity for the GreedySearch algorithm to succeed even if the query’s nearest neighbors are actually split between multiple shards. We would like to remark that there have been earlier works [9, 22] which construct indices for large datasets by merging several smaller, overlapping indices. However, their ideas for constructing the overlapping clusters are different, and a more detailed comparison of these different techniques needs to be done.")]),e("li",null,[e("p",null,"我们解决第二个问题的下一个自然想法是在主内存中为每个数据库点 p∈P 存储压缩向量 ̃xp，同时在固态硬盘上存储该图。我们使用一种称为乘积量化[17]4 的流行压缩方案，它将数据点和查询点编码成短代码（例如，每个数据点 32 字节），可以在算法 1 中查询时有效地获得近似距离 d( ̃xp,xq)。我们想指出的是，Vamana在构建图索引时使用全精度坐标，因此能够有效地引导搜索朝向图的正确区域，尽管我们在查询时仅使用压缩数据。")]),e("li",null,[e("p",null,[s("Our next and natural idea to address the second question is to store "),e("em",null,"compressed vectors"),s(),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{x_p}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6306em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])]),e("span",{style:{top:"-3.5506em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])]),s(" for every database point "),e("em",null,"p"),s(),e("em",null,"∈"),s(),e("em",null,"P"),s(" in main memory, along with storing the graph on the SSD. We use a popular compression scheme known as "),e("em",null,"Product Quantization"),s("[17], which encodes the data and query points into short "),e("em",null,"codes"),s(" (e.g., 32 bytes per data point) that can be used to efficiently obtain approximate distances "),e("em",null,"d"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("msub",null,[e("mi",null,"x"),e("mi",null,"p")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{x_p}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6306em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])]),e("span",{style:{top:"-3.5506em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])]),s(" , "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"x"),e("mi",null,"q")])]),e("annotation",{encoding:"application/x-tex"},"x_q")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7167em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"x"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])])])])]),s(") at query time in Algorithm 1.")])]),e("li",null,[e("p",null,[s("We would like to remark that Vamana uses "),e("em",null,"full"),s(),e("em",null,"precision coordinates"),s(" when building the graph index, and hence is able to efficiently guide the search towards the right region of the graph, although we use only the compressed data at search time.")]),e("ul",null,[e("li",null,"也就是说在构建图的过程中的 涉及到 search的时候 也是这样的使用pq，然后再进行rerank的时候使用全精度？")])])],-1),K=l('<h3 id="_3-2-diskannindex-layout" tabindex="-1"><a class="header-anchor" href="#_3-2-diskannindex-layout" aria-hidden="true">#</a> 3.2 DiskANNIndex Layout</h3><ul><li>我们将所有数据点的压缩向量存储在内存中，并将图和全精度向量一起存储在固态硬盘上SSD。在磁盘上，对于每个点i，我们都会存储其全精度向量xi，以及其≤R 个邻居的身份信息。如果节点的度数degree小于 R，我们就用0填充，这样计算任意点i 对应的数据在磁盘中的偏移量就非常简单，不需要在内存中存储偏移量了。我们将在下一节解释存储全精度坐标的必要性。</li><li>We store the compressed vectors of all the data points in memory, and store the graph along with the full-precision vectors on the SSD. On the disk, for each point <em>i</em>, we store its full precision vector x<em>i</em> followed by the identities of its <em>≤</em> <em>R</em> neighbors.</li><li>If the degree of a node is smaller than <em>R</em>, we pad with zeros, so that computing the offset within the disk of the data corresponding to any point <em>i</em> is a simple calculation, and does not require storing the offsets in memory. We will explain the need to store full-precision coordinates in the following section.</li></ul><h3 id="_3-3-diskann-beam-search" tabindex="-1"><a class="header-anchor" href="#_3-3-diskann-beam-search" aria-hidden="true">#</a> 3.3 DiskANN Beam Search</h3>',3),Q=e("ul",null,[e("li",null,"搜索给定查询 xq 的邻居的自然方法是运行算法 1，根据需要从 SSD 获取邻居信息 Nout(p∗)。可以使用压缩矢量进行距离计算，以确定从磁盘读取的最佳顶点（和邻域）。 这样做虽然合理，但需要多次往返固态硬盘（耗时数百微秒），从而导致更高的延迟。 为了在不增加过多计算量（距离计算）的情况下减少到 SSD 的往返次数（依次获取邻域），我们会一次性获取 L （或 V）中最接近点的少量邻域 W（比如 4，8），并将 L 更新为 L 中的前 L 个候选点以及在此步骤中获取的所有邻域。请注意，从固态硬盘中获取少量随机扇区所需的时间与获取一个扇区所需的时间几乎相同。我们将这种改进的搜索算法称为BeamSearch。如果W = 1，这个搜索类似于正常的贪婪搜索。注意，如果W太大，比如16或更多，那么计算和SSD带宽都可能被浪费。"),e("li",null,[s("A natural way to search for neighbors of a given query x"),e("em",null,"q"),s(" would be to run Algorithm 1, fetching the neighborhood information "),e("em",null,"N"),s("out("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"p^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8831em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s(" ) from the SSD as needed. Distance calculations to guide the best vertices (and neighborhoods) to read from disk can be done using the compressed vectors. 当进行search的时候，找到了"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"p^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8831em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s(" 那么需要得到"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"p^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8831em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s("的邻居，然后就到ssd中去摘取，可以得到"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"p"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"p^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8831em","vertical-align":"-0.1944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"p"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s("的全精度向量和 其邻居index 但是并没有其邻居的vector，所以使用存在主存中的codes")]),e("li",null,[s("While reasonable, this requires many rountrips to SSD (which take few hundred microseconds) resulting in higher latencies. To reduce the number of round triprs to SSD (to fetch neighborhoods sequentially) without increasing compute (distance calculations) excessively, we fetch the neighborhoods of a small number, "),e("em",null,"W"),s(" (say 4*,* 8), of the closest points in "),e("em",null,"L \\ V"),s(" in one shot, and update "),e("em",null,"L"),s(" to be the top "),e("em",null,"L"),s(" candidates in "),e("em",null,"L"),s(" along with all the neighbors retrieved in this step.")]),e("li",null,[s("Note that fetching a small number of random sectors from an SSD takes almost the same time as one sector. We refer to this modified search algorithm as BeamSearch. If "),e("em",null,"W"),s(" = 1, this search resembles normal greedy search. Note that if "),e("em",null,"W"),s(" is too large, say 16 or more, then both compute and SSD bandwidth could be wasted.这个的意思 是指 再对候选队列里面的点进行获取的时候吗？？？")])],-1),U=l('<figure><img src="'+w+'" alt="figure2" tabindex="0" loading="lazy"><figcaption>figure2</figcaption></figure><figure><img src="'+b+'" alt="figure3" tabindex="0" loading="lazy"><figcaption>figure3</figcaption></figure><ul><li>虽然基于 NAND 闪存的固态硬盘每秒可提供 500K+ 随机读取，但要获得最大读取吞吐量，所有 I/O 请求队列都必须达到饱和状态。然而，在队列积压的情况下以峰值吞吐量运行，会导致磁盘读取延迟超过一毫秒。因此，有必要以较低的负载率运行固态硬盘，以获得较低的搜索延迟。我们发现，以较低的波束宽度（如 W = 2、4、8）运行，可以在延迟和吞吐量之间取得良好的平衡。在这种情况下，固态硬盘的负载率在 30 - 40% 之间，运行搜索算法的每个线程在 I/O 中花费的查询处理时间在 40 - 50% 之间。</li><li>Although NAND-flash based SSDs can serve 500K+ random reads per second, extracting mimaxmum read throughput requires saturating all I/O request queues. However, operating at peak throughput with backlogged queues results in disk read latencies of over a millisecond. Therefore, it is necessary to operate the SSD at a lower load factor to obtain low search latency. We have found that operating at low beam widths (e.g., <em>W</em> = 2*,* 4*,* 8) can strike a good balance between latency and throughput.</li><li>In this setting, the load factor on the SSD is between 30 <em>−</em> 40% and each thread running our search algorithm spends between 40 <em>−</em> 50% of the query processing time in I/O.</li></ul><h3 id="_3-4-diskanncaching-frequently-visited-vertices" tabindex="-1"><a class="header-anchor" href="#_3-4-diskanncaching-frequently-visited-vertices" aria-hidden="true">#</a> 3.4 DiskANNCaching Frequently Visited Vertices</h3><ul><li>为了进一步减少每个查询的磁盘访问次数，我们在动态随机存取存储器（DRAM）中缓存与一部分顶点相关的数据，要么基于已知的查询分布，要么简单地通过缓存所有距离起始点为 C=3 或 4 步的顶点。由于索引图中距离 C 处的节点数量随着 C 呈指数增长，较大的 C 值会导致过大的内存占用。</li><li>To further reduce the number of disk accesses per query, we cache the data associated with a subset of vertices in DRAM, either based on a known query distribution, or simply by caching all vertices that are <em>C</em> = 3 or 4 hops from the starting point <em>s</em>.</li><li>Since the number of nodes in the index graph at distance <em>C</em> grows exponentially with <em>C</em>, larger values of <em>C</em> incur excessively large memory footprint.</li></ul><h3 id="_3-5-diskannimplicit-re-ranking-using-full-precision-vectors" tabindex="-1"><a class="header-anchor" href="#_3-5-diskannimplicit-re-ranking-using-full-precision-vectors" aria-hidden="true">#</a> 3.5 DiskANNImplicit Re-Ranking Using Full-Precision Vectors</h3><ul><li>由于乘积量化是一种有损压缩方法，使用基于乘积量化的近似距离计算出的与查询最接近的 k 个候选者与使用实际距离计算出的之间存在差异。为了弥补这一差距，我们使用为每个点在磁盘上其邻域旁边存储的全精度坐标。实际上，当我们在搜索过程中检索一个点的邻域时，我们也会检索该点的全坐标，而不会产生额外的磁盘读取。这是因为，将 4KB 对齐的磁盘地址读入内存并不比读取 512B 更昂贵，并且一个顶点的邻域（对于 128 度的图为 4*128 字节长）和全精度坐标可以存储在同一个磁盘扇区上。因此，当BeamSearc加载搜索前沿的邻域时，它也可以缓存搜索过程中访问的所有节点的全精度坐标，而无需对固态硬盘进行额外的读取。这使得我们能够根据全精度向量返回前 k 个候选者。与我们的工作无关，在固态硬盘上获取和重新排序存储的全精度坐标的想法也在[24]中使用，但该算法一次性获取所有向量进行重新排序，这将导致一次性进行数百次随机磁盘访问，反过来又会对吞吐量和延迟产生不利影响。我们在第 4.3 节中提供了更详细的解释。在我们的情况下，全精度坐标基本上是附带在扩展邻域的成本上。</li><li>Since Product Quantization is a lossy compression method, there is a discrepancy between the closest <em>k</em> candidates to the query computed using PQ-based approximate distances and using the actual distances. To bridge this gap, we use <em>full-precision coordinates</em> stored for each point next to its neighborhood on the disk.</li><li>In fact, when we retrieve the neighborhood of a point during search, we also retrieve the full coordinates of the point without incurring extra disk reads. This is because, reading 4<em>KB</em>-aligned disk address into memory is no more expensive than reading 512<em>B</em>, and the neighborhood of a vertex (4 * 128 bytes long for degree 128 graphs) and full-precision coordinates can be stored on the same disk sector.</li><li>Hence, as BeamSearch loads neighborhoods of the search frontier, it can also cache full-precision coordinates of all the nodes visited during the search process, using no extra reads to the SSD. This allows us to return the top <em>k</em> candidates based on the full precision vectors. Independent of our work, the idea of fetching and re-ranking full-precision coordinates stored on the SSD is also used in [24], but the algorithm fetches all the vectors to re-rank in one shot, which would result in hundreds of random disk accesses all in one shot, in turn adversely affecting throughput and latency. We provide a more detailed explanation in Section 4.3. In our case, full precision coordinates <em>essentially piggyback</em> on the cost of expanding the neighborhoods.</li></ul><h2 id="_4-evaluation" tabindex="-1"><a class="header-anchor" href="#_4-evaluation" aria-hidden="true">#</a> 4 Evaluation</h2><ul><li><p>现在我们将Vamana与其他用于近似最近邻搜索的相关算法进行比较。首先，对于内存中搜索，我们将我们的算法与 NSG[13]和 HNSW[21]进行比较，它们在大多数公共基准数据集上提供了同类最佳的延迟与召回率权衡。接下来，对于数十亿点的大型数据集，我们将 DiskANN 与基于压缩的技术如 FAISS[18]和 IVF-OADC+G+P[8]进行比较。</p></li><li><p>We now compare Vamana with other relevant algorithms for approximate nearest neighbor search. First, for in-memory search, we compare our algorithm with NSG [13] and HNSW [21], which offer best-in-class latency vs recall on most public benchmark datasets.</p></li><li><p>Next, for large billion point datasets, we compare DiskANN with compression based techniques such as FAISS [18] and IVF-OADC+G+P [8].</p></li><li><p>We use the following two machines for all experiments.</p><ul><li>z840：一款裸金属中端工作站，配备双至强 E5-2620v4 处理器（16 核）、64GB DDR4 内存以及两块三星 960 EVO 1TB 固态硬盘，采用 RAID-0 配置。</li><li>M64-32ms：一台具有双至强 E7-8890v3s（32 个虚拟 CPU）和 1792GB DDR3 内存的虚拟机，我们用它来为十亿点数据集构建一次性的内存索引。</li></ul></li></ul><h3 id="_4-1-comparison-of-hnsw-nsg-and-vamanafor-in-memory-search-performance" tabindex="-1"><a class="header-anchor" href="#_4-1-comparison-of-hnsw-nsg-and-vamanafor-in-memory-search-performance" aria-hidden="true">#</a> 4.1 Comparison of HNSW, NSG and Vamanafor In-Memory Search Performance</h3>',10),X=e("ul",null,[e("li",null,[e("p",null,"我们在三个常用的公共基准上比较了Vamana与 HNSW 和 NSG：SIFT1M（128 维）和 GIST1M（960 维），这两个都是图像描述符的百万点数据集[1]，以及 DEEP1M（96 维），这是 DEEP1B 的一个随机的一百万大小样本，DEEP1B 是一个机器学习得到的十亿个向量的集合[6]。对于所有这三种算法，我们进行了参数扫描，并为最佳召回率与延迟的权衡选择了接近最优的参数选择。所有 HNSW 索引都是使用 M=128，efC=512 构建的，而Vamana索引使用 R=70，L=75，α=1.2。对于 SIFT1M 和 GIST1M 上的 NSG，由于其出色的性能，我们使用了他们存储库中列出的参数 5，对于 DEEP1M，我们使用 R=60，L=70，C=500。此外，由于这项工作的主要重点是基于固态硬盘的搜索，我们没有实现我们自己的内存中搜索算法来测试Vamana。相反，我们只是在Vamana生成的索引上使用了 NSG 存储库中优化后的搜索算法的实现。从图 3 中，我们可以看到一个明显的趋势——在所有情况下，NSG 和Vamana的性能都优于 HNSW，并且在 960 维的 GIST1M 数据集上，Vamana的性能优于 NSG 和 HNSW 两者。此外，在所有三个实验中，Vamana的索引构建时间都优于 HNSW 和 NSG。例如，在 z840 上对 DEEP1M 进行索引构建时，Vamana、HNSW 和 NSG6 的总索引构建时间分别为 129 秒、219 秒和 480 秒。从这些实验中，我们得出结论，Vamana在从不同来源获得的数百维和数千维数据集上与当前最好的近似最近邻方法相匹配，有时甚至表现更优。")]),e("li",null,[e("p",null,"We compared Vamana with HNSW and NSG on three commonly used public benchmarks: SIFT1M (128-dimensions) and GIST1M (960-dimensions), both of which are million point datasets of image descriptors [1], and DEEP1M (96-dimensions), a random one million size sample of DEEP1B, a machine-learned set of one billion vectors [6].")]),e("li",null,[e("p",null,"For all three algorithms, we did a parameter sweep and selected near-optimal choice of parameters for the best recall vs latency trade-off.")]),e("li",null,[e("p",null,[s("All HNSW indices were constructed using "),e("em",null,"M"),s(" = 128, "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"e"),e("msub",null,[e("mi",null,"f"),e("mi",null,"C")])]),e("annotation",{encoding:"application/x-tex"},"ef_C")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal"},"e"),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.3283em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.1076em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"C")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])])])])]),s(" = 512, while Vamana indices used "),e("em",null,"R"),s(" = 70*, L* = 75*, α* = 1*."),e("em",null,[s("2. For NSG on SIFT1M and GIST1M, we use the parameters listed on their repository due to their excellent performance, and used "),e("em",null,"R"),s(" = 60")]),s(", L* = 70*, C* = 500 for DEEP1M.")])]),e("li",null,[e("p",null,"Moreover, since the main focus of this work is on the SSD-based search, we did not implement our own in-mmeory search"),e("p",null,"algorithm to test Vamana. Instead, we simply used the implementation of the optimized search algorithm on the NSG repository, on the indices generated by Vamana. 这里就是说在测试vamana的时候在对比内存的算法的时候，其建图的时候以及搜索的时候涉及到的search都是用到的nsg的，并且可能建图的时候就是使用了 选边策略不一样；")]),e("li",null,[e("p",null,"From Figure 3, we can see one clear trend – NSG and Vamana out-perform HNSW in all instances, and on the 960-dimensional"),e("p",null,"GIST1M dataset, Vamana outperforms both NSG and HNSW. Moreover, the indexing time of Vamana was better than both HNSW and NSG in all three experiments.")]),e("li",null,[e("p",null,"For example, when indexing DEEP1M on z840, the total index construction times were 129s, 219s, and 480s for Vamana, HNSW"),e("p",null,"and NSG6 respectively. From these experiments, we conclude that Vamana matches, and sometimes outperforms, the current best ANNS methods on both hundred and thousand-dimensional datasets obtained from different sources.")])],-1),Z=l('<h3 id="_4-2-comparison-of-hnsw-nsg-andvamanafor-number-of-hops" tabindex="-1"><a class="header-anchor" href="#_4-2-comparison-of-hnsw-nsg-andvamanafor-number-of-hops" aria-hidden="true">#</a> 4.2 Comparison of HNSW, NSG andVamanafor Number of Hops</h3><ul><li>Vamana比其他基于图的算法更适合基于固态硬盘的服务，因为与 HNSW 和 NSG 相比，在大型数据集中，它为了使搜索收敛而进行的跳数要少 2 到 3 倍。所谓跳数，我们指的是在搜索关键路径上进行磁盘读取的轮数。在BeamSearch中，它映射为通过进行 W 次并行磁盘读取来扩展搜索前沿的次数。跳数很重要，因为它直接影响搜索延迟。对于 HNSW，我们假设除了基础层之外的所有层中的节点都缓存在动态随机存取存储器（DRAM）中，并且只计算基础层图上的跳数。对于 NSG 和Vamana索引，我们假设围绕导航节点的前 3 层广度优先搜索（BFS）级别可以缓存在 DRAM 中。在图 2(c)中，我们通过改变最大图度数来比较达到目标 5-recall@5为 98%所需的跳数。我们注意到 HNSW 和 NSG 都有停滞趋势，而Vamana随着最大度数和α的增加，跳数减少。使用较大的 R 和α值会由于它们能够添加更多远程边而导致跳数减少。因此，我们推断Vamana在α&gt;1 时比其他基于图的方法能更好地利用固态硬盘提供的高容量。</li><li>Vamana is more suitable for SSD-based serving than other graph-based algorithms as it makes 2 <em>−</em> 3 times fewer hops for search to converge on large datasets compared to HNSW and NSG.</li><li>By hops, we refer to the number of rounds of disk reads on the critical path of the search. In BeamSearch, it maps to the number of times the search frontier is expanded by making <em>W</em> parallel disk reads. The number of hops is important as it directly affects search latency. For HNSW, we assume nodes in all levels excluding the base level are cached in DRAM and only count the number of hops on base-level graph.</li><li>For NSG and Vamana indices, we assume that the first 3 BFS levels around the navigating node(s) can be cached in DRAM. We compare the number of hops required to achieve a target 5-recall@5 of 98% by varying the maximum graph degrees in Figure 2(c). We notice a stagnation trend for both HNSW and NSG, while Vamana shows a reduction in number of hops with increasing max degree and <em>α</em>. Using large values of <em>R</em> and <em>α</em> result in fewer hops due to their ability to add more long-range edges. We thus infer that Vamana with <em>α &gt;</em> 1 makes better use of the high capacity offered by SSDs than other graph-based methods.</li></ul><h3 id="_4-3-comparison-on-billion-scale-datasets-one-shotvamanavs-merged-vamana" tabindex="-1"><a class="header-anchor" href="#_4-3-comparison-on-billion-scale-datasets-one-shotvamanavs-merged-vamana" aria-hidden="true">#</a> 4.3 Comparison on Billion-Scale Datasets: One-ShotVamanavs Merged Vamana</h3>',3),j=e("ul",null,[e("li",null,[e("p",null,"对于我们接下来的一组实验，我们将评估重点放在拥有 10^9 个点的 ANN_SIFT1B[1]大型数据集上，该数据集是 128 个无符号 8 位整数类型的 SIFT 图像描述符。为了展示在第 3 节中描述的合并Vamana方案的有效性，我们使用我们的Vamana构建了两个索引。第一个是在整个十亿点数据集上具有 L=50、R=128、α=1.2 的单一索引。这个过程在 M64-32ms 上大约需要 2 天，峰值内存使用量约为 1100GB，并生成了一个平均度数为 113.9 的索引。第二个是合并索引，其构建方式如下：(1)使用 k -means聚类将数据集划分为 k=40 个分片，(2)将数据集中的每个点发送到距离最近的 2 个分片，(3)为每个分片构建具有 L=50、R=64、α=1.2 的索引，(4)合并所有图的边集。结果是一个 348GB 的索引，平均度数为 92.1。这些索引是在 z840 上构建的，整个过程大约需要 5 天，内存使用量在整个过程中保持在 64GB 以下。分片和合并速度很快，并且可以在同一台机器上从磁盘进行。")]),e("li",null,[e("p",null,[s("For our next set of experiments, we focus our evaluations on the "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mn",null,"1"),e("msup",null,[e("mn",null,"0"),e("mn",null,"9")])]),e("annotation",{encoding:"application/x-tex"},"10^9")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},"1"),e("span",{class:"mord"},[e("span",{class:"mord"},"0"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"9")])])])])])])])])])]),s(" point ANN_SIFT1B [1] "),e("em",null,"bigann"),s(" dataset of SIFT image descriptors of 128 uint8s. To demonstrate the effectiveness of the merged Vamana scheme described in Section 3, we built two indices using our Vamana.")])]),e("li",null,[e("p",null,[s("The first is a "),e("strong",null,"single"),s(),e("strong",null,"index"),s(" with "),e("em",null,"L"),s(" = 50*, R* = 128*, α* = 1*."),e("em",null,[s("2 on the full billion-point dataset. This procedure takes about 2 days on M64-32 with a peak memory usage at "),e("em",null,"≈"),s("1100GB, and generates an index with an average degree of 113")]),s(".*9.")])]),e("li",null,[e("p",null,[s("The second is the "),e("strong",null,"merged"),s(" index, which is constructed as follows:")]),e("ul",null,[e("li",null,[s("(1) partition the dataset into "),e("em",null,"k"),s(" = 40 shards using k-means clustering,")]),e("li",null,[s("(2) send each point in the dataset to the "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"l"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"l^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s("= 2closest shards,")]),e("li",null,[s("(3) build indices for each shard with "),e("em",null,"L"),s(" = 50*, R* = 64*, α* = 1*.*2, and")]),e("li",null,"(4) merge the edge sets of all the graphs. The result is a 348GB index with an average degree of 92*.*1. The indices were built on z480 and took about 5 days with memory usage remaining under 64GB for the entire process. Sharding and merging are fast and can be done on the same machine from the disk.")])]),e("li",null,[e("p",null,[s("在图 2(a)中，我们针对这两种配置比较了1-recall@1 与延迟以及 10,000 个查询大型数据集。从这个实验我们得出以下结论。(a)单一索引的性能优于合并索引，因为合并索引要遍历更多链接才能到达相同的邻域，从而增加了搜索延迟。这可能是因为合并索引中每个节点的入边和出边被限制在大约所有点的"),e("code",null,"l/k=5%。(b)对于十亿规模的 k-最近邻索引编制和服务单个节点，合并索引仍然是一个非常好的选择，它轻松超越了现有的最先进方法，并且与单一索引相比，在目标召回率方面所需的额外延迟不超过 20%。另一方面，单一索引在延迟小于 5 毫秒的情况下实现了新的最先进的 1 召回率@1 为 98.68%。合并索引对于 DEEP1B 数据集也是一个不错的选择。图 2(b)展示了在 z840 机器上使用 k=40 个分片和"),s("l=2 构建的 DEEP1B 数据集的合并 DiskANN 索引的召回率与延迟曲线。")])]),e("li",null,[e("p",null,[s("We compare 1-recall@1 vs latency with the 10,000 query "),e("em",null,"bigann"),s(" dataset for both configurations in Figure 2(a). From this experiment we conclude the following.")]),e("ul",null,[e("li",null,[s("(a) The "),e("strong",null,"single"),s(" index outperforms the "),e("strong",null,"merged"),s(" index, which traverses more links to reach the same neighborhood, thus increasing search latency. This could possibly be because the in- and out-edges of each node in the "),e("strong",null,"merged"),s(" index are limited to about "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"l"),e("mo",null,"∗")]),e("mi",{mathvariant:"normal"},"/"),e("mi",null,"k")]),e("annotation",{encoding:"application/x-tex"},"l^* / k")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])]),e("span",{class:"mord"},"/"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k")])])]),s(" = 5% of all points.")]),e("li",null,[s("(b) The "),e("strong",null,"merged"),s(" index is still a very good choice for billion scale k-ANN indexing and serving single-node, easily outperforming the existing state-of-the-art methods and requires no more than 20% extra latency for a target recall when compared to the "),e("strong",null,"single"),s(" index. The "),e("strong",null,"single"),s(" index, on the other hand, achieves a new state-of-the-art 1-recall@1 of 98*.*68% with <5 milliseconds latency. The merged index is also a good choice for the DEEP1B dataset. Figure 2(b) shows the recall vs latency curve of the merged DiskANN index for the DEEP1B dataset built using "),e("em",null,"k"),s(" = 40 shards and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"l"),e("mo",null,"∗")])]),e("annotation",{encoding:"application/x-tex"},"l^*")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.6887em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mbin mtight"},"∗")])])])])])])])])])]),s("= 2 on the z480 machine.")])])])],-1),$=e("h3",{id:"_4-4-comparison-on-billion-scale-datasets-diskann-vs-ivf-based-methods",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_4-4-comparison-on-billion-scale-datasets-diskann-vs-ivf-based-methods","aria-hidden":"true"},"#"),s(" 4.4 Comparison on Billion-Scale Datasets: DiskANN vs IVF-based Methods")],-1),Y=e("ul",null,[e("li",null,[e("p",null,"我们最后的比较是FAISS[18]和IVFOADC+G+P[7]，这是两种最近在单个节点上构建十亿个点指数的方法。这两种方法都利用倒置索引和基于产品量化(PQ)的压缩方案来开发具有低内存占用的索引，可以服务于具有高吞吐量和良好的1-recall@100的查询。我们将DiskANN与仅IVFOADC+G+P进行比较，因为[7]对IVFOADC+G+P的召回率优于FAISS，而且，此外，使用 FAISS 进行十亿规模的索引需要 GPU，而某些平台可能没有 GPU。")]),e("li",null,[e("p",null,"Our final comparisons are with FAISS[18] and IVFOADC+G+P[7], two recent approaches to constructing billion point indices on a single node. Both methods utilize Inverted Indexing and Product Quantization-based compression schemes to develop indices with low-memory footprint that can serve queries with high-throughput and good 1-recall@100. We compare DiskANN with only IVFOADC+G+P since [7] demonstrates superior recall for IVFOADC+G+P over FAISS, and moreover, billion-scale indexing using FAISS requires GPUs that might not be available in some platforms.")]),e("li",null,[e("p",null,"IVFOADC+G+P 使用 HNSW 作为路由层来获得一小组簇，这些簇使用一种新颖的分组和修剪策略进一步细化。使用他们的开源代码，我们在 SIFT1B 基础数据集上构建具有 16 字节和 32 字节 OPQ 码本的索引。图 2(a)中的 IVFOADC+G+P-16 和 IVFOADC+G+P-32 曲线代表这两种配置。虽然 IVFOADC+G+P-16 在 1-recall@1达到 37.04%时趋于平稳，但较大的 IVFOADC+G+P-32 索引在 1-recall@1 达到 62.74%。在与 IVFOADC+G+P-32 相同的内存占用情况下，DiskANN 在完美的1-recall@1 达到 100%时达到饱和，同时在不到 3.5 毫秒内提供超过 95%的 1-recall@1。因此，DiskANN 在与基于压缩的方法匹配内存占用的同时，能够在相同延迟下实现显著更高的召回率。基于压缩的方法由于对坐标进行有损压缩导致精度损失，从而导致距离计算略有不准确，因此提供较低的召回率。")]),e("li",null,[e("p",null,[s("IVFOADC+G+P uses HNSW as a "),e("em",null,"routing"),s(" layer to obtain a small set of clusters that are further refined using a novel grouping and pruning strategy. Using their open-source code, we build indices with 16 and 32-byte OPQ code-books on the SIFT1B base set. IVFOADC+G+P-16 and IVFOADC+G+P-32 curves in 2(a) represent the two configurations.")])]),e("li",null,[e("p",null,[s("While IVFOADC+G+P-16 plateaus at 1-recall@1 of 37*."),e("em",null,"04%, the larger IVFOADC+G+P-32 indices reach 1-recall@1 at 62"),s("."),e("em",null,"74%. With the same memory footprint as IVFOADC+G+P-32, DiskANN saturates at a perfect 1-recall@1 of 100%, while providing 1-recall@1 of above 95% in under 3"),s(".*5ms. Thus DiskANN, while matching the memory footprint of compression-based methods, can achieve significantly higher recall at the same latency. Compression-based methods provide low recall due to loss of precision from lossy")]),e("p",null,"compression of coordinates which results in slightly inaccurate distance calculations.")]),e("li",null,[e("p",null,"Zoom[24]是一种基于压缩的方法，与 IVFOADC+G+P 类似，它利用压缩向量来确定近似的最近的 K'（K'＞K）个候选者，然后通过从磁盘获取全精度坐标对它们进行重新排序，以输出最终的 K 个候选者集合。然而，Zoom 存在两个缺点：（a）它通过同时进行随机磁盘读取来获取所有的 K'（即使 K=1，通常也接近一百个）全精度向量，这会影响延迟和吞吐量；（b）它需要使用成百上千个质心进行昂贵的 k-means聚类来构建基于 HNSW 的路由层。例如，[24]中描述的在 1000 万基础数据集上的聚类步骤使用了 20 万个质心，并且可能不容易扩展到数十亿个点的数据集。")]),e("li",null,[e("p",null,[s("Zoom[24] is a compression-based method, similar to IVFOADC+G+P, that identifies the approximate nearest "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"K"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"K'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7519em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(),e("em",null,"> K"),s(" candidates using the compressed vectors, and re-ranks them by fetching the full precision coordinates from the disk to output the final set of "),e("em",null,"K"),s(" candidates. However, Zoom suffers from two drawbacks:")]),e("ul",null,[e("li",null,[s("(a) it fetches all the "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"K"),e("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")])]),e("annotation",{encoding:"application/x-tex"},"K'")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7519em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.7519em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"′")])])])])])])])])])])]),s(" (often close to hundred even if "),e("em",null,"K"),s(" = 1) full-precision vectors using simultaneous random disk reads, which would affect latency and throughput, and")]),e("li",null,[s("(b) it requires expensive "),e("em",null,"k"),s("-means clustering using hundreds of thousands of centroids to build the HNSW-based routing layer. For example, the clustering step described in [24] utilizes 200K centroids on 10M base set, and might not scale easily to billion-point datasets.")])])])],-1),J=l('<h2 id="_5-conclusion" tabindex="-1"><a class="header-anchor" href="#_5-conclusion" aria-hidden="true">#</a> 5 Conclusion</h2><ul><li><p>仅仅使用了64GB主存 就进行了 10亿点的 index</p></li><li><p>我们提出并评估了一种新的基于图的索引算法，称为Vamana，其索引可与目前最先进(state-of-the-art)的内存的(in-memory)搜索方法在高召回率相媲美。此外，我们演示了在仅使用64GB主存的10亿点数据集上构建一个高质量的SSD驻留索引DiskANN。我们详细介绍并激励了算法的改进，使我们能够使用廉价的零售级SSD，以几毫秒的延迟为这些指数提供服务。通过将基于图的方法的高召回率、低延迟特性与基于压缩的方法的内存效率和可伸缩性特性相结合，我们建立了新的最先进的索引构建和服务的十亿点数据集。</p></li><li><p>We presented and evaluated a new graph-based indexing algorithm called Vamana for ANNS whose indices are comparable to the current state-of-the-art methods for in-memory search in high recall regimes. In addition, we demonstrated the construction of a high-quality SSD-resident index DiskANN on a billion point dataset using only 64GB of main memory.</p><ul><li>10亿级别的数据仅仅使用了 64GB；</li></ul></li><li><p>We detailed and motivated the algorithmic improvements that enabled us to serve these indices using inexpensive retail-grade SSDs with latencies of few milliseconds. By combining the high-recall, low-latency properties of graph-based methods with the memory efficiency and scalability properties of compression-based methods, we established the new state-of-the-art for indexing and serving billion point datasets.</p></li></ul>',2);function ee(se,ae){const i=r("ExternalLinkIcon"),t=r("router-link");return m(),h("div",null,[y,x,p(" more "),e("div",S,[k,e("ul",null,[N,_,G,e("li",null,[e("p",null,[s("code： "),e("a",M,[s("https://github.com/microsoft/DiskANN"),a(i)])])]),D,e("li",null,[e("p",null,[e("a",A,[s("https://www.zhihu.com/search?type=content&q=diskANN"),a(i)])])]),e("li",null,[e("p",null,[e("a",V,[s("https://blog.csdn.net/whenever5225/article/details/106863674"),a(i)])])]),e("li",null,[e("p",null,[e("a",z,[s("https://blog.csdn.net/weixin_44839084/article/details/119217569"),a(i)])])]),e("li",null,[e("p",null,[e("a",P,[s("https://blog.csdn.net/weixin_44839084/article/details/129679691"),a(i)])])])])]),e("nav",q,[e("ul",null,[e("li",null,[a(t,{to:"#no-0-abstract"},{default:n(()=>[s("No.0 Abstract")]),_:1})]),e("li",null,[a(t,{to:"#no-1-introduction"},{default:n(()=>[s("No.1 Introduction")]),_:1})]),e("li",null,[a(t,{to:"#再重新细节看看看不懂"},{default:n(()=>[s("再重新细节看看看不懂")]),_:1}),e("ul",null,[e("li",null,[a(t,{to:"#_1-1-our-technical-contribution"},{default:n(()=>[s("1.1 Our technical contribution")]),_:1})]),e("li",null,[a(t,{to:"#_1-2-notation"},{default:n(()=>[s("1.2 Notation")]),_:1})]),e("li",null,[a(t,{to:"#_1-3-paper-outline"},{default:n(()=>[s("1.3 Paper Outline")]),_:1})])])]),e("li",null,[a(t,{to:"#no-2-the-vamana-graph-construction-algorithm"},{default:n(()=>[s("No.2 The Vamana Graph Construction Algorithm")]),_:1}),e("ul",null,[e("li",null,[a(t,{to:"#_2-1-relative-neighborhood-graphs-and-the-greedysearch-algorithm"},{default:n(()=>[s("2.1 Relative Neighborhood Graphs and the GreedySearch algorithm")]),_:1})]),e("li",null,[a(t,{to:"#_2-2-the-robust-pruning-procedure"},{default:n(()=>[s("2.2 The Robust Pruning Procedure")]),_:1})]),e("li",null,[a(t,{to:"#_2-3-the-vamana-indexing-algorithm"},{default:n(()=>[s("2.3 The Vamana Indexing Algorithm")]),_:1})]),e("li",null,[a(t,{to:"#_2-4-comparison-ofvamanawith-hnsw-21-and-nsg-13"},{default:n(()=>[s("2.4 Comparison ofVamanawith HNSW [21] and NSG [13]")]),_:1})])])]),e("li",null,[a(t,{to:"#_3-diskann-constructing-ssd-resident-indices"},{default:n(()=>[s("3 DiskANN: Constructing SSD-Resident Indices")]),_:1}),e("ul",null,[e("li",null,[a(t,{to:"#_3-1-thediskannindex-design"},{default:n(()=>[s("3.1 TheDiskANNIndex Design")]),_:1})]),e("li",null,[a(t,{to:"#_3-2-diskannindex-layout"},{default:n(()=>[s("3.2 DiskANNIndex Layout")]),_:1})]),e("li",null,[a(t,{to:"#_3-3-diskann-beam-search"},{default:n(()=>[s("3.3 DiskANN Beam Search")]),_:1})]),e("li",null,[a(t,{to:"#_3-4-diskanncaching-frequently-visited-vertices"},{default:n(()=>[s("3.4 DiskANNCaching Frequently Visited Vertices")]),_:1})]),e("li",null,[a(t,{to:"#_3-5-diskannimplicit-re-ranking-using-full-precision-vectors"},{default:n(()=>[s("3.5 DiskANNImplicit Re-Ranking Using Full-Precision Vectors")]),_:1})])])]),e("li",null,[a(t,{to:"#_4-evaluation"},{default:n(()=>[s("4 Evaluation")]),_:1}),e("ul",null,[e("li",null,[a(t,{to:"#_4-1-comparison-of-hnsw-nsg-and-vamanafor-in-memory-search-performance"},{default:n(()=>[s("4.1 Comparison of HNSW, NSG and Vamanafor In-Memory Search Performance")]),_:1})]),e("li",null,[a(t,{to:"#_4-2-comparison-of-hnsw-nsg-andvamanafor-number-of-hops"},{default:n(()=>[s("4.2 Comparison of HNSW, NSG andVamanafor Number of Hops")]),_:1})]),e("li",null,[a(t,{to:"#_4-3-comparison-on-billion-scale-datasets-one-shotvamanavs-merged-vamana"},{default:n(()=>[s("4.3 Comparison on Billion-Scale Datasets: One-ShotVamanavs Merged Vamana")]),_:1})]),e("li",null,[a(t,{to:"#_4-4-comparison-on-billion-scale-datasets-diskann-vs-ivf-based-methods"},{default:n(()=>[s("4.4 Comparison on Billion-Scale Datasets: DiskANN vs IVF-based Methods")]),_:1})])])]),e("li",null,[a(t,{to:"#_5-conclusion"},{default:n(()=>[s("5 Conclusion")]),_:1})])])]),I,W,F,L,T,R,H,C,B,O,E,K,Q,U,X,Z,j,$,Y,J])}const le=o(v,[["render",ee],["__file","diskann.html.vue"]]);export{le as default};
