import{_ as r}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as k,c as d,d as m,a as n,e as a,w as p,b as s,f as o}from"./app-2a2d189a.js";const b="/assets/figure9-1-365ec61a.png",v="/assets/figure9-2-83bbc0a3.png",P="/assets/figure9-3-c671ee31.png",_="/assets/figure9-4-b21b956a.png",g="/assets/figure9-5-93e21589.png",h="/assets/figure9-6-b4b27d81.png",M="/assets/figure9-7-8cf13d04.png",f="/assets/figure9-8-6a8e9e74.png",U="/assets/figure9-9-42731c7c.png",G="/assets/figure9-10-ffe3db2c.png",y="/assets/figure9-11-95a77535.png",I="/assets/figure9-12-afb54ec8.png",D="/assets/figure9-13-31fda0d6.png",C={},A=n("h1",{id:"i-第九章",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#i-第九章","aria-hidden":"true"},"#"),s(" I-第九章")],-1),w=n("p",null,"I-第九章",-1),B={class:"table-of-contents"},x=o('<h2 id="简单介绍主要是基础" tabindex="-1"><a class="header-anchor" href="#简单介绍主要是基础" aria-hidden="true">#</a> 简单介绍主要是基础</h2><div class="hint-container info"><p class="hint-container-title">说明</p><p>主要是各种搜索找的学习；</p><p>主题：CUDA核心GPU编程</p><p>前置条件：</p><ul><li>应具备C++编程知识</li><li>需理解内存管理，如malloc和free</li><li>理解STL及其模板机制</li><li>需配置NVIDIA显卡，型号需为900系列或更高</li><li>所用扩展要求版本为11或更新</li><li>编译器版本不低于11</li><li>CMake版本需在3.18以上</li></ul></div><h2 id="第9章-多gpu编程" tabindex="-1"><a class="header-anchor" href="#第9章-多gpu编程" aria-hidden="true">#</a> 第9章 多GPU编程</h2><ul><li><p>本章内容：<br> ·多GPU管理<br> ·跨多GPU执行核函数<br> ·GPU间的叠加计算和通信<br> ·GPU间的同步<br> ·使用CUDA-aware MPI交换数据<br> ·使用GPUDirect RDMA的CUDA-aware MPI交换数据<br> ·跨GPU加速集群扩展应用程序<br> ·理解CPU和GPU的亲和性</p></li><li><p>到目前为止，本书中的大部分示例使用的都是单一的GPU。在本章中，会介绍多GPU<br> 编程的内容：在一个计算节点内或者跨多个GPU加速节点实现跨GPU扩展应用。CUDA提<br> 供了大量实现多GPU编程的功能，包括：在一个或多个进程中管理多设备，使用统一的虚<br> 拟寻址（Unified Virtual Addressing，UVA）直接访问其他设备内存，GPUDirect，以及使用<br> 流和异步函数实现的多设备计算通信重叠。在本章中需要掌握的内容有以下几个方面：<br> ·在多GPU上管理和执行内核<br> ·跨GPU的重叠计算和通信<br> ·使用流和事件实现多GPU同步执行<br> ·在GPU加速集群上扩展CUDA-aware MPI应用程序</p></li><li><p>通过几个例子，理解应用程序在多个设备上执行时近线性可伸缩性的实现</p></li></ul><h2 id="_9-1-从一个gpu到多gpu" tabindex="-1"><a class="header-anchor" href="#_9-1-从一个gpu到多gpu" aria-hidden="true">#</a> 9.1 从一个GPU到多GPU</h2><ul><li><p>在应用程序中添加对多GPU的支持，其最常见的原因是以下几个方面：<br> ·问题域的大小：现有的数据集太大，单GPU内存大小与其不相符合<br> ·吞吐量和效率：如果单GPU适合处理单任务，那么可以通过使用多GPU并发地处理<br> 多任务来增加应用程序的吞吐量</p></li><li><p>在多GPU系统中，允许分摊跨GPU的服务器节点的功率消耗，具体方式是为给定的功<br> 率消耗单元提供更多的性能，同时提高吞吐量。</p></li><li><p>当使用多GPU运行应用程序时，需要正确设计GPU间的通信。GPU间数据传输的效率<br> 取决于GPU是如何连接在一个节点上并跨集群的。在多GPU系统里有两种连接方式：</p></li></ul><p>·多GPU通过单个节点连接到PCIe总线上<br> ·多GPU连接到集群中的网络交换机上</p><ul><li><p>这些连接拓扑结构不是互斥的。图9-1展示了一个集群的简化拓扑结构，它其中有两<br> 个计算节点。GPU0和GPU1通过PCIe总线连接到node0上。同样，GPU2和GPU3在通过PCIe<br> 总线连接到node1上。两个节点（node0和node1）通过Infiniband交换机互相连接。<br><img src="'+b+`" alt="figure9-1" loading="lazy"></p></li><li><p>每个节点可能包括以下内容中的一个或多个：通过CPU插槽和主机芯片连接的多个<br> CPU，主机DRAM，本地存储设备，网络主机卡适配器（HCA），板载网络和USB端口，<br> 以及连接多个GPU的PCIe交换机。系统可能有一个PCIe根节点和多个PCIe交换机，这些<br> PCIe交换机连接到根节点上，并在一个树结构中连接GPU。因为PCIe链路是双工的，所以<br> 可以使用CUDA API在PCIe链路之间映射一条路径，以避免总线竞争，同时也可以在GPU<br> 间共享数据。</p></li><li><p>为了设计一个利用多GPU的程序，需要跨设备分配工作负载。根据应用程序，这种分<br> 配会导致两种常见的GPU间通信模式：<br> ·问题分区之间没有必要进行数据交换，因此在各GPU间没有数据共享<br> ·问题分区之间有部分数据交换，在各GPU间需要冗余数据存储</p></li><li><p>第一种模式是最基本的情况：每个问题分区可以在不同的GPU上独立运行。要处理这<br> 些情况，只需了解如何在多个设备中传输数据及调用内核。在第二种情况下，GPU之间的<br> 数据交换是必需的，必须考虑数据如何在设备之间实现最优移动。总之，要避免通过主机<br> 内存中转数据（即数据复制到主机，只能将它复制到另一个GPU上）。重要的是要注意有<br> 多少数据被传输了和发生了多少次传输。</p></li><li><p>第二种情况回想起了第5章中1D模板示例的halo区域的概念。halo区域指的是输入数<br> 据必须被一个问题的子集所访问，但它对于该子集不产生输出。通常情况下，与内部数据<br> 子集（内部区域）相比，问题分区周围的halo区域相对较小，不需要进行交换。根据交换<br> halo区域的通信代价和计算内部区域需要的时间，两者之间的重叠可能会减少多GPU系统<br> 中的开销。这类似于在主机和设备之间的PCIe总线上隐藏数据传输开销的技术。</p></li></ul><h3 id="_9-1-1-在多gpu上执行" tabindex="-1"><a class="header-anchor" href="#_9-1-1-在多gpu上执行" aria-hidden="true">#</a> 9.1.1 在多GPU上执行</h3><ul><li><p>CUDA 4.0中增加的功能使CUDA程序员能更容易地使用多GPU。CUDA运行时API支持<br> 在多GPU系统中管理设备和执行内核的多种方式。</p></li><li><p>单个主机线程可以管理多个设备。一般来说，第一步是确定系统内可用的使能CUDA<br> 设备的数量，使用如下函数获得：<br><code>cudaError_t cudaGetDeviceCount(int* count);</code></p></li><li><p>该函数返回计算能力为1.0或更高的设备数量。下面的代码说明了如何确定使能CUDA<br> 设备的数量，对其进行遍历，并查询性能。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> ngpus<span class="token punctuation">;</span>
<span class="token function">cudaGetDeviceCount</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>ngpus<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 cudaDeviceProp devProp<span class="token punctuation">;</span>
 <span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>devProp<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Device %d has compute capability %d.%d.\\n&quot;</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> devProp<span class="token punctuation">.</span>major<span class="token punctuation">,</span>
 devProp<span class="token punctuation">.</span>minor<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>在利用与多GPU一起工作的CUDA应用程序时，必须显式地指定哪个GPU是当前所有<br> CUDA运算的目标。使用以下函数设置当前设备：<br><code>cudaError_t cudaSetDevice(int id);</code></p></li><li><p>该函数将具有标识符id的设备设置为当前设备。该函数不会与其他设备同步，因此是<br> 一个低开销的调用。使用此函数，可以在任何时间从任何主机线程中选择任何设备。有效<br> 的设备标识符范围是从0到ngpus-1。如果在首个CUDA API调用发生之前，没有显式地调<br> 用cudaSetDevice函数，那么当前设备会被自动设置设备0。</p></li><li><p>一旦选定了当前设备，所有的CUDA运算将被应用到那个设备上：<br> ·任何从主线程中分配来的设备内存将完全地常驻于该设备上<br> ·任何由CUDA运行时函数分配的主机内存都会有与该设备相关的生存时间<br> ·任何由主机线程创建的流或事件都会与该设备相关<br> ·任何由主机线程启动的内核都会在该设备上执行</p></li><li><p>可以在以下情况中同时使用多GPU：<br> ·在一个节点的单CPU线程上<br> ·在一个节点的多CPU线程上<br> ·在一个节点的多CPU进程上<br> ·在多个节点的多CPU进程上</p></li><li><p>下面的代码准确展示了如何执行内核和在单一的主机线程中进行内存拷贝，使用循环<br> 遍历设备：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// set the current device</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token comment">// execute kernel on current device</span>
 kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// asynchronously transfer data between the host and current device</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>因为循环中的内核启动和数据传输是异步的，因此在每次调用操作后控制将很快返回<br> 到主机线程。但是，即使内核或由当前线程发出的传输仍然在当前设备上执行时，也可以<br> 安全地转变设备，因为cudaSetDevice函数不会导致主机同步。</p></li><li><p>总之，想要在单一节点内获取GPU的数量和它们的性能，可以使用下述函数：<br><code>cudaError_t cudaGetDeviceCount(int *count);</code><br><code>cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp *prop, int device);</code></p></li><li><p>然后可以使用下述函数设置当前设备：<code>cudaError_t cudaSetDevice(int device);</code></p></li><li><p>一旦设置好了当前设备，所有CUDA操作都会在那个设备的上下文发出。然后当前被<br> 选择的设备可以用本书中提供的GPU编程方式来使用。</p></li></ul><h3 id="_9-1-2-点对点通信" tabindex="-1"><a class="header-anchor" href="#_9-1-2-点对点通信" aria-hidden="true">#</a> 9.1.2 点对点通信</h3><ul><li>在计算能力为2.0或以上的设备中，在64位应用程序上执行的内核，可以直接访问任<br> 何GPU的全局内存，这些GPU连接到同一个PCIe根节点上。如果想这样操作，必须使用<br> CUDA点对点（P2P）API来实现设备间的直接通信。点对点通信需要CUDA 4.0或更高版<br> 本，相应的GPU驱动器，以及一个具有两个或两个以上连接到同一个PCIe根节点上的<br> Fermi或Kepler GPU系统。有两个由CUDA P2P API支持的模式，它们允许GPU之间直接通<br> 信：</li></ul><p>·点对点访问：在CUDA内核和GPU间直接加载和存储地址<br> ·点对点传输：在GPU间直接复制数据</p><ul><li>在一个系统内，如果两个GPU连接到不同的PCIe根节点上，那么不允许直接进行点对<br> 点访问，并且CUDA P2P API将会通知你。仍然可以使用CUDA P2P API在这些设备之间进<br> 行点对点传输，但是驱动器将通过主机内存透明地传输数据，而不是通过PCIe总线直接传<br> 输数据。</li></ul><h4 id="_9-1-2-1-启用点对点访问" tabindex="-1"><a class="header-anchor" href="#_9-1-2-1-启用点对点访问" aria-hidden="true">#</a> 9.1.2.1 启用点对点访问</h4><ul><li><p>点对点访问允许各GPU连接到同一个PCIe根节点上，使其直接引用存储在其他GPU设<br> 备内存上的数据。对于透明的内核，引用的数据将通过PCIe总线传输到请求的线程上</p></li><li><p>因为不是所有的GPU都支持点对点访问，所以需要使用下述函数显式地检查设备是否<br> 支持P2P：<code>cudaError_t cudaDeviceCanAccessPeer(int* canAccessPeer, int device, int peerDevice);</code></p></li><li><p>如果设备device能够直接访问对等设备peerDevice的全局内存，那么函数变量can-<br> AccessPeer返回值为整型1，否则返回0。</p></li><li><p>在两个设备间，必须用以下函数显式地启用点对点内存访问：<code>cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int flag);</code></p></li><li><p>这个函数允许从当前设备到peerDevice进行点对点访问。flag参数被保留以备将来使<br> 用，目前必须将其设置为0。一旦成功，该对等设备的内存将立即由当前设备进行访问。</p></li><li><p>这个函数授权的访问是单向的，即这个函数允许从当前设备到peerDevice的访问，但<br> 不允许从peerDevice到当前设备的访问。如果希望对等设备能直接访问当前设备的内存，<br> 则需要另一个方向单独的匹配调用。</p></li><li><p>点对点访问保持启用状态，直到它被以下函数显式地禁用：<code>cudaError_t cudaDeviceDisablePeerAccess(int peerDevice);</code></p></li><li><p>32位应用程序不支持点对点访问。</p></li></ul><h4 id="_9-1-2-2-点对点内存复制" tabindex="-1"><a class="header-anchor" href="#_9-1-2-2-点对点内存复制" aria-hidden="true">#</a> 9.1.2.2 点对点内存复制</h4><ul><li><p>两个设备之间启用对等访问之后，使用下面的函数，可以异步地复制设备上的数据：<br><code>cudaError_t cudaMemcpyPeerAsync(void* dst, int dstDev, void* src, int srcDev,size_t nBytes, cudaStream_t stream);</code></p></li><li><p>这个函数将数据从设备的srcDev设备内存传输到设备dstDev的设备内存中。函数<br> cudaMemcpyPeerAsync对于主机和所有其他设备来说是异步的。如果srcDev和dstDev共享<br> 相同的PCIe根节点，那么数据传输是沿着PCIe最短路径执行的，不需要通过主机内存中转</p></li></ul><h3 id="_9-1-3-多gpu间的同步" tabindex="-1"><a class="header-anchor" href="#_9-1-3-多gpu间的同步" aria-hidden="true">#</a> 9.1.3 多GPU间的同步</h3><ul><li><p>在第6章中介绍的用于流和事件的CUDA API，也适用于多GPU应用程序。每一个流和<br> 事件与单一设备相关联。在多GPU应用程序上可以使用和单GPU应用程序相同的同步函<br> 数，但是必须指定适合的当前设备。多GPU应用程序中使用流和事件的典型工作流程如下<br> 所示：<br> 1.选择这个应用程序将使用的GPU集。<br> 2.为每个设备创建流和事件。<br> 3.为每个设备分配设备资源（如设备内存）。<br> 4.通过流在每个GPU上启动任务（例如，数据传输或内核执行）。<br> 5.使用流和事件来查询和等待任务完成。<br> 6.清空所有设备的资源。</p></li><li><p>只有与该流相关联的设备是当前设备时，在流中才能启动内核。只有与该流相关联的<br> 设备是当前设备时，才可以在流中记录事件。</p></li><li><p>任何时间都可以在任何流中进行内存拷贝，无论该流与什么设备相关联或当前设备是<br> 什么。即使流或事件与当前设备不相关，也可以查询或同步它们。</p></li></ul><h2 id="_9-2-多gpu间细分计算" tabindex="-1"><a class="header-anchor" href="#_9-2-多gpu间细分计算" aria-hidden="true">#</a> 9.2 多GPU间细分计算</h2><ul><li>在本节中，将会利用多GPU扩展第2章中的向量加法示例。学习如何通过多GPU分离<br> 输入和输出向量。向量加法是多GPU编程的典型案例，在问题分区之间不需要交换数据。</li></ul><h3 id="_9-2-1-在多设备上分配内存" tabindex="-1"><a class="header-anchor" href="#_9-2-1-在多设备上分配内存" aria-hidden="true">#</a> 9.2.1 在多设备上分配内存</h3><ul><li>在从主机向多个设备分配计算任务之前，首先需要确定在当前系统中有多少可用的GPU：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> ngpus<span class="token punctuation">;</span> 
<span class="token function">cudaGetDeviceCount</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>ngpus<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot; CUDA-capable devices: %i\\n&quot;</span><span class="token punctuation">,</span> ngpus<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>一旦GPU的数量已经被确定，接下来就需要为多个设备声明主机内存、设备内存、流<br> 和事件。保存这些变量的一个简单方法是使用数组，声明如下：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">float</span> <span class="token operator">*</span>d_A<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>d_B<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>d_C<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token keyword">float</span> <span class="token operator">*</span>h_A<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>h_B<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>hostRef<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>gpuRef<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">;</span>
cudaStream_t stream<span class="token punctuation">[</span>NGPUS<span class="token punctuation">]</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>在向量加法的例子中，元素的总输入大小为16M，所有设备平分，给每个设备isize个<br> 元素：<br><code>int size = 1 &lt;&lt; 24;</code><br><code>int iSize = size / ngpus;</code></p></li><li><p>设备上一个浮点向量的字节大小按如下方式进行计算：<code>size_t iBytes = iSize * sizeof(float);</code></p></li><li><p>现在，可以分配主机和设备内存了，为每个设备创建CUDA流，代码如下：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// set current device</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// allocate device memory</span>
 <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>d_C<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// allocate page locked host memory for asynchronous data transfer</span>
 <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>h_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>h_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>hostRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>gpuRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// create streams for timing and synchronizing</span>
 <span class="token function">cudaStreamCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>请注意，分配锁页（固定）主机内存是为了在设备和主机之间进行异步数据传输。同<br> 时，在分配任何内存或创建任何流之前，使用上面提到的cudasetDevice函数在每次循环迭<br> 代的开始，设置当前设备。</li></ul><h3 id="_9-2-2-单主机线程分配工作" tabindex="-1"><a class="header-anchor" href="#_9-2-2-单主机线程分配工作" aria-hidden="true">#</a> 9.2.2 单主机线程分配工作</h3><ul><li>在设备间分配操作之前，需要为每个设备初始化主机数组的状态：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">initialData</span><span class="token punctuation">(</span>h_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iSize<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">initialData</span><span class="token punctuation">(</span>h_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iSize<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>随着所有资源都被分配和初始化，可以使用一个循环在多个设备之间分配数据和计<br> 算：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// distributing the workload across multiple devices</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> h_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> h_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 iKernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span>d_A<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> d_B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> d_C<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iSize<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>gpuRef<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> d_C<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
<span class="token punctuation">}</span>
<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这个循环遍历多个GPU，为设备异步地复制输入数组。然后在相同的流中操作iSize个<br> 数据元素以便启动内核。最后，设备发出的异步拷贝命令，把结果从内核返回到主机。因<br> 为所有的函数都是异步的，所以控制会立即返回到主机线程。因此，当任务仍在当前设备<br> 上运行时，切换到下一个设备是安全的。</li></ul><h3 id="_9-2-3-编译和执行" tabindex="-1"><a class="header-anchor" href="#_9-2-3-编译和执行" aria-hidden="true">#</a> 9.2.3 编译和执行</h3><ul><li><p>从Wrox.com中下载simpleMultiGPU.cu文件，其中含有完整的多GPU向量加法示例。用<br> 以下命令编译它：<code>$ nvcc -O3 simpleMultiGPU.cu -o simpleMultiGPU</code></p></li><li><p>simpleMultiGPU函数的采样输出为：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleMultiGPU 
<span class="token operator">&gt;</span> starting <span class="token punctuation">.</span><span class="token operator">/</span>simpleMultiGPU with <span class="token number">2</span> CUDA<span class="token operator">-</span>capable devices
<span class="token operator">&gt;</span> total array size<span class="token operator">:</span> <span class="token number">16</span>M<span class="token punctuation">,</span> <span class="token keyword">using</span> <span class="token number">2</span> devices with each device handling <span class="token number">8</span>M
GPU timer elapsed<span class="token operator">:</span> <span class="token number">35.35</span>ms
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>通过将命令行选项设为1，尝试只用一个GPU运行它，代码如下：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleMultiGPU <span class="token number">1</span>
<span class="token operator">&gt;</span> starting <span class="token punctuation">.</span><span class="token operator">/</span>simpleMultiGPU with <span class="token number">2</span> CUDA<span class="token operator">-</span>capable devices
<span class="token operator">&gt;</span> total array size<span class="token operator">:</span> <span class="token number">16</span>M<span class="token punctuation">,</span> <span class="token keyword">using</span> <span class="token number">1</span> devices with each device handling <span class="token number">16</span>M
GPU timer elapsed<span class="token operator">:</span> <span class="token number">42.25</span>ms

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>尽管使用双倍数量的GPU时，运行时间并没有减少一半，但是仍然取得了显著的性能<br> 提升。</p></li><li><p>使用nvprof可以获得每个设备行为的更多细节：<code>$ nvprof --print-gpu-trace ./simpleMultiGPU</code></p></li><li><p>在有两个M2090 GPU的系统上产生的输出总结如下：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>Duration Size Throughput Device Stream Name
<span class="token number">6.5858</span>ms <span class="token number">33.554</span>MB <span class="token number">5.0950</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
<span class="token number">6.5921</span>ms <span class="token number">33.554</span>MB <span class="token number">5.0901</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token number">21</span> <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
<span class="token number">6.6171</span>ms <span class="token number">33.554</span>MB <span class="token number">5.0708</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
<span class="token number">6.6020</span>ms <span class="token number">33.554</span>MB <span class="token number">5.0825</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token number">21</span> <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
<span class="token number">9.8417</span>ms <span class="token number">33.554</span>MB <span class="token number">3.4094</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy DtoH<span class="token punctuation">]</span>
<span class="token number">9.8460</span>ms <span class="token number">33.554</span>MB <span class="token number">3.4079</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token number">21</span> <span class="token punctuation">[</span>CUDA memcpy DtoH<span class="token punctuation">]</span>
<span class="token number">720.49u</span>s <span class="token operator">-</span> <span class="token operator">-</span> Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token number">21</span> <span class="token function">iKernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span class="token number">721.32u</span>s <span class="token operator">-</span> <span class="token operator">-</span> Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token function">iKernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在只有一个M2090 GPU上运行，产生的输出如下所示：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>Duration Size Throughput Device Stream Name
<span class="token number">13.171</span>ms <span class="token number">67.109</span>MB <span class="token number">5.0951</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy HtoD
<span class="token number">13.117</span>ms <span class="token number">67.109</span>MB <span class="token number">5.1161</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy HtoD
<span class="token number">14.918</span>ms <span class="token number">67.109</span>MB <span class="token number">4.4984</span>GB<span class="token operator">/</span>s Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token punctuation">[</span>CUDA memcpy DtoH
<span class="token number">1.4371</span>ms <span class="token operator">-</span> <span class="token operator">-</span> Tesla <span class="token function">M2090</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token number">13</span> <span class="token function">iKernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>从这些结果中可以看到，在两个设备之间的操作完美地被分配了。</li></ul><h2 id="_9-3-多gpu上的点对点通信" tabindex="-1"><a class="header-anchor" href="#_9-3-多gpu上的点对点通信" aria-hidden="true">#</a> 9.3 多GPU上的点对点通信</h2><ul><li>在本节中，将介绍两个GPU之间的数据传输。将测试以下3种情况：<br> ·两个GPU之间的单向内存复制<br> ·两个GPU之间的双向内存复制<br> ·内核中对等设备内存的访问</li></ul><h3 id="_9-3-1-实现点对点访问" tabindex="-1"><a class="header-anchor" href="#_9-3-1-实现点对点访问" aria-hidden="true">#</a> 9.3.1 实现点对点访问</h3><ul><li>首先，必须对所有设备启用双向点对点访问，如以下代码所示：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">/*
 * enable P2P memcopies between GPUs (all GPUs must be compute capability 2.0 or
 * later (Fermi or later)).
 */</span>
<span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">enableP2P</span> <span class="token punctuation">(</span><span class="token keyword">int</span> ngpus<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> j <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> j<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">if</span><span class="token punctuation">(</span>i <span class="token operator">==</span> j<span class="token punctuation">)</span> <span class="token keyword">continue</span><span class="token punctuation">;</span>
 <span class="token keyword">int</span> peer_access_available <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
 <span class="token function">cudaDeviceCanAccessPeer</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>peer_access_available<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>peer_access_available<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaDeviceEnablePeerAccess</span><span class="token punctuation">(</span>j<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;&gt; GPU%d enabled direct access to GPU%d\\n&quot;</span><span class="token punctuation">,</span>i<span class="token punctuation">,</span>j<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
 <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;(%d, %d)\\n&quot;</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token punctuation">}</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>函数enableP2P遍历所有设备对（i，j），如果支持点对点访问，则使用cudaDeviceEnablePeerAccess函数启用双向点对点访问。</li></ul><h3 id="_9-3-2-点对点的内存复制" tabindex="-1"><a class="header-anchor" href="#_9-3-2-点对点的内存复制" aria-hidden="true">#</a> 9.3.2 点对点的内存复制</h3><ul><li><p>启用点对点访问后，可以在两个设备之间直接复制数据。如果不支持点对点访问，该<br> 例子输出不能启用点对点访问的设备ID（不能启用的最有可能的原因是因为它们没有连接<br> 到同一个PCIe根节点上），并且没有错误继续运行了。然而，回想一下可知，如果在两个<br> GPU之间不支持点对点访问，那么这两个设备之间的点对点内存复制将通过主机内存中<br> 转，从而会降低其性能。性能降低对应用程序的影响程度取决于内核进行时间计算和执行<br> 对等传输需要的时间。如果有足够的时间来进行计算，那么可以隐藏点对点复制的延时，<br> 该延时主要是通过主机内存使用设备计算进行重叠的。</p></li><li><p>启用点对点访问后，下面的代码在两个设备间执行ping-pong同步内存复制，次数为一<br> 百次。如果点对点访问在所有设备上都被成功启用了，那么将直接通过PCIe总线进行数据<br> 传输而不用与主机交互。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// ping pong unidirectional gmem copy</span>
 <span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_src<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_src<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_src<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_src<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>请注意，在内存复制之前没有设备转换，因为跨设备的内存复制不需要显式地设置当<br> 前设备。如果在内存复制前指定了设备，也不会影响它的行为。</p></li><li><p>如需衡量设备之间数据传输的性能，需要把启动和停止事件记录在同一设备上，并将<br> ping-pong内存复制包含在内。然后，用cudaEventElapsedTime计算两个事件之间消耗的时<br> 间。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
<span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>stop<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventSynchronize</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">float</span> elapsed_time_ms<span class="token punctuation">;</span>
<span class="token function">cudaEventElapsedTime</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>elapsed_time_ms<span class="token punctuation">,</span> start<span class="token punctuation">,</span> stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>然后，通过ping-pong测试所获得的带宽按照下面所示的代码进行估计：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>elapsed_time_ms <span class="token operator">/=</span> <span class="token number">100.0f</span><span class="token punctuation">;</span>
<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Ping-pong unidirectional cudaMemcpy:\\t\\t %8.2f ms&quot;</span><span class="token punctuation">,</span> elapsed_time_ms<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;performance: %8.2f GB/s\\n&quot;</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> iBytes <span class="token operator">/</span><span class="token punctuation">(</span>elapsed_time_ms <span class="token operator">*</span> <span class="token number">1e6f</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>从Wrox.com中可以下载包含这个例子的文件simpleP2P_PingPong.cu。编译和运行如下<br> 所示：<br><code>$ nvcc -O3 simpleP2P_PingPong.cu -o simplePingPong</code><br><code>$ ./simplePingPong</code></p></li><li><p>simpleP2P_PingPong的输出如下所示：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>Allocating <span class="token function">buffers</span> <span class="token punctuation">(</span><span class="token number">64</span>MB on each GPU <span class="token operator">and</span> CPU Host<span class="token punctuation">)</span>
Ping<span class="token operator">-</span>pong unidirectional cudaMemcpy<span class="token operator">:</span> <span class="token number">13.41</span>ms performance<span class="token operator">:</span> <span class="token number">5.00</span> GB<span class="token operator">/</span>s
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>因为PCIe总线支持任何两个端点之间的全双工通信，所以也可以使用异步复制函数来<br> 进行双向的且点对点的内存复制：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// bidirectional asynchronous gmem copy</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_src<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_src<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span>
 stream<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_rcv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_rcv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span>
 stream<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>双向内存复制的测试在同一个文件中可以被实现。下面是一个示例输出：<br><code>Ping-pong bidirectional cudaMemcpyAsync: 13.39ms performance: 10.02 GB/s</code></p></li><li><p>注意，因为PCIe总线是一次在两个方向上使用的，所以获得的带宽增加了一倍。如果<br> 通过在simpleP2P_PingPong.cu中移除对enableP2P的调用来禁用点对点访问，那么无论是单<br> 向还是双向的例子都会不带任何错误的运行，但由于通过主机内存中转传输，所以测得的<br> 带宽将会下降。</p></li></ul><h3 id="_9-3-3-统一虚拟寻址的点对点内存访问" tabindex="-1"><a class="header-anchor" href="#_9-3-3-统一虚拟寻址的点对点内存访问" aria-hidden="true">#</a> 9.3.3 统一虚拟寻址的点对点内存访问</h3><ul><li><p>第4章中介绍的统一虚拟寻址（UVA），是将CPU系统内存和设备的全局内存映射到<br> 一个单一的虚拟地址空间中，如图9-2所示。所有由cudaHostAlloc分配的主机内存和由<br> cudaMalloc分配的设备内存驻留在这个统一的地址空间内。内存地址所驻留的设备可以根<br> 据地址本身确定。<br><img src="`+v+`" alt="figure9-2" loading="lazy"></p></li><li><p>将点对点CUDA API与UVA相结合，可以实现对任何设备内存的透明访问。不必手动<br> 管理单独的内存缓冲区，也不必从主机内存中进行显式的复制。底层系统能使我们避免显<br> 式地执行这些操作，从而简化了代码。请注意，过于依赖UVA进行对等访问对性能将产生<br> 负面的影响，如跨PCIe总线的许多小规模的传输会明显地有过大的消耗。</p></li><li><p>下面的代码演示了如何检查设备是否支持统一寻址：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> deviceId <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
cudaDeviceProp prop<span class="token punctuation">;</span>
<span class="token function">cudaGetDeviceProperties</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>prop<span class="token punctuation">,</span> deviceId<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;GPU%d: %s unified addressing\\n&quot;</span><span class="token punctuation">,</span> deviceId<span class="token punctuation">,</span>
 prop<span class="token punctuation">.</span>unifiedAddressing <span class="token operator">?</span> <span class="token string">&quot;supports&quot;</span> <span class="token operator">:</span> <span class="token string">&quot;does not support&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>为了使用UVA，应用程序必须在设备的计算能力为2.0及以上的64位架构上进行编<br> 译，并且CUDA版本为4.0或以上。如果同时启用点对点访问和UVA，那么在一个设备上执<br> 行的核函数，可以解除另一个设备上存储的指针。</p></li><li><p>可以使用以下简单的核函数（该函数将输入数组扩展了2倍，并将结果存储在输出数<br> 组中），来测试GPU的直接点对点内存访问：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">iKernel</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>src<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>dst<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">const</span> <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 dst<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> src<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2.0f</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>以下代码将设备0设置为当前设备，并有一个核函数使用指针d_src[1]从设备1中读取<br> 全局内存，同时通过全局内存的<code>指针d_rcv[0]</code>将结果写入当前设备中。<br><code>cudaSetDevice(0);</code><br><code>iKernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_rcv[0], d_src[1]);</code></p></li><li><p>以下代码将设备1设置为当前设备，并有一个核函数使用指针d_src[0]从设备0中读取<br> 全局内存，同时通过全局内存的指针d_rcv[1]将结果写入当前设备中。<br><code>cudaSetDevice(1);</code><br><code>iKernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_rcv[1], d_src[0]);</code></p></li><li><p>这些代码包含在simpleP2P_PingPong.cu文件中。以下输出表明这些核函数运行成功：<br><code>2. Running kernel on GPU1, taking source data from GPU0 and writing to GPU1...</code><br><code>3. Running kernel on GPU0, taking source data from GPU1 and writing to GPU0...</code></p></li><li><p>如果GPU没有连接到相同的PCIe根节点上，或点对点访问被禁止，那么将会出现以下<br> 的错误信息：<br><code>&gt; GPU0 disabled direct access to GPU1</code><br><code>&gt; GPU1 disabled direct access to GPU0</code></p></li></ul><h2 id="_9-4-多gpu上的有限差分" tabindex="-1"><a class="header-anchor" href="#_9-4-多gpu上的有限差分" aria-hidden="true">#</a> 9.4 多GPU上的有限差分</h2><ul><li>在本节中，通过使用有限差分的方法求解二维波动方程，将会学习到如何跨设备重叠<br> 计算和通信。此示例扩展了前面介绍过的向量加法和ping-pong例子的概念，因为它同时包<br> 含重要的计算和通信操作。请注意，该例子涉及的物理方程和术语都已包括在本章中，但<br> 不需要从数学角度理解该问题。所有的概念将从CUDA编程角度给出解释，包括为有兴趣<br> 的读者提供的特定领域信息。</li></ul><h3 id="_9-4-1-二维波动方程的模板计算" tabindex="-1"><a class="header-anchor" href="#_9-4-1-二维波动方程的模板计算" aria-hidden="true">#</a> 9.4.1 二维波动方程的模板计算</h3><ul><li><p>二维波的传播由以下波动方程来决定：<br><code>波动方程</code></p></li><li><p>其中，u（x，y，t）是波场，v（x，y）是介质的速度。这是一个二阶偏微分方程。求<br> 解这种偏微分方程的典型方法是使用规则的笛卡尔网格上的有限差分法</p></li><li><p>更简单地说，有限差分法近似于使用一个模板（如在第5章中介绍的，尽管是一个二<br> 维的）求导以计算规则网格中单一点的导数，具体方法是在围绕该点的多个局部点上应用<br> 一个函数。图9-3展示了17点的模板，它将在这一节中作为一个例子。如果要求解中心点<br> 的导数，则需要使用16个离中心点最近的局部点<br><img src="`+P+`" alt="figure9-3" loading="lazy"></p></li><li><p>按计算方式，偏导数可以由一个泰勒展开式来表示，其在一个维度的实现用以下伪代<br> 码来表示。这个伪代码使用一维数组der_u来从当前元素<code>u[i]</code>前面的4个元素<code>（u[i＋d]）</code>和4<br> 个后面的元素<code>（u[i―d]）</code>中积累贡献。c数组存储导数系数，u[i]是计算的中心点，<br><code>der_u[i]</code>是得到的中心点的导数。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>der_u<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> c<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> u<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> d <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> d <span class="token operator">&lt;=</span> <span class="token number">4</span><span class="token punctuation">;</span> d<span class="token operator">++</span><span class="token punctuation">)</span> 
 der_u<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> c<span class="token punctuation">[</span>d<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>u<span class="token punctuation">[</span>i<span class="token operator">-</span>d<span class="token punctuation">]</span> <span class="token operator">+</span> u<span class="token punctuation">[</span>i<span class="token operator">+</span>d<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_9-4-2-多gpu程序的典型模式" tabindex="-1"><a class="header-anchor" href="#_9-4-2-多gpu程序的典型模式" aria-hidden="true">#</a> 9.4.2 多GPU程序的典型模式</h3><ul><li><p>为了准确地模拟通过不同介质的波传播，需要大量的数据。但单GPU的全局内存没有<br> 足够的空间存储模拟过程的状态。这就需要跨多个GPU的数据域分解。假设在二维数组中<br> x轴是最内层的维度，那么可以沿y轴分割数据使其分布在多个GPU上。因为对一个给定点<br> 的计算需要它两侧最近的4个点，所以需要为存储在每个GPU上的数据添加填充区域，如<br> 图9-4所示。此填充区域或halo区域，使得在波传播计算的每次循环中，相邻设备之间进<br> 行数据交换。</p></li><li><p>图9-5所示的域分解，使用了多GPU来求解波动方程，在模拟的每一步时都会使用以<br> 下的通用模式：<br><img src="`+_+'" alt="figure9-4" loading="lazy"></p></li></ul><p><img src="'+g+`" alt="figure9-5" loading="lazy"><br> 1.在一个流中使用相邻的GPU计算halo区域和交换halo数据。<br> 2.在不同的流中计算内部区域。<br> 3.在进行下一个循环之前，在所有设备上进行同步计算。</p><ul><li><p>如果使用两个不同的流，一个用于halo计算和通信，另一个用于内部区域的计算，步<br> 骤1可以与步骤2重叠。如果内部计算所需的计算时间比halo操作所需的时间长，可以通过<br> 使用多个GPU隐藏halo通信的性能影响来实现线性加速。</p></li><li><p>在两个GPU上进行模板计算的伪代码如下：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> istep <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> istep <span class="token operator">&lt;</span> nsteps<span class="token punctuation">;</span> istep<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// calculate halo in stream_halo</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token number">2</span>dfd_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// exchange halo on stream halo</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// calculate the internal region in stream_internal</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token number">2</span>dfd_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> stream_internal<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// synchronize before next iteration</span>
 <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>不需要为复制操作设置当前设备，但是在启动内核之前，必须指定当前设备</li></ul><h3 id="_9-4-3-多gpu上的二维模板计算" tabindex="-1"><a class="header-anchor" href="#_9-4-3-多gpu上的二维模板计算" aria-hidden="true">#</a> 9.4.3 多GPU上的二维模板计算</h3><ul><li><p>在本节中，将使用多GPU实现一个简单的二维模板。在二维模板计算中使用两个设备<br> 数组。一个保存当前的波场，另一个保存更新的波场。如果将x定义为最内层的数组维<br> 度，并且将y作为最外层的维度，那么可以沿着y轴跨设备均匀地分配计算。</p></li><li><p>因为更新一个点需要访问9个最近点，所以很多点都将共享输入数据。因此，使用共<br> 享内存可以减少全局内存的访问。共享内存的使用量等同于保存相邻数据的线程块的大<br> 小，该线程块被8个点填充，如图9-6和下面的代码所示。<br><code>__shared__ float line[4 + BDIMX + 4];</code></p></li><li><p>用来存储y轴模板值的9个浮点值被声明为一个核函数的本地数组，并因此存储在寄存<br> 器中。当沿y轴在当前元素的前后加载元素时，用到的寄存器很像用来减少冗余访问的共<br> 享内存。<br><code>// registers for the y dimension</code><br><code>float yval[9];</code></p></li></ul><figure><img src="`+h+'" alt="figure9-6" tabindex="0" loading="lazy"><figcaption>figure9-6</figcaption></figure><ul><li><p>图9-7说明了单线程中沿x轴存储模板值的共享内存和沿y轴存储模板值的9个寄存器。<br><img src="'+M+'" alt="figure9-7" loading="lazy"></p></li><li><p>一旦输入数据被分配并初始化，在每个GPU线程上实现有限差分（FD）的模板计算<br> 可以写成如下代码：</p></li></ul>',89),R={class:"hint-container details"},S=n("summary",null,"Click me to view the code!",-1),E=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token comment"},"// central point"),s(`
`),n("span",{class:"token keyword"},"float"),s(" tmp "),n("span",{class:"token operator"},"="),s(" coef"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"*"),s(" line"),n("span",{class:"token punctuation"},"["),s("stx"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token number"},"2.0f"),n("span",{class:"token punctuation"},";"),s(` 
`),n("span",{class:"token comment"},"// stencil computation in the x dimension"),s(`
`),n("span",{class:"token keyword"},"for"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" d "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},";"),s(" d "),n("span",{class:"token operator"},"<="),s(),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},";"),s(" d"),n("span",{class:"token operator"},"++"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 tmp `),n("span",{class:"token operator"},"+="),s(" coef"),n("span",{class:"token punctuation"},"["),s("d"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token punctuation"},"("),s("line"),n("span",{class:"token punctuation"},"["),s("stx"),n("span",{class:"token operator"},"-"),s("d"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"+"),s(" line"),n("span",{class:"token punctuation"},"["),s("stx"),n("span",{class:"token operator"},"+"),s("d"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(` 
`),n("span",{class:"token punctuation"},"}"),s(`
`),n("span",{class:"token comment"},"// stencil computation in the y dimension"),s(`
`),n("span",{class:"token keyword"},"for"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" d "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},";"),s(" d "),n("span",{class:"token operator"},"<="),s(),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},";"),s(" d"),n("span",{class:"token operator"},"++"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 tmp `),n("span",{class:"token operator"},"+="),s(" coef"),n("span",{class:"token punctuation"},"["),s("d"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token punctuation"},"("),s("yval"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"4"),n("span",{class:"token operator"},"-"),s("d"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"+"),s(" yval"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"4"),n("span",{class:"token operator"},"+"),s("d"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`),n("span",{class:"token comment"},"// update the new value for the central point"),s(`
g_u1`),n("span",{class:"token punctuation"},"["),s("idx"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"2.0f"),s(),n("span",{class:"token operator"},"*"),s(" current "),n("span",{class:"token operator"},"-"),s(" g_u1"),n("span",{class:"token punctuation"},"["),s("idx"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"+"),s(" alpha "),n("span",{class:"token operator"},"*"),s(" tmp"),n("span",{class:"token punctuation"},";"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),O=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s("yarn add "),n("span",{class:"token operator"},"-"),s("D vuepress"),n("span",{class:"token operator"},"-"),s("theme"),n("span",{class:"token operator"},"-"),s(`hope
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"})])],-1),z=o(`<h3 id="_9-4-4-重叠计算与通信" tabindex="-1"><a class="header-anchor" href="#_9-4-4-重叠计算与通信" aria-hidden="true">#</a> 9.4.4 重叠计算与通信</h3><ul><li><p>此二维模板的执行配置使用一个具有一维线程块的一维网格，在主机上的声明如下所示：<br><code>dim3 block(BDIMX);</code><br><code>dim3 grid(nx / block.x);</code></p></li><li><p>在主机上波的传播时间通过使用nsteps次迭代的时间循环来控制。在第一次时间步骤<br> 中，核函数kernel_add_wavelet引入GPU0介质中一个扰动。随着时间的变化进一步的迭代<br> 传递干扰。因为halo区域计算和数据交换被安排在每个设备的stream_halo流中，内部区域<br> 的计算被安排在每个设备的stream_internal流中，所以在此二维模板上计算和通信可以重叠。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// add a disturbance onto gpu0 on the first time step</span>
<span class="token function">cudaSetDevice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
kernel_add_wavelet<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>d_u2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">20.0</span><span class="token punctuation">,</span> nx<span class="token punctuation">,</span> iny<span class="token punctuation">,</span> ngpus<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// for each time step</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> istep <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> istep <span class="token operator">&lt;</span> nsteps<span class="token punctuation">;</span> istep<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// add a disturbance onto gpu0 at first step</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>istep<span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>gpuid<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_add_wavelet <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span>block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>d_u2<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">20.0</span><span class="token punctuation">,</span>nx<span class="token punctuation">,</span>iny<span class="token punctuation">,</span>ngpus<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// update halo and internal asynchronously</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// compute the halo region values in the halo stream</span>
 kernel_2dfd<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 
 <span class="token punctuation">(</span>d_u1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> d_u2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> nx<span class="token punctuation">,</span> haloStart<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> haloEnd<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// compute the internal region values in the internal stream</span>
 kernel_2dfd<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream_internal<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 
 <span class="token punctuation">(</span>d_u1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> d_u2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> nx<span class="token punctuation">,</span> bodyStart<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> bodyEnd<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// exchange halos in the halo stream</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ngpus <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_u1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> dst_skip<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_u1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> src_skip<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 iexchange<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span>d_u1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> dst_skip<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_u1<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> src_skip<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 iexchange<span class="token punctuation">,</span> cudaMemcpyDeviceToDevice<span class="token punctuation">,</span> stream_halo<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// synchronize for the next step</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> ngpus<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// swap global memory pointers</span>
 <span class="token keyword">float</span> <span class="token operator">*</span>tmpu0 <span class="token operator">=</span> d_u1<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 d_u1<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> d_u2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 d_u2<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> tmpu0<span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_9-4-5-编译和执行" tabindex="-1"><a class="header-anchor" href="#_9-4-5-编译和执行" aria-hidden="true">#</a> 9.4.5 编译和执行</h3>`,4),q={href:"http://xn--Wrox-z25f.xn--comsimple2DFD-r40uop47g6a22aq1htw1alqe2obm35h8x1cwnaq167a5nil0i5x8o.cu",target:"_blank",rel:"noopener noreferrer"},H=n("br",null,null,-1),V=n("code",null,"$ nvcc -arch=sm_20 -O3 -use_fast_math simple2DFD.cu -o simple2DFD",-1),T=n("li",null,[n("p",null,"以下是系统输出的例子，该系统有两个M2090设备：")],-1),L=o(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simple2DFD 
<span class="token operator">&gt;</span> CUDA<span class="token operator">-</span>capable device count<span class="token operator">:</span> <span class="token number">2</span>
<span class="token operator">&gt;</span> GPU0<span class="token operator">:</span> Tesla M2090 is capable of Peer<span class="token operator">-</span>to<span class="token operator">-</span>Peer access
<span class="token operator">&gt;</span> GPU1<span class="token operator">:</span> Tesla M2090 is capable of Peer<span class="token operator">-</span>to<span class="token operator">-</span>Peer access
<span class="token operator">&gt;</span> GPU0<span class="token operator">:</span> Tesla M2090 support unified addressing
<span class="token operator">&gt;</span> GPU1<span class="token operator">:</span> Tesla M2090 support unified addressing
<span class="token operator">&gt;</span> run with device<span class="token operator">:</span> <span class="token number">2</span>
GPU <span class="token number">0</span><span class="token operator">:</span> allocated <span class="token number">2.03</span> MB gmem
GPU <span class="token number">1</span><span class="token operator">:</span> allocated <span class="token number">2.03</span> MB gmem
gputime<span class="token operator">:</span> <span class="token number">0.27</span>ms performance<span class="token operator">:</span> <span class="token number">962.77</span> MCells<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>使用的性能指标用Mcells/s表示。对于二维情况，其定义如下：<br> nx * ny * number of iterations / total time in seconds * 10^6</p></li><li><p>只用一个设备运行时，产生以下结果:</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simple2DFD <span class="token number">1</span>
<span class="token operator">&gt;</span> CUDA<span class="token operator">-</span>capable device count<span class="token operator">:</span> <span class="token number">2</span>
<span class="token operator">&gt;</span> GPU0<span class="token operator">:</span> Tesla M2090 is capable of Peer<span class="token operator">-</span>to<span class="token operator">-</span>Peer access
<span class="token operator">&gt;</span> GPU1<span class="token operator">:</span> Tesla M2090 is capable of Peer<span class="token operator">-</span>to<span class="token operator">-</span>Peer access
<span class="token operator">&gt;</span> GPU0<span class="token operator">:</span> Tesla M2090 support unified addressing
<span class="token operator">&gt;</span> GPU1<span class="token operator">:</span> Tesla M2090 support unified addressing
<span class="token operator">&gt;</span> run with device<span class="token operator">:</span> <span class="token number">1</span>
GPU <span class="token number">0</span><span class="token operator">:</span> allocated <span class="token number">4.00</span> MB gmem
gputime<span class="token operator">:</span> <span class="token number">0.52</span>ms performance<span class="token operator">:</span> <span class="token number">502.98</span> MCells<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>simple2DFD显示了从一个设备移动到两个设备的近似线形扩展（有效率为96%）。由<br> 此，可以得出结论，转移halo区域增加的通信开销可以采用CUDA流在多个GPU上有效地<br> 隐藏。</p></li><li><p>可以用nvvp检查simple2DFD的并发性：<code>$ nvvp ./simple2DFD</code></p></li><li><p>图9-8显示了两个设备并发执行生成的时间线。注意，nvvp表明每个GPU正在使用两<br> 个流。可以看到一个流用于数据交换和计算，而另一个流则是只用于纯计算的。</p></li><li><p>可以使用以下命令来保存该应用程序在时间步长为400时的状态：<code>$ ./simple2DFD 2 400</code><br><img src="`+f+'" alt="figure9-8" loading="lazy"></p></li><li><p>这个命令在两个GPU上运行应用程序，并且在时间步长为400时将波场保存到磁盘上<br> 的数据文件中。然后使用绘图工具可以显示出原始数据，如图9-9所示。<br><img src="'+U+`" alt="figure9-9" loading="lazy"></p></li><li><p>在simple2DFD中检查内核的资源使用情况也是很有趣的。在nvcc编译器中添加标志可<br> 以报告内核资源的使用情况：<code>$ nvcc -arch=sm_20 -Xptxas -v simple2DFD.cu -o simple2DFD</code></p></li><li><p>nvcc编译器输出以下信息：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>ptxas info <span class="token operator">:</span> <span class="token number">0</span> bytes gmem<span class="token punctuation">,</span> <span class="token number">20</span> bytes cmem<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> 
ptxas info <span class="token operator">:</span> Compiling entry function <span class="token char">&#39;_Z18kernel_add_waveletPffii&#39;</span> <span class="token keyword">for</span> <span class="token char">&#39;sm_20&#39;</span>
ptxas info <span class="token operator">:</span> Function properties <span class="token keyword">for</span> _Z18kernel_add_waveletPffii
ptxas info <span class="token operator">:</span> Used <span class="token number">4</span> registers<span class="token punctuation">,</span> <span class="token number">52</span> bytes cmem<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
ptxas info <span class="token operator">:</span> Compiling entry function <span class="token char">&#39;_Z11kernel_2dfdPfS_iii&#39;</span> <span class="token keyword">for</span> <span class="token char">&#39;sm_20&#39;</span>
ptxas info <span class="token operator">:</span> Function properties <span class="token keyword">for</span> _Z11kernel_2dfdPfS_iii
ptxas info <span class="token operator">:</span> Used <span class="token number">26</span> registers<span class="token punctuation">,</span> <span class="token number">160</span> bytes smem<span class="token punctuation">,</span> <span class="token number">60</span> bytes cmem<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">8</span> bytes 
cmem<span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>上面代码输出的最后一行表明，核函数kernel_2dfd使用了26个寄存器、160个字节的<br> 共享内存和每个线程上的一些常量内存（用来存储核函数的参数）。</li></ul><h2 id="_9-5-跨gpu集群扩展应用程序" tabindex="-1"><a class="header-anchor" href="#_9-5-跨gpu集群扩展应用程序" aria-hidden="true">#</a> 9.5 跨GPU集群扩展应用程序</h2>`,7),W=o("<li><p>与同构系统相比，GPU加速集群被公认为极大地提升了性能效果和节省了计算密集型<br> 应用程序的功耗。当处理超大数据集时，通常需要多个计算节点来有效地解决问题。<br> MPI（消息传递接口）是一个标准化和便携式的用于数据通信的API，它通过分布式进程<br> 之间的消息进行数据通信。在大多数MPI实现中，库例程是直接从C或其他语言中调用<br> 的。</p></li><li><p>MPI与CUDA是完全兼容的。支持GPU之间不同节点上移动数据的MPI有两种实现方<br> 式：传统的MPI与CUDA-aware MPI。在传统的MPI中，只有主机内存的内容可以直接通过<br> MPI函数来传输。在MPI把数据传递到另一个节点之前，GPU内存中的内容必须首先使用<br> CUDA API复制回主机内存。在CUDA-aware MPI中，可以把GPU内存中的内容直接传递到<br> MPI函数上，而不用通过主机内存中转数据。</p></li>",2),F=n("br",null,null,-1),K={href:"http://mvapich.cse.ohio-state.edu/download/mvapich2/%EF%BC%89",target:"_blank",rel:"noopener noreferrer"},N=n("br",null,null,-1),$={href:"http://mvapich.cse.ohio-state.edu/download/mvapich2gdr/%EF%BC%89",target:"_blank",rel:"noopener noreferrer"},Y=n("br",null,null,-1),Z={href:"http://www.open-mpi.org/software/ompi/v1.7/%EF%BC%89",target:"_blank",rel:"noopener noreferrer"},j=n("br",null,null,-1),X=n("br",null,null,-1),J=o("<li><p>MVAPICH2是一个开源的MPI实现，用来探索InfiniBand网络特点，对基于MPI的应用<br> 程序提供高性能和高可扩展性。当前存在两个版本：MVAPICH2是一个CUDA-aware MPI<br> 实现，而MVAPICH2-GDR是一个扩展版本，增加了对GPUDirect RDMA的支持（更多内容<br> 将在本章中GPUDirect部分进行介绍）。</p></li><li><p>对InfiniBand集群而言，MVAPICH2是一个被广泛使用的开源MPI库，并且支持从一个<br> GPU的设备内存到另一个GPU的设备内存间直接进行MPI通信。接下来的部分将使用<br> MVAPICH平台来测试以下情况：<br> ·用MVAPICH2和InfiniBand进行CPU到CPU间的数据传输<br> ·用传统的MPI和InfiniBand进行GPU到GPU间的数据传输<br> ·用CUDA-aware MPI和InfiniBand进行GPU到GPU间的数据传输<br> ·用MVAPICH2-GDR和GPUDirect RDMA以及InfiniBand进行GPU到GPU间的数据传输</p></li>",2),Q=o(`<h3 id="_9-5-1-cpu到cpu的数据传输" tabindex="-1"><a class="header-anchor" href="#_9-5-1-cpu到cpu的数据传输" aria-hidden="true">#</a> 9.5.1 CPU到CPU的数据传输</h3><ul><li>为了建立一个便于比较的基准，可以使用MVAPICH2在集群中测试两个CPU间互相连<br> 接的节点，以了解数据传输的带宽和延迟。一般来说，MPI程序包括4个步骤：<br> 1.初始化MPI环境。<br> 2.使用阻塞或非阻塞MPI函数在不同节点间的进程传递消息。<br> 3.跨节点同步。<br> 4.清理MPI环境。</li></ul><h4 id="_9-5-1-1-实现节点间的mpi通信" tabindex="-1"><a class="header-anchor" href="#_9-5-1-1-实现节点间的mpi通信" aria-hidden="true">#</a> 9.5.1.1 实现节点间的MPI通信</h4><ul><li>下面的代码展示了一个简单的MPI程序框架，该程序在跨节点同步和退出前发送和接<br> 收单个消息。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// initialize the MPI environment </span>
 <span class="token keyword">int</span> rank<span class="token punctuation">,</span> nprocs<span class="token punctuation">;</span>
 <span class="token function">MPI_Init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span> <span class="token operator">&amp;</span>argv<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Comm_size</span><span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>nprocs<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Comm_rank</span><span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>rank<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// transmit messages with MPI calls</span>
 <span class="token function">MPI_Send</span><span class="token punctuation">(</span>sbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Recv</span><span class="token punctuation">(</span>rbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// synchronize across nodes</span>
 <span class="token function">MPI_Barrier</span><span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// clean up the MPI environment</span>
 <span class="token function">MPI_Finalize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token keyword">return</span> EXIT_SUCCESS<span class="token punctuation">;</span> 
<span class="token punctuation">}</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>想要测试两节点间的带宽和延迟，首先需要分配MYBUFSIZE大小的数组，作为发送/<br> 接收缓冲区，代码如下所示：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">char</span> <span class="token operator">*</span>s_buf<span class="token punctuation">,</span> <span class="token operator">*</span>r_buf<span class="token punctuation">;</span>
s_buf <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
r_buf <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>然后，两个计算节点间的双向数据传输可以使用非阻塞MPI的发送和接收函数来完成：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// If this is the first MPI process</span>
<span class="token keyword">if</span><span class="token punctuation">(</span>rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// Repeatedly</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nRepeat<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// Asynchronously receive size bytes from other_proc into rbuf</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>rbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Asynchronously send size bytes too other_proc from sbuf</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>sbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Wait for the send to complete</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Wait for the receive to complete</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>rank <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nRepeat<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// Asynchronously receive size bytes from other_proc into rbuf</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>rbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Asynchronously send size bytes to other_proc from sbuf</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>sbuf<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Wait for the send to complete</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token comment">// Wait for the receive to complete</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>为了获得准确的性能指标，发送和接收操作需要重复nRepeat次，然后取这些结果的<br> 平均值。从Wrox.com中下载文件simpleC2C.c，文件中包含全部的示例代码。用以下命令<br> 编译它：<code>$ mpicc -std=c99 -O3 simpleC2C.c -o simplec2c</code></p></li><li><p>如果启用了MPI的集群，则可以在两个节点上运行MPI程序。指定两节点的主机名，<br> 如node01和node02（取决于集群的配置），并运行下面的例子：<br><code>$ mpirun_rsh -np 2 node01 node02 ./simplec2c</code></p></li><li><p>一个输出例子如下所示：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token punctuation">.</span><span class="token operator">/</span>simplec2c to allocate <span class="token number">4</span> MB dynamic memory aligned to <span class="token number">64</span> byte
node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>node01<span class="token punctuation">)</span><span class="token operator">:</span> other_proc <span class="token operator">=</span> <span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>node02<span class="token punctuation">)</span><span class="token operator">:</span> other_proc <span class="token operator">=</span> <span class="token number">0</span>
 size Elapsed Performance
 <span class="token number">1</span> KB <span class="token number">3.96</span> μs <span class="token number">258.27</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> KB <span class="token number">3.75</span> μs <span class="token number">1092.17</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> KB <span class="token number">8.77</span> μs <span class="token number">1867.12</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> KB <span class="token number">20.18</span> μs <span class="token number">3247.61</span> MB<span class="token operator">/</span>sec
 <span class="token number">256</span> KB <span class="token number">49.48</span> μs <span class="token number">5297.96</span> MB<span class="token operator">/</span>sec
 <span class="token number">1</span> MB <span class="token number">169.99</span> μs <span class="token number">6168.63</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">662.81</span> μs <span class="token number">6328.06</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>mpirun_rsh是一个由MVAPICH2提供的作业启动命令。也可以使用mpirun运行作业。<code>$ mpirun -np 2 -host node01,node02 ./simplec2c</code></li></ul><h4 id="_9-5-1-2-cpu亲和性" tabindex="-1"><a class="header-anchor" href="#_9-5-1-2-cpu亲和性" aria-hidden="true">#</a> 9.5.1.2 CPU亲和性</h4><ul><li><p>在多核系统中（例如，在MPI程序中），当多个进程或线程同时需要CPU时间时，操<br> 作系统将为线程和进程分配可用的CPU核心。这意味着在操作系统的控制下，一个进程或<br> 线程将被暂停或移动到一个新核心内。这种行为将造成较差的数据局部性，从而对性能有<br> 负面影响。如果进程被移动到一个新CPU核心内，那么该进程中的任何数据都不会局部存<br> 储在新核的缓冲区内。这个进程需要重新从系统内存中获取所有数据。因此，将一个进程<br> 或线程绑定到一个单CPU核（或一组相邻的CPU核）上可以帮助提高主机性能。</p></li><li><p>限制进程或线程在特定的CPU核上执行，被称为CPU亲和性。有几个方法可以将进程<br> 和线程绑定到处理器上。一旦设置了亲和性，操作系统的调度程序必须服从设置约束，并<br> 且只能在指定的处理器上运行进程。</p></li><li><p>CPU亲和性直接影响MPI程序性能。MVAPICH2提供了一种方法，使其在运行时使用<br> MV2_ENABLE_AFFINITY环境变量来设置CPU亲和性。在调用MPI程序期间可以按照以下<br> 命令来启用CPU亲合性：<code>$ mpirun_rsh -np 2 node01 node02 MV2_ENABLE_AFFINITY=1 ./simplec2c</code></p></li><li><p>使用如下命令禁用CPU亲和性：<code>$ mpirun_rsh -np 2 node01 node02 MV2_ENABLE_AFFINITY=0 ./simplec2c</code></p></li><li><p>对于单线程或单进程的应用程序而言，启用CPU亲和性可以避免操作系统在处理器之<br> 间移动进程或线程，从而提供同等或更好的性能。另一方面，当禁用CPU亲和性时，多线<br> 程和多进程应用程序的性能可能会得到提升。</p></li></ul><h3 id="_9-5-2-使用传统mpi在gpu和gpu间传输数据" tabindex="-1"><a class="header-anchor" href="#_9-5-2-使用传统mpi在gpu和gpu间传输数据" aria-hidden="true">#</a> 9.5.2 使用传统MPI在GPU和GPU间传输数据</h3><ul><li><p>本节将用传统MPI结合CUDA从而在独立节点的GPU间交换数据。</p></li><li><p>考虑一个多节点计算集群，每个节点有多个GPU。节点内GPU之间的数据交换可以使<br> 用点对点访问或传输来实现，具体内容详见本章中9.1.2节。另一方面，在不同节点的GPU<br> 之间的数据交换需要一个节点间的通信库，如MPI。为了简化GPU间的数据交换和提高性<br> 能，应该在每个节点的每个GPU上绑定MPI进程。</p></li></ul><h4 id="_9-5-2-1-mpi-cuda程序内的亲和性" tabindex="-1"><a class="header-anchor" href="#_9-5-2-1-mpi-cuda程序内的亲和性" aria-hidden="true">#</a> 9.5.2.1 MPI-CUDA程序内的亲和性</h4><ul><li><p>在CPU核心中绑定MPI进程被称为CPU亲合性，与此类似，在特定的GPU中绑定MPI进<br> 程被称为GPU亲和性。在GPU中绑定MPI进程，通常是在使用MPI_Init函数初始化MPI环境<br> 之前进行的。</p></li><li><p>为了在一个节点中跨GPU均匀地分配进程，必须首先使用由MPI库提供的环境变量，<br> 确定节点内部进程的本地ID。例如，MAPICH2保证会为每个MPI进程设置环境变量<br> MV2_COMM_WORLD_LOCAL_RANK。MV2_COMM_WORLD_LOCAL_RANK是在同一节<br> 点的每个MPI进程中唯一标识的整数。其他的MPI实现提供类似的支持。这个本地ID，也<br> 可称为本地秩，可以将一个MPI进程绑定到一个CUDA设备上：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> n_devices<span class="token punctuation">;</span>
<span class="token keyword">int</span> local_rank <span class="token operator">=</span> <span class="token function">atoi</span><span class="token punctuation">(</span><span class="token function">getenv</span><span class="token punctuation">(</span><span class="token string">&quot;MV2_COMM_WORLD_LOCAL_RANK&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaGetDeviceCount</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>n_devices<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">int</span> device <span class="token operator">=</span> local_rank <span class="token operator">%</span> n_devices<span class="token punctuation">;</span>
<span class="token function">cudaSetDevice</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token function">MPI_Init</span><span class="token punctuation">(</span>argc<span class="token punctuation">,</span> argv<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>然而，如果首次使用环境变量MV2_ENABLE_AFFINITY设置MPI进程的CPU亲和性，<br> 然后用MV2_COMM_WORLD_LOCAL_RANK设置GPU亲和性，那么无法保证正在运行<br> MPI进程的CPU与分配的GPU是最佳组合。如果它们不是最佳组合，那么主机应用程序和<br> 设备内存之间的延迟和带宽可能会变得不理想。因此，可以使用便携的Hardware Locality<br> 包（hwloc）来分析节点的硬件拓扑结构，并且让MPI进程所在的CPU核与分配给该MPI进<br> 程的GPU是最佳组合。</p></li><li><p>下面的示例代码，使用了进程的MPI局部秩来选择一个GPU。然后，对于选定的<br> GPU，用hwloc确定最佳CPU核心来绑定这个进程。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>rank <span class="token operator">=</span> <span class="token function">atoi</span><span class="token punctuation">(</span> <span class="token function">getenv</span><span class="token punctuation">(</span> <span class="token string">&quot;MV2_COMM_WORLD_RANK&quot;</span> <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
local_rank <span class="token operator">=</span> <span class="token function">atoi</span><span class="token punctuation">(</span> <span class="token function">getenv</span><span class="token punctuation">(</span> <span class="token string">&quot;MV2_COMM_WORLD_LOCAL_RANK&quot;</span> <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// Load a full hardware topology of all PCI devices in this node.</span>
<span class="token function">hwloc_topology_init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>topology<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_topology_set_flags</span><span class="token punctuation">(</span>topology<span class="token punctuation">,</span> HWLOC_TOPOLOGY_FLAG_WHOLE_IO<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_topology_load</span><span class="token punctuation">(</span>topology<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// choose a GPU based on MPI local rank</span>
<span class="token function">cudaSetDevice</span><span class="token punctuation">(</span> local_rank <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaGetDevice</span><span class="token punctuation">(</span> <span class="token operator">&amp;</span>device <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// Iterate through all CPU cores that are physically close to the selected GPU.</span>
<span class="token comment">// This code evenly distributes processes across cores using local_rank.</span>
cpuset <span class="token operator">=</span> <span class="token function">hwloc_bitmap_alloc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_cudart_get_device_cpuset</span><span class="token punctuation">(</span>topology<span class="token punctuation">,</span> device<span class="token punctuation">,</span> cpuset<span class="token punctuation">)</span><span class="token punctuation">;</span>
match <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token function">hwloc_bitmap_foreach_begin</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>cpuset<span class="token punctuation">)</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>match <span class="token operator">==</span> local_rank<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 cpu <span class="token operator">=</span> i<span class="token punctuation">;</span>
 <span class="token keyword">break</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token operator">++</span>match<span class="token punctuation">;</span>
<span class="token function">hwloc_bitmap_foreach_end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// Bind this process to the selected CPU.</span>
onecpu <span class="token operator">=</span> <span class="token function">hwloc_bitmap_alloc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_bitmap_set</span><span class="token punctuation">(</span>onecpu<span class="token punctuation">,</span> cpu<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_set_cpubind</span><span class="token punctuation">(</span>topology<span class="token punctuation">,</span> onecpu<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// Cleanup.</span>
<span class="token function">hwloc_bitmap_free</span><span class="token punctuation">(</span>onecpu<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_bitmap_free</span><span class="token punctuation">(</span>cpuset<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">hwloc_topology_destroy</span><span class="token punctuation">(</span>topology<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">gethostname</span><span class="token punctuation">(</span> hostname<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>hostname<span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>
cpu <span class="token operator">=</span> <span class="token function">sched_getcpu</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;MPI rank %d using GPU %d and CPU %d on host %s\\n&quot;</span><span class="token punctuation">,</span>
 rank<span class="token punctuation">,</span> device<span class="token punctuation">,</span> cpu<span class="token punctuation">,</span> hostname <span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">MPI_Init</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>argc<span class="token punctuation">,</span> <span class="token operator">&amp;</span>argv<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">MPI_Comm_rank</span><span class="token punctuation">(</span>MPI_COMM_WORLD<span class="token punctuation">,</span> <span class="token operator">&amp;</span>rank<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span>MPI_SUCCESS <span class="token operator">!=</span> <span class="token function">MPI_Get_processor_name</span><span class="token punctuation">(</span>procname<span class="token punctuation">,</span> <span class="token operator">&amp;</span>length<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">strcpy</span><span class="token punctuation">(</span>procname<span class="token punctuation">,</span> <span class="token string">&quot;unknown&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_9-5-2-2-使用mpi执行gpu间的通信" tabindex="-1"><a class="header-anchor" href="#_9-5-2-2-使用mpi执行gpu间的通信" aria-hidden="true">#</a> 9.5.2.2 使用MPI执行GPU间的通信</h4><ul><li>一旦MPI进程通过cudaSetDevice函数被调度到一个GPU中，那么设备内存和主机固定<br> 内存可以被分配给当前设备：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">char</span> <span class="token operator">*</span>h_src<span class="token punctuation">,</span> <span class="token operator">*</span>h_rcv<span class="token punctuation">;</span>
<span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>h_src<span class="token punctuation">,</span> MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>h_rcv<span class="token punctuation">,</span> MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">char</span> <span class="token operator">*</span>d_src<span class="token punctuation">,</span> <span class="token operator">*</span>d_rcv<span class="token punctuation">;</span>
<span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_src<span class="token punctuation">,</span> MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_rcv<span class="token punctuation">,</span> MYBUFSIZE<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>使用传统MPI的两个GPU间的双向数据传输可以分两步执行：首先，将数据从设备内<br> 存复制到主机内存；其次，使用MPI通信库在MPI进程之间交换主机内存里的数据：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">if</span><span class="token punctuation">(</span>rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> loop<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_src<span class="token punctuation">,</span> d_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// bi-directional bandwidth</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>h_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>h_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_rcv<span class="token punctuation">,</span> h_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> loop<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>h_src<span class="token punctuation">,</span> d_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token comment">// bi-directional bandwidth</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>h_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>h_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">cudaMemcpy</span><span class="token punctuation">(</span>d_rcv<span class="token punctuation">,</span> h_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>从Wrox.com中可以下载含有这个示例完整代码的simpleP2P.c文件。用以下命令进行编<br> 译：<code>$ mpicc -std=c99 -O3 simpleP2P.c -o simplep2p</code></p></li><li><p>用mpirun_rsh启动MPI程序，如下所示：<code>$ mpirun_rsh -np 2 node01 node02 ./simplep2p</code></p></li><li><p>有两个Fermi M2090 GPU的系统，示例报告如下所示。要注意到，与CPU-to-CPU示例<br> 进行比较可知，此处带宽大幅减少，延迟显著增加了。这种性能损耗源于在使用MPI传输<br> 之前从GPU传输数据的额外开销。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token punctuation">.</span><span class="token operator">/</span>simplep2p to allocate <span class="token number">4</span> MB device memory <span class="token operator">and</span> pinned host memory CUDA <span class="token operator">+</span> MPI
node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>node01<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>node02<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">0</span>
 size Elapsed Performance
 <span class="token number">1</span> KB <span class="token number">10.24</span> μs <span class="token number">99.95</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> KB <span class="token number">11.51</span> μs <span class="token number">355.88</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> KB <span class="token number">19.77</span> μs <span class="token number">828.74</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> KB <span class="token number">46.34</span> μs <span class="token number">1414.24</span> MB<span class="token operator">/</span>sec
 <span class="token number">256</span> KB <span class="token number">114.31</span> μs <span class="token number">2293.37</span> MB<span class="token operator">/</span>sec
 <span class="token number">1</span> MB <span class="token number">394.07</span> μs <span class="token number">2660.89</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">1532.47</span> μs <span class="token number">2736.96</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_9-5-3-使用cuda-aware-mpi进行gpu到gpu的数据传输" tabindex="-1"><a class="header-anchor" href="#_9-5-3-使用cuda-aware-mpi进行gpu到gpu的数据传输" aria-hidden="true">#</a> 9.5.3 使用CUDA-aware MPI进行GPU到GPU的数据传输</h3><ul><li>MVAPICH2也是一个CUDA-aware MPI实现，它通过标准MPI API支持GPU到GPU的通<br> 信。可以直接把设备内存指针传给MPI函数（并且避免传统MPI所需的额外的cuda-Memcpy<br> 调用）：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">if</span><span class="token punctuation">(</span>rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> loop<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>d_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>d_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> loop<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">MPI_Irecv</span><span class="token punctuation">(</span>d_rcv<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span>
 <span class="token operator">&amp;</span>recv_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Isend</span><span class="token punctuation">(</span>d_src<span class="token punctuation">,</span> size<span class="token punctuation">,</span> MPI_CHAR<span class="token punctuation">,</span> other_proc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> MPI_COMM_WORLD<span class="token punctuation">,</span> 
 <span class="token operator">&amp;</span>send_request<span class="token punctuation">)</span><span class="token punctuation">;</span>
 
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>recv_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">MPI_Waitall</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>send_request<span class="token punctuation">,</span> <span class="token operator">&amp;</span>reqstat<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>从Wrox.com中可以下载含有这个示例完整代码的simpleP2P_CUDA_Aware.c文件。用<br> 以下命令编译它：<code>$ mpicc -std=c99 -O3 simpleP2P_CUDA_Aware.c -o simplep2p.aware</code></p></li><li><p>启动MPI程序之前，需要通过设置下列环境变量来确保在MVAPICH2上启用了CUDA<br> 支持。<code>$ export MV2_USE_CUDA=1</code></p></li><li><p>也可以在MPI程序调用时设置该环境变量：<code>$ mpirun_rsh -np 2 node01 node02 MV2_USE_CUDA=1 ./simplep2p.aware</code></p></li><li><p>带有两个Fermi M2090 GPU系统的示例报告如下。请注意，相比于在CUDA上使用传<br> 统MPI，使用CUDA-aware MPI，在用4MB消息的情况下，可以获得17%的性能提升。此<br> 外，该代码已被大大简化，没有与设备之间显式传输数据的步骤</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token punctuation">.</span><span class="token operator">/</span>simplep2p<span class="token punctuation">.</span>aware to allocate <span class="token number">4</span> MB device memory <span class="token operator">and</span> pinned host memory CUDA <span class="token operator">+</span> MPI
node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>node01<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>node02<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">0</span>
 size Elapsed Performance
 <span class="token number">1</span> KB <span class="token number">63.78</span> μs <span class="token number">16.06</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> KB <span class="token number">14.79</span> μs <span class="token number">277.03</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> KB <span class="token number">24.27</span> μs <span class="token number">675.21</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> KB <span class="token number">47.65</span> μs <span class="token number">1375.39</span> MB<span class="token operator">/</span>sec
 <span class="token number">256</span> KB <span class="token number">112.43</span> μs <span class="token number">2331.72</span> MB<span class="token operator">/</span>sec
 <span class="token number">1</span> MB <span class="token number">331.16</span> μs <span class="token number">3166.42</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">1306.02</span> μs <span class="token number">3211.50</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_9-5-4-使用cuda-aware-mpi进行节点内gpu到gpu的数据传输" tabindex="-1"><a class="header-anchor" href="#_9-5-4-使用cuda-aware-mpi进行节点内gpu到gpu的数据传输" aria-hidden="true">#</a> 9.5.4 使用CUDA-aware MPI进行节点内GPU到GPU的数据传输</h3><ul><li><p>在同一节点的两个GPU间，也可以使用CUDA-aware MPI库执行数据传输。如果两个<br> GPU连接在同一个PCIe总线上，会自动使用点对点传输。我们此处还使用前面用过的例<br> 子，在同一节点进行两GPU之间的数据传输，假定node01至少有两个GPU：<br><code>$ mpirun_rsh -np 2 node01 node01 MV2_USE_CUDA=1 ./simplep2p.aware</code></p></li><li><p>在有两个Fermi M2090的GPU上，结果如下。与预想的一致，在同一节点的GPU之间<br> 通过PCIe总线传输数据比跨节点互连的传输会有更好的带宽和延迟。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token punctuation">.</span><span class="token operator">/</span>simplep2p<span class="token punctuation">.</span>aware to allocate <span class="token number">4</span> MB device memory
node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>node01<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>node01<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">and</span> other_proc <span class="token operator">=</span> <span class="token number">0</span>
 size Elapsed Performance
 <span class="token number">1</span> KB <span class="token number">47.66</span> μs <span class="token number">21.48</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> KB <span class="token number">11.33</span> μs <span class="token number">361.38</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> KB <span class="token number">26.64</span> μs <span class="token number">615.02</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> KB <span class="token number">28.72</span> μs <span class="token number">2281.90</span> MB<span class="token operator">/</span>sec
 <span class="token number">256</span> KB <span class="token number">54.75</span> μs <span class="token number">4788.40</span> MB<span class="token operator">/</span>sec
 <span class="token number">1</span> MB <span class="token number">171.34</span> μs <span class="token number">6120.00</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">646.50</span> μs <span class="token number">6487.75</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>之前的4个示例的性能表现绘制在图9-10中。这些结果表明，当传输的数据大于1MB<br> 时，可从CUDA-aware MPI获得更好的性能。<br><img src="`+G+`" alt="figure9-10" loading="lazy"></li></ul><h3 id="_9-5-5-调整消息块大小" tabindex="-1"><a class="header-anchor" href="#_9-5-5-调整消息块大小" aria-hidden="true">#</a> 9.5.5 调整消息块大小</h3><ul><li><p>通过重叠主机与设备通信和节点间通信来最小化通信开销，MVAPICH2将来自GPU内<br> 存的大量信息自动划分成块。块的大小可以用MV2_CUDA_BLOCK_SIZE环境变量调整。<br> 默认的块大小是256 KB。它可以被设置为512 KB，命令如下所示：<br><code>$ mpirun_rsh -np 2 node01 node02 MV2_USE_CUDA=1 \\ MV2_CUDA_BLOCK_SIZE=524288 ./simplep2p.aware</code></p></li><li><p>在两个不同的节点上，当在两个Fermi M2090 GPU上运行时，改变块的大小会带来以<br> 下影响：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>size Elapsed Performance
 <span class="token number">1</span> KB <span class="token number">154.06</span> μs <span class="token number">6.65</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> KB <span class="token number">15.13</span> μs <span class="token number">270.63</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> KB <span class="token number">24.86</span> μs <span class="token number">659.05</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> KB <span class="token number">48.08</span> μs <span class="token number">1363.18</span> MB<span class="token operator">/</span>sec
 <span class="token number">256</span> KB <span class="token number">111.92</span> μs <span class="token number">2342.23</span> MB<span class="token operator">/</span>sec
 <span class="token number">1</span> MB <span class="token number">341.14</span> μs <span class="token number">3073.70</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">1239.15</span> μs <span class="token number">3384.82</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>以上结果显示块大小为4MB信息时性能最好，这表明较大尺寸的块性能更佳。</p></li><li><p>最优的块大小值取决于多个因素，包括互连带宽/延迟、GPU适配器的特点、平台的<br> 特点，以及MPI函数允许使用的内存大小。因此，最好使用不同大小的块来实验，从而判<br> 定哪一个性能最佳。</p></li></ul><h3 id="_9-5-6-使用gpudirect-rdma技术进行gpu到gpu的数据传输" tabindex="-1"><a class="header-anchor" href="#_9-5-6-使用gpudirect-rdma技术进行gpu到gpu的数据传输" aria-hidden="true">#</a> 9.5.6 使用GPUDirect RDMA技术进行GPU到GPU的数据传输</h3><ul><li><p>NVIDIA的GPUDirect实现了在PCIe总线上的GPU和其他设备之间的低延迟通信。使用<br> GPUDirect，第三方网络适配器和其他设备可以直接通过基于主机的固定内存区域交换数<br> 据，从而消除了不必要的主机内存复制，使得运行在多个设备上的应用程序的数据传输性<br> 能得到了显著提升。多年来，NVIDIA逐步更新GPUDirect技术，每一次发布都在不断地提<br> 升其可编程性和降低延迟。GPUDirect的第一个版本，与CUDA 3.1一同发布，允许<br> InfiniBand设备和GPU设备共享CPU内存中相同的锁页缓冲区。数据从一个节点中的GPU发<br> 送到另一个节点的GPU中，该数据是从源GPU复制到系统内存中固定的、共享的数据缓冲<br> 区，然后通过Infiniband互连直接从共享缓冲区复制到其他GPU可以访问的与目的节点相匹<br> 配的缓冲区（如图9-11描述所示）。<br><img src="`+y+'" alt="figure9-11" loading="lazy"></p></li><li><p>GPUDirect的第二个版本，与CUDA 4.0一同发布，加入了点对点API和在本章前面介<br> 绍的统一虚拟寻址支持。这些改进提高了单节点内多GPU的性能，并通过消除不同地址空<br> 间中管理多个指针的需要提高了程序员的效率。</p></li><li><p>GPUDirect的第三个版本，与CUDA 5.0一同发布，添加了远程直接内存访问<br> （RDMA）支持。RDMA允许通过Infiniband使用直接通信路径，它在不同集群节的GPU间<br> 使用标准的PCIe适配器。图9-12展示了两GPU在网络上的直接连接。使用GPUDirect<br> RDMA，在两个节点的GPU间通信可以在没有主机处理器参与的情况下执行。这减少了处<br> 理器的消耗和通信延迟。</p></li><li><p>因为GPUDirect RDMA对应用程序代码而言是透明的，所以可以使用相同的simpleP2P_CUDA_Aware.cu例子来比较MVAPICH2库和MVAPICH2-GDR库之间的性能，其中<br> MVAPICH2-GDR库包括GPUDirect RDMA支持。需要按正确的路径更新环境，使用MVAPI-CH2-GDR装置编译程序。<br><img src="'+I+`" alt="figure9-12" loading="lazy"></p></li><li><p>首先，通过使用了mpicc编译器和mpirun_rsh命令，获取CUDA-aware MVAPICH2性能<br> 基准：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ mpicc <span class="token operator">-</span>std<span class="token operator">=</span>c99 <span class="token operator">-</span>O3 simpleP2P_CUDA_Aware<span class="token punctuation">.</span>c <span class="token operator">-</span>o bibwCudaAware
$ mpirun_rsh <span class="token operator">-</span>np <span class="token number">2</span> ivb108 ivb110 MV2_USE_CUDA<span class="token operator">=</span><span class="token number">1</span> \\
 MV2_CUDA_BLOCK_SIZE<span class="token operator">=</span><span class="token number">524288</span> <span class="token punctuation">.</span><span class="token operator">/</span>bibwCudaAware
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>两个包含Kepler K40 GPU的节点产生的结果报告如下，它们由一个单轨道Mellanox<br> Connect-IB网络连接，并使用MVAPICH2 v2.0b：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>ivb108<span class="token punctuation">)</span><span class="token operator">:</span> my other _proc <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>ivb110<span class="token punctuation">)</span><span class="token operator">:</span> my other _proc <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">and</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">0</span>
 <span class="token number">1</span> MB <span class="token number">134.40</span> ms <span class="token number">7801.62</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">441.00</span> ms <span class="token number">9501.88</span> MB<span class="token operator">/</span>sec
 <span class="token number">16</span> MB <span class="token number">1749.55</span> ms <span class="token number">9589.42</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> MB <span class="token number">6553.89</span> ms <span class="token number">10239.55</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>相比之下，可以用CUDA-aware MVAPICH2-GDR版本中的mpicc编译相同的程序。确<br> 保设置MV2_USE_GPUDIRECT环境变量作为mpirun_rsh命令的一部分，代码如下所示：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ mpicc <span class="token operator">-</span>std<span class="token operator">=</span>c99 <span class="token operator">-</span>O3 simpleP2P_CUDA_Aware<span class="token punctuation">.</span>c <span class="token operator">-</span>o bibwCudaAwareGDR
Scaling Applications across GPU Clusters ❘ <span class="token number">421</span>
c09<span class="token punctuation">.</span>indd <span class="token number">08</span><span class="token operator">/</span><span class="token number">19</span><span class="token operator">/</span><span class="token number">2014</span> Page <span class="token number">421</span>
$ mpirun_rsh <span class="token operator">-</span>np <span class="token number">2</span> ivb108 ivb110 MV2_USE_CUDA<span class="token operator">=</span><span class="token number">1</span> MV2_USE_GPUDIRECT<span class="token operator">=</span><span class="token number">1</span> \\
 MV2_CUDA_BLOCK_SIZE<span class="token operator">=</span><span class="token number">524288</span> <span class="token punctuation">.</span><span class="token operator">/</span>bibwCudaAwareGDR
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在同一个集群上显示以下结果：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>to test max size <span class="token number">64</span> MB
node<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">(</span>ivb108<span class="token punctuation">)</span><span class="token operator">:</span> my other _proc <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">and</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">1</span>
node<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">(</span>ivb110<span class="token punctuation">)</span><span class="token operator">:</span> my other _proc <span class="token operator">=</span> <span class="token number">0</span> <span class="token operator">and</span> <span class="token keyword">using</span> GPU<span class="token operator">=</span><span class="token number">0</span>
 <span class="token number">1</span> MB <span class="token number">132.50</span> ms <span class="token number">8242.83</span> MB<span class="token operator">/</span>sec
 <span class="token number">4</span> MB <span class="token number">404.15</span> ms <span class="token number">10401.88</span>MB<span class="token operator">/</span>sec
 <span class="token number">16</span> MB <span class="token number">1590.29</span> ms <span class="token number">10801.00</span> MB<span class="token operator">/</span>sec
 <span class="token number">64</span> MB <span class="token number">5929.98</span> ms <span class="token number">11539.94</span> MB<span class="token operator">/</span>sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>图9-13直观地比较了以下两个测试示例中的双向带宽：<br> ·CUDA-aware MPI<br> ·具有GPUDirect RDMA的CUDA-aware MPI<br><img src="`+D+'" alt="figure9-13" loading="lazy"></p></li><li><p>当GPUDirect RDMA被添加到CUDA-aware MPI时，性能得到了显著的提升（达到13%）。</p></li><li><p>请注意，因为使用CUDA加速了应用程序的计算分配，所以应用程序中的I/O将迅速成<br> 为整体性能的阻碍。GPUDirect通过减少GPU之间的延迟，提供一个直截了当的解决方案。</p></li></ul><h2 id="_9-6-总结" tabindex="-1"><a class="header-anchor" href="#_9-6-总结" aria-hidden="true">#</a> 9.6 总结</h2><ul><li><p>多GPU系统非常适合处理现实中那些单GPU无法处理的超大数据集的问题，或者是那<br> 些吞吐量和效率可以通过使用多GPU系统得到提升的问题。通常，执行多GPU应用程序有<br> 两种配置：<br> ·单节点上的多设备<br> ·多节点GPU加速集群上的多设备</p></li><li><p>本章涵盖了用在粗细粒度上进行多GPU编程的技术和API，展示了如何管理多个GPU<br> 以及如何从主机应用程序中发出指令。</p></li><li><p>MVAPICH2是一个CUDA-aware MPI通用的实现形式，它使用InfiniBand、<br> 10GigE/iWARP和RoCE网络技术，实现了高端计算系统所需的低延迟、高带宽、可扩展性<br> 和容错性。通过MPI函数直接传递设备内存，大大减化了MPI-CUDA程序的开发，提高了<br> GPU加速集群的性能。</p></li><li><p>GPUDirect促进了点对点设备内存的访问。使用GPUDirect，可以直接在多个设备上交<br> 换数据，这些设备属于一个集群内的相同节点或不同节点。在没有中转数据通过CPU内存<br> 的情况下，也可以发生这个交换。GPUDirect的RDMA功能可以使第三方设备直接访问<br> GPU全局内存，如固态硬盘、网络接口卡和InfiniBand适配器，这使得那些设备和GPU间的<br> 延迟显著减少了。</p></li><li><p>CUDA提供了许多在多设备上管理和执行核函数的方法。在一个计算节点内跨多设备<br> 可以扩展应用程序，或跨GPU加速集群节点来扩展应用程序。使用计算来隐藏通信延迟的<br> 负载平衡，可以实现近似线性的性能增益。</p></li></ul><h2 id="_9-7-习题" tabindex="-1"><a class="header-anchor" href="#_9-7-习题" aria-hidden="true">#</a> 9.7 习题</h2>',54),nn={href:"http://1.xn--simpleMultiGPU-k79vm57csi5e3v9e.cu",target:"_blank",rel:"noopener noreferrer"},sn=n("br",null,null,-1),an=o("<li><p>2.使用nvprof运行之前的可执行程序，代码如下所示，并确定使用两个设备与一个设<br> 备有什么不同。<code>$ nvprof --print-gpu-trace ./simpleMultiGPUEvents 1</code></p></li><li><p>3.使用nvvp运行之前的可执行程序，代码如下所示：<code>$ nvvp ./simpleMultiGPU</code><br> 检查控制台选项卡和详细信息选项卡中的结果。接着，在设置选项卡上设置参数为1，<br> 并且只在一个GPU上运行代码。把结果与在两个GPU上运行的结果进行比较。</p></li><li><p>4.把下列代码放在simpleMultiGPU主循环的最后：<code>cudaStreamSynchronize(stream[i]);</code><br> 对于使用一个GPU和两个GPU的情况，分别使用nvvp重新编译并运行代码，然后将结<br> 果与练习9.3中代码结果相比较，并且解释造成差异的原因。</p></li><li><p>5.在simpleMultiGPU.cu中，把数据初始化（initialData）移到主内核循环中。使用nvvp<br> 运行看看有什么变化。</p></li><li><p>6.参考文件simpleP2P_PingPong.cu，修改注释为unidirectional gmem copy的代码使其使<br> 用异步复制。</p></li><li><p>7.参考文件simpleP2P_PingPong.cu，基于双向异步ping-pong示例，使用以下函数，在<br> 该文件中添加两个GPU间的ping-pongs数据：<br><code>cudaMemcpyPeerAsync(void* dst,int dstDev,const void* src,int srcDev,size_t count, cudaStream_t stream)</code></p></li>",6),pn={href:"http://8.xn--simpleP2P-PingPong-3s52ay51eoj9fy05f.cu",target:"_blank",rel:"noopener noreferrer"},en=n("br",null,null,-1),tn={href:"http://9.xn--simpleP2P-PingPong-3s52ay51eoj9fy05f.cu",target:"_blank",rel:"noopener noreferrer"},on=n("br",null,null,-1),cn={href:"http://10.xn--simple2DFD-0m4pp47bwj0d8q9d.cu",target:"_blank",rel:"noopener noreferrer"},ln=n("br",null,null,-1),un=n("br",null,null,-1),rn=n("br",null,null,-1),kn=n("br",null,null,-1),dn=o("<li><p>11.解释CPU和GPU亲和性是如何影响每一个应用程序的执行时间的。假设运行了<br> CUDA-MPI应用程序两次，代码如下：<br><code>$ mpirun_rsh -np 2 node01 MV2_ENABLE_AFFINITY=1 ./simplec2c</code><br><code>$ mpirun_rsh -np 2 node01 MV2_ENABLE_AFFINITY=0 ./simplec2c</code><br> 第一个命令启用CPU亲和性，而第二个不启用。在以上两种情况下，描述你将使用什<br> 么技术为每个MPI进程创建GPU亲和性，并且解释其原因。</p></li><li><p>12.GPUDirect RDMA是什么？它是如何提高性能的？描述GPUDirect的3个版本？使用<br> GPUDirect RDMA对硬件和软件有什么要求？</p></li><li><p>13.如何使用MPI函数，cudaMemcpyAsync和流调用来建立一个simpleP2P.c的异步版本？</p></li><li><p>14.参考文件simpleP2P.c。将固定主机内存改为分页主机内存，看看性能会发生什么<br> 样的变化并说明理由。如果不能运行它，则描述期望的结果。</p></li><li><p>15.思考简单的P2P_CUDA_Aware.c。在没有GPUdirect的平台上（即不能在PCIe设备之<br> 间直接传递数据），当传递一个设备指针时，你认为MPI_Isend是如何工作的？</p></li><li><p>16.MVAPICH的CUDA-aware MPI允许更改复制数据的块大小。描述块的大小是如何影<br> 响CUDA-Aware MPI的内部性能的。为什么较大的块一般表现会更好？</p></li>",6);function mn(bn,vn){const e=i("router-link"),u=i("CodeTabs"),t=i("ExternalLinkIcon");return k(),d("div",null,[A,w,m(" more "),n("nav",B,[n("ul",null,[n("li",null,[a(e,{to:"#简单介绍主要是基础"},{default:p(()=>[s("简单介绍主要是基础")]),_:1})]),n("li",null,[a(e,{to:"#第9章-多gpu编程"},{default:p(()=>[s("第9章 多GPU编程")]),_:1})]),n("li",null,[a(e,{to:"#_9-1-从一个gpu到多gpu"},{default:p(()=>[s("9.1 从一个GPU到多GPU")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_9-1-1-在多gpu上执行"},{default:p(()=>[s("9.1.1 在多GPU上执行")]),_:1})]),n("li",null,[a(e,{to:"#_9-1-2-点对点通信"},{default:p(()=>[s("9.1.2 点对点通信")]),_:1})]),n("li",null,[a(e,{to:"#_9-1-3-多gpu间的同步"},{default:p(()=>[s("9.1.3 多GPU间的同步")]),_:1})])])]),n("li",null,[a(e,{to:"#_9-2-多gpu间细分计算"},{default:p(()=>[s("9.2 多GPU间细分计算")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_9-2-1-在多设备上分配内存"},{default:p(()=>[s("9.2.1 在多设备上分配内存")]),_:1})]),n("li",null,[a(e,{to:"#_9-2-2-单主机线程分配工作"},{default:p(()=>[s("9.2.2 单主机线程分配工作")]),_:1})]),n("li",null,[a(e,{to:"#_9-2-3-编译和执行"},{default:p(()=>[s("9.2.3 编译和执行")]),_:1})])])]),n("li",null,[a(e,{to:"#_9-3-多gpu上的点对点通信"},{default:p(()=>[s("9.3 多GPU上的点对点通信")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_9-3-1-实现点对点访问"},{default:p(()=>[s("9.3.1 实现点对点访问")]),_:1})]),n("li",null,[a(e,{to:"#_9-3-2-点对点的内存复制"},{default:p(()=>[s("9.3.2 点对点的内存复制")]),_:1})]),n("li",null,[a(e,{to:"#_9-3-3-统一虚拟寻址的点对点内存访问"},{default:p(()=>[s("9.3.3 统一虚拟寻址的点对点内存访问")]),_:1})])])]),n("li",null,[a(e,{to:"#_9-4-多gpu上的有限差分"},{default:p(()=>[s("9.4 多GPU上的有限差分")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_9-4-1-二维波动方程的模板计算"},{default:p(()=>[s("9.4.1 二维波动方程的模板计算")]),_:1})]),n("li",null,[a(e,{to:"#_9-4-2-多gpu程序的典型模式"},{default:p(()=>[s("9.4.2 多GPU程序的典型模式")]),_:1})]),n("li",null,[a(e,{to:"#_9-4-3-多gpu上的二维模板计算"},{default:p(()=>[s("9.4.3 多GPU上的二维模板计算")]),_:1})]),n("li",null,[a(e,{to:"#_9-4-4-重叠计算与通信"},{default:p(()=>[s("9.4.4 重叠计算与通信")]),_:1})]),n("li",null,[a(e,{to:"#_9-4-5-编译和执行"},{default:p(()=>[s("9.4.5 编译和执行")]),_:1})])])]),n("li",null,[a(e,{to:"#_9-5-跨gpu集群扩展应用程序"},{default:p(()=>[s("9.5 跨GPU集群扩展应用程序")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_9-5-1-cpu到cpu的数据传输"},{default:p(()=>[s("9.5.1 CPU到CPU的数据传输")]),_:1})]),n("li",null,[a(e,{to:"#_9-5-2-使用传统mpi在gpu和gpu间传输数据"},{default:p(()=>[s("9.5.2 使用传统MPI在GPU和GPU间传输数据")]),_:1})]),n("li",null,[a(e,{to:"#_9-5-3-使用cuda-aware-mpi进行gpu到gpu的数据传输"},{default:p(()=>[s("9.5.3 使用CUDA-aware MPI进行GPU到GPU的数据传输")]),_:1})]),n("li",null,[a(e,{to:"#_9-5-4-使用cuda-aware-mpi进行节点内gpu到gpu的数据传输"},{default:p(()=>[s("9.5.4 使用CUDA-aware MPI进行节点内GPU到GPU的数据传输")]),_:1})]),n("li",null,[a(e,{to:"#_9-5-5-调整消息块大小"},{default:p(()=>[s("9.5.5 调整消息块大小")]),_:1})]),n("li",null,[a(e,{to:"#_9-5-6-使用gpudirect-rdma技术进行gpu到gpu的数据传输"},{default:p(()=>[s("9.5.6 使用GPUDirect RDMA技术进行GPU到GPU的数据传输")]),_:1})])])]),n("li",null,[a(e,{to:"#_9-6-总结"},{default:p(()=>[s("9.6 总结")]),_:1})]),n("li",null,[a(e,{to:"#_9-7-习题"},{default:p(()=>[s("9.7 习题")]),_:1})])])]),x,n("details",R,[S,a(u,{id:"703",data:[{id:"demo1"},{id:"demo2"}],"tab-id":"shell"},{title0:p(({value:c,isActive:l})=>[s("demo1")]),title1:p(({value:c,isActive:l})=>[s("demo2")]),tab0:p(({value:c,isActive:l})=>[E]),tab1:p(({value:c,isActive:l})=>[O]),_:1})]),z,n("ul",null,[n("li",null,[n("p",null,[n("a",q,[s("从Wrox.com中可以下载包含完整示例代码的文件simple2DFD.cu"),a(t)]),s("。通过以下代码进行"),H,s(" 编译："),V])]),T]),L,n("ul",null,[W,n("li",null,[n("p",null,[s("如下所示，是几个商业化且开源的CUDA-aware MPI实现："),F,s(" ·MVAPICH22.0rc2（"),n("a",K,[s("http://mvapich.cse.ohio-state.edu/download/mvapich2/）"),a(t)]),N,s(" ·MVAPICH2-GDR 2.0b（"),n("a",$,[s("http://mvapich.cse.ohio-state.edu/download/mvapich2gdr/）"),a(t)]),Y,s(" ·OpenMPI 1.7（"),n("a",Z,[s("http://www.open-mpi.org/software/ompi/v1.7/）"),a(t)]),j,s(" ·CRAY MPI（MPT 5.6.2）"),X,s(" ·IBM Platform MPI（8.3）")])]),J]),Q,n("ul",null,[n("li",null,[n("p",null,[n("a",nn,[s("1.参考文件simpleMultiGPU.cu"),a(t)]),s("，使用事件记录GPU运行时间，并替换CPU计时器的代"),sn,s(" 码，然后比较结果。")])]),an,n("li",null,[n("p",null,[n("a",pn,[s("8.参考文件simpleP2P-PingPong.cu"),a(t)]),s("，在异步内存复制运行时函数里使用一个默认流，"),en,s(" 将其结果与使用非默认流进行比较。")])]),n("li",null,[n("p",null,[n("a",tn,[s("9.参考文件simpleP2P-PingPong.cu"),a(t)]),s("，首先禁用点对点访问，然后分别比较单向与双向"),on,s(" 内存复制、同步与异步函数的结果。")])]),n("li",null,[n("p",null,[n("a",cn,[s("10.参考文件simple2DFD.cu"),a(t)]),s("，用下面的逻辑重新安排波传播："),ln,s(" （1）在流halo上计算halo。"),un,s(" （2）在流halo上交换halo。"),rn,s(" （3）在流内计算内部区域。"),kn,s(" （4）同步每个设备。将结果与原结果进行比较，并解释性能变化的原因。")])]),dn])])}const gn=r(C,[["render",mn],["__file","I-第九章.html.vue"]]);export{gn as default};
