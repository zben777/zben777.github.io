import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as a,o as r,c as s,d as p,a as l,b as e,e as n,f as t}from"./app-2a2d189a.js";const c={},_=l("h1",{id:"a-线性神经网络笔记",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#a-线性神经网络笔记","aria-hidden":"true"},"#"),e(" A-线性神经网络笔记")],-1),d=l("p",null,"A-线性神经网络笔记",-1),h={class:"hint-container note"},u=l("p",{class:"hint-container-title"},"链接",-1),m={href:"https://blog.csdn.net/weixin_51429773/article/details/134180958",target:"_blank",rel:"noopener noreferrer"},f=l("h2",{id:"一、关于梯度下降的思考",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#一、关于梯度下降的思考","aria-hidden":"true"},"#"),e(" 一、关于梯度下降的思考")],-1),x=t("<li><p>对于过程中的求偏导；公式是WX+b；是把未知数W 作为 自变量了；然后X是输入已经知道的数据；所以对于w进行求导；损失函数Y是关于W的损失函数；</p></li><li><p>关于梯度问题：先拿一个样本来说，因为最终也是对于一个样本数据进行输入然后经过模型进行输出的。</p><ul><li><p>一个样本的话就是进行多维度的梯度的话就是：w的各个切线的反方向就是损失函数下降的方向；、</p></li><li><p>现在想象 关于 自变量W的函数即损失函数是一个简单曲线；y值往下降的方向，就是w的各切线的方向的反方向；求偏导的；</p></li><li><p>然后就是n维度向量同理即可；这样就解释了一个样本的反向传播的通过梯度下降达到参数更新的目的；</p></li><li><p>比如 y=x*W<sup>2</sup>; 那么关于W的求导的方向是 关于 x 的数值；x是样本的给的x的数值；那么岂不是对于每个样本就有一个本样本<mark>固定的 方向</mark>？</p></li><li><p>然后目的是找一套W参数即自变量的数值使得满足 n 各样本 并且 使得 损失函数 y 数值 尽量 最低；</p></li><li><p>开始想象大量样本的总体情况就是；一套参数适用于所有的样本；就是取平均即比如最后的损失取平均做为这次参数更新的评价，并且每次更新w都是要遍历全部样本的；然后不断的进行；但是w参数并不是取平均目前来看：</p></li><li><p>还有就是：假设一个样本一个样本的进行去整参数会过拟合之类的；所以</p></li><li><p>但是如果100个样本进行整体参数更新的话；那会不会每个样本的就是每个样本的整体梯度下降的方向不一样的；拿二维来说的话：基本就是每个样本的输入的x就是自变量w的系数了；</p></li></ul></li>",2),g={href:"https://zhuanlan.zhihu.com/p/580645925",target:"_blank",rel:"noopener noreferrer"},w=t("<li><p>关于样本数量以及梯度下降和反向传播的思考</p></li><li><p>所以说是这样的</p></li><li><p>参数更新是一个参数一个参数进行更新的；求偏导；然后呢就是其它wi就为0了；然后就是这样；比如上面这个图；线性回归的梯度下降</p></li><li><p>就是第m个参数就是使用了n个样本以及X里面的最后一列的x(i,m)这一列数据进行参数更新的；</p></li><li><p>看上面那个图片的最后那个推导公式：对于第j个w的梯度是求解的过程中涉及的数据也就是n个样本的y值和n个样本的第j个变量特征的数值；其它的变量特征的数值都没有相对的用到；因为f(xi)基本就是此时的损失函数的数值是此次已经求的得到的结果了；因为是反向传播即结果是明显的知道的东西</p></li><li><p>此刻损失函数是包括了xij所有的x的即n，m的矩阵了；即样本矩阵数据；</p></li><li><p>当然其他神经网络的是推导太难；其他的所以直接调用梯度下降</p></li>",7),b=l("figure",null,[l("img",{src:"https://cdn.jsdelivr.net/gh/zben777/figures/img/202310301334035.png",alt:"图片",tabindex:"0",loading:"lazy"}),l("figcaption",null,"图片")],-1),k=l("ul",null,[l("li",null,"以方向的视角来看整个线性模型"),l("li",null,"首先每个自变量W的方向是由样本数据的某一列特征数据 决定的；"),l("li",null,"然后整个模型的方向 是 由 所有的自变量w的方向 矢量决定的；"),l("li",null,"而线性的是这样简单的；而神经网络就 不那么整齐了；")],-1);function W(v,y){const i=a("ExternalLinkIcon");return r(),s("div",null,[_,d,p(" more "),l("div",h,[u,l("ul",null,[l("li",null,[l("a",m,[e("3.线性神经网络-3GPT版"),n(i)])])])]),f,l("ul",null,[x,l("li",null,[l("p",null,[l("a",g,[e("梯度下降详解（主观理解+推导证明+例题）"),n(i)])])]),w]),b,k])}const j=o(c,[["render",W],["__file","A-线性神经网络笔记.html.vue"]]);export{j as default};
