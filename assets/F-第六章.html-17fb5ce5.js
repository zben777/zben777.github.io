import{_ as r}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as k,c as d,d as m,a as n,e as t,w as a,b as s,f as c}from"./app-2a2d189a.js";const b="/assets/figure6-1-066ac1b3.png",v="/assets/figure6-2-7d1c2d09.png",g="/assets/figure6-3-a7310070.png",_="/assets/figure6-4-5829b35c.png",h="/assets/figure6-5-83d09271.png",f="/assets/figure6-6-f166788c.png",y="/assets/figure6-7-42eef587.png",C="/assets/figure6-8-cf788ca5.png",A="/assets/figure6-9-f6b698c4.png",U="/assets/figure6-10-2293ac7a.png",D="/assets/figure6-11-a00e7ec3.png",E="/assets/figure6-12-4bf6efb8.png",w="/assets/figure6-13-1fd7d4e9.png",S="/assets/figure6-14-42b924c5.png",P="/assets/figure6-15-97d53e4c.png",x="/assets/figure6-16-2d11d912.png",M="/assets/figure6-17-639a6f51.png",G={},H=n("h1",{id:"f-第六章",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#f-第六章","aria-hidden":"true"},"#"),s(" F-第六章")],-1),T=n("p",null,"F-第六章",-1),I={class:"table-of-contents"},N=c(`<h2 id="简单介绍主要是基础" tabindex="-1"><a class="header-anchor" href="#简单介绍主要是基础" aria-hidden="true">#</a> 简单介绍主要是基础</h2><div class="hint-container info"><p class="hint-container-title">说明</p><p>主要是各种搜索找的学习；</p><p>主题：CUDA核心GPU编程</p><p>前置条件：</p><ul><li>应具备C++编程知识</li><li>需理解内存管理，如malloc和free</li><li>理解STL及其模板机制</li><li>需配置NVIDIA显卡，型号需为900系列或更高</li><li>所用扩展要求版本为11或更新</li><li>编译器版本不低于11</li><li>CMake版本需在3.18以上</li></ul></div><div class="hint-container info"><p class="hint-container-title">问题</p><ul><li>回调函数啊；</li></ul></div><h2 id="第6章-流和并发" tabindex="-1"><a class="header-anchor" href="#第6章-流和并发" aria-hidden="true">#</a> 第6章 流和并发</h2><ul><li><p>本章内容：<br> ·理解流和事件的本质<br> ·理解网格级并发<br> ·重叠内核执行和数据传输<br> ·重叠CPU和GPU执行<br> ·理解同步机制<br> ·避免不必要的同步<br> ·调整流的优先级<br> ·注册设备回调函数<br> ·通过NVIDIA可视化性能分析器显示应用程序执行的时间轴</p></li><li><p>一般来说，在CUDA C编程中有两个级别的并发：<br> ·内核级并发<br> ·网格级并发</p></li><li><p>到目前为止，你的关注点可能仅限于内核级的并发，在此级别的并发中，单一的任务<br> 或内核被GPU的多个线程并行执行。前面几章已经介绍了提升内核性能的几种方法，它们<br> 分别是从编程模型、执行模型和内存模型的角度进行介绍的。想必你已经了解了一些通过<br> 命令行性能分析器来研究和分析内核行为的方法。</p></li><li><p>本章将研究网格级的并发。在网格级并发中，多个内核在同一设备上同时执行，这往<br> 往会让设备利用率更好。在本章中，你将学习到如何使用CUDA流实现网格级的并发。还<br> 将使用CUDA的可视化性能分析器nvvp将内核并发执行可视化。</p></li></ul><h2 id="_6-1-流和事件概述" tabindex="-1"><a class="header-anchor" href="#_6-1-流和事件概述" aria-hidden="true">#</a> 6.1 流和事件概述</h2><ul><li><p>CUDA流是一系列异步的CUDA操作，这些操作按照主机代码确定的顺序在设备上执<br> 行。流能封装这些操作，保持操作的顺序，允许操作在流中排队，并使它们在先前的所有<br> 操作之后执行，并且可以查询排队操作的状态。这些操作包括在主机与设备间进行数据传<br> 输，内核启动以及大多数由主机发起但由设备处理的其他命令。流中操作的执行相对于主<br> 机总是异步的。CUDA运行时决定何时可以在设备上执行操作。我们的任务是使用CUDA<br> 的API来确保一个异步操作在运行结果被使用之前可以完成。在同一个CUDA流中的操作<br> 有严格的执行顺序，而在不同CUDA流中的操作在执行顺序上不受限制。使用多个流同时<br> 启动多个内核，可以实现网格级并发。【流就像一个队列，这个队列装有异步的CUDA操作】</p></li><li><p>因为所有在CUDA流中排队的操作都是异步的，所以在主机与设备系统中可以重叠执<br> 行其他操作。在同一时间内将流中排队的操作与其他有用的操作一起执行，可以隐藏执行<br> 那些操作的开销。</p></li><li><p>在本书中，CUDA编程的一个典型模式是以下形式：<br> 1.将输入数据从主机移到设备上。<br> 2.在设备上执行一个内核。<br> 3.将结果从设备移回主机中。</p></li><li><p>在许多情况下，执行内核比传输数据耗时更多。在这些情况下，可以完全隐藏CPU和<br> GPU之间的通信延迟。通过将内核执行和数据传输调度到不同的流中，这些操作可以重<br> 叠，程序的总运行时间将被缩短。流在CUDA的API调用粒度上可实现流水线或双缓冲技<br> 术。【这个是什么？？？】</p></li><li><p>CUDA的API函数一般可以分为同步或异步。具有同步行为的函数会阻塞主机端线<br> 程，直到它们完成。具有异步行为的函数被调用后，会立即将控制权归还给主机。异步函<br> 数和流是在CUDA中构建网格级并发的<mark>两个基本支柱</mark>。</p></li><li><p>从软件的角度来看，CUDA操作在不同的流中并发运行；而从硬件上来看，不一定总<br> 是如此。根据PCIe总线争用或每个SM资源的可用性，<mark>完成不同的CUDA流可能仍然需要<br> 互相等待。</mark>【还是受到资源的限制】</p></li><li><p>在本章中，你可以仔细研究在有多种计算能力的设备上流是如何运行的。</p></li></ul><h3 id="_6-1-1-cuda流" tabindex="-1"><a class="header-anchor" href="#_6-1-1-cuda流" aria-hidden="true">#</a> 6.1.1 CUDA流</h3><ul><li><p>所有的CUDA操作（包括内核和数据传输）都在一个流中显式或隐式地运行。流分为两种类型：<br> ·隐式声明的流（空流）<br> ·显式声明的流（非空流）</p></li><li><p>如果没有显式地指定一个流，那么内核启动和数据传输将默认使用空流。本书中前面<br> 章节所使用的例子都是空流或默认流。</p></li><li><p>另一方面，非空流可以被显式地创建和管理。如果想要重叠不同的CUDA操作，必须<br> 使用非空流。基于流的异步的内核启动和数据传输支持以下类型的粗粒度并发：<br> ·重叠主机计算和设备计算<br> ·重叠主机计算和主机与设备间的数据传输<br> ·重叠主机与设备间的数据传输和设备计算<br> ·并发设备计算</p></li><li><p>思考下面使用默认流的代码：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> cudaMemcpyHostToDevice<span class="token punctuation">)</span><span class="token punctuation">;</span>
kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMemcpy</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> cudaMemcpyDeviceToHost<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>要想理解一个CUDA程序，应该从设备和主机两个角度去考虑。从设备的角度来看，<br> 上述代码中所有的3个操作都被发布到默认的流中，并且按发布顺序执行。设备不知道其<br> 他被执行的主机操作。从主机的角度来看，每个数据传输都是同步的，在等待它们完成<br> 时，将强制空闲主机时间。内核启动是异步的，所以无论内核是否完成，主机的应用程序<br> 几乎都立即恢复执行。这种内核启动的默认异步行为使它可以直接重叠设备和主机计算。<br> 所以host的话是：强制空闲 + 恢复执行 + 强制空闲？<br> 然后如果仅仅是调用核函数的话，直接就是可以直接重叠设备和主机计算。</li></ul><br><ul><li>数据传输也可以被异步发布，但是必须显式地设置一个CUDA流来装载它们。CUDA<br> 运行时提供了以下cudaMemcpy函数的异步版本：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>
cudaError_t <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span> dst<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">void</span><span class="token operator">*</span> src<span class="token punctuation">,</span> size_t count<span class="token punctuation">,</span>
 cudaMemcpyKind kind<span class="token punctuation">,</span> cudaStream_t stream <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>请注意附加的流标识符作为第五个参数。默认情况下，流标识符被设置为默认流。这<br> 个函数与主机是异步的，所以调用发布后，控制权将立即返回到主机。将复制操作和非空<br> 流进行关联是很容易的，但是首先需要使用如下代码创建一个非空流：<br><code>cudaError_t cudaStreamCreate(cudaStream_t* pStream);</code></p></li><li><p>cudaStreamCreate创建了一个可以显式管理的非空流。之后，返回到pStream中的流就<br> 可以被当作流参数供cudaMemcpyAsync和其他异步CUDA的API函数来使用。在使用异步<br> CUDA函数时，常见的疑惑在于，它们可能会从先前启动的异步操作中返回错误代码。因<br> 此返回错误的API调用并不一定是产生错误的那个调用。<br> 【这个是个问题就是返回错误不知道流中的哪个异步CUDA操作】</p></li><li><p>还有就是创建stream到底是哪个呢？</p></li><li><p><code>cudaStream_t stream</code>:这个是声明</p></li><li><p><code>cudaError_t cudaStreamCreate(cudaStream_t* pStream);</code>：这个是创建</p></li></ul><br><ul><li>当执行异步数据传输时，必须使用固定（或非分页的）主机内存。<br> 可以使用cudaMallocHost函数或cudaHostAlloc函数分配固定内存：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>cudaError_t <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span>ptr<span class="token punctuation">,</span> size_t size<span class="token punctuation">)</span><span class="token punctuation">;</span>
cudaError_t <span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span>pHost<span class="token punctuation">,</span> size_t size<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> flags<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>在主机虚拟内存中固定分配，可以确保其在CPU内存中的物理位置在应用程序的整个<br> 生命周期中保持不变。否则，操作系统可以随时自由改变主机虚拟内存的物理位置。如果<br> 在没有固定主机内存的情况下执行一个异步CUDA转移操作，操作系统可能会在物理层面<br> 上移动数组，而CUDA操作运行时将该数组移动到设备中，这样会导致未定义的行为。</p></li><li><p>在非默认流中启动内核，必须在内核执行配置中提供一个流标识符作为第四个参数：<br><code>kernel_name&lt;&lt;&lt;grid, block, sharedMemSize, stream&gt;&gt;&gt;(argument list);</code></p></li><li><p>一个非默认流声明如下：<code>cudaStream_t stream;</code></p></li><li><p>非默认流可以使用如下方式进行创建：<code>cudaStreamCreate(&amp;stream);</code></p></li><li><p>可以使用如下代码释放流中的资源：<code>cudaError_t cudaStreamDestroy(cudaStream_t stream);</code></p></li><li><p>在一个流中，当cudaStreamDestroy函数被调用时，如果该流中仍有未完成的工作，<br> cudaStreamDestroy函数将立即返回，当流中所有的工作都已完成时，与流相关的资源将被<br> 自动释放。</p></li><li><p>因为所有的CUDA流操作都是异步的，所以CUDA的API提供了两个函数来检查流中所<br> 有操作是否都已经完成：<br><code>cudaError_t cudaStreamSynchronize(cudaStream_t stream);</code><br><code>cudaError_t cudaStreamQuery(cudaStream_t stream);</code></p></li><li><p>cudaStreamSynchronize强制阻塞主机，直到在给定流中所有的操作都完成了。【相当于同步】<br> cudaStreamQuery会检查流中所有操作是否都已经完成，但在它们完成前不会阻塞主机。当所<br> 有操作都完成时cudaStreamQuery函数会返回cudaSuccess，当一个或多个操作仍在执行或等<br> 待执行时返回cudaErrorNotReady。</p></li><li><p>为了说明在实践中如何使用CUDA流，下面是一个在多个流中调度CUDA操作的常见模式。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nStreams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">int</span> offset <span class="token operator">=</span> i <span class="token operator">*</span> bytesPerStream<span class="token punctuation">;</span> 
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">[</span>offset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>a<span class="token punctuation">[</span>offset<span class="token punctuation">]</span><span class="token punctuation">,</span> bytePerStream<span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token generic-function"><span class="token function">kernel</span><span class="token generic class-name"><span class="token operator">&lt;&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span></span></span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_a<span class="token punctuation">[</span>offset<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">[</span>offset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>d_a<span class="token punctuation">[</span>offset<span class="token punctuation">]</span><span class="token punctuation">,</span> bytesPerStream<span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nStreams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaStreamSynchronize</span><span class="token punctuation">(</span>streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+b+'" alt="figure6-1" tabindex="0" loading="lazy"><figcaption>figure6-1</figcaption></figure><ul><li><p>图6-1所示为一个简单的时间轴，展示了使用3个流的CUDA操作。数据传输和内核计<br> 算都是均匀分布在3个并发流中的。</p></li><li><p>在图6-1中，数据传输操作虽然分布在不同的流中，但是并没有并发执行。这是由一<br> 个共享资源导致的：PCIe总线。虽然从编程模型的角度来看这些操作是独立的，但是因为<br> 它们共享一个相同的硬件资源，所以它们的执行必须是串行的。具有双工PCIe总线的设备<br> 可以重叠两个数据传输，但它们必须在不同的流中以及不同的方向上。在图6-1中可以观<br> 察到，在一个流中从主机到设备的数据传输与另一个流中从设备到主机的数据传输是重叠<br> 的。</p></li><li><p>并发内核的最大数量是依赖设备而确定的。Fermi设备支持16路并发，Kepler设备支<br> 持32路并发。设备上可用的计算资源进一步限制了并发内核的数量，如共享内存和寄存<br> 器。在本章后面的例子中将会探索这些局限性。<br> 【感觉内核并发还是有点疑问，毕竟资源限制；感觉一个内核就够抢共享内存和寄存器这些了】</p></li></ul><h3 id="_6-1-2-流调度" tabindex="-1"><a class="header-anchor" href="#_6-1-2-流调度" aria-hidden="true">#</a> 6.1.2 流调度</h3><ul><li>从概念上讲，所有的流可以同时运行。但是，当将流映射到物理硬件时并不总是这样<br> 的。本节将说明如何通过硬件调度多个CUDA流内的并发内核操作。<br> 【就是说按照概念，所有的流中的内核是可以并发的，但是数据传输由于PCIe总线不行】<br> 【按照概念，设备支持多少路并发，大概就是能够多少个流中的内核进行并发执行】</li></ul><h4 id="_6-1-2-1-虚假的依赖关系" tabindex="-1"><a class="header-anchor" href="#_6-1-2-1-虚假的依赖关系" aria-hidden="true">#</a> 6.1.2.1 虚假的依赖关系</h4><figure><img src="'+v+'" alt="figure6-2" tabindex="0" loading="lazy"><figcaption>figure6-2</figcaption></figure><ul><li>虽然Fermi GPU支持16路并发，即多达16个网格同时执行，但是所有的流最终是被多<br> 路复用<mark>到单一的硬件工作队列中的</mark>。当选择一个网格执行时，在队列前面的任务由CUDA<br> 运行时调度。运行时检查任务的依赖关系，如果仍有任务在执行，那么将等待该任务依赖<br> 的任务执行完。最后，当所有依赖关系都执行结束时，新任务被调度到可用的SM中。这<br> 种单一流水线可能会导致虚假的依赖关系。如图6-2所示，最终只有带圆圈的任务对被并<br> 行执行，因为在启动其他网格前，运行时将会被阻塞。在工作队列中，一个被阻塞的操作<br> 会将队列中该操作后面的所有操作都阻塞，即使它们属于不同的流。<br> 【即 单一的硬件工作队列中 排着：流1 流2 流3........】<br> 【就是说在硬件工作队列中，只要前面有 CUDA操作被 阻塞了，后面只能等待】</li></ul><h4 id="_6-1-2-2-hyper-q技术" tabindex="-1"><a class="header-anchor" href="#_6-1-2-2-hyper-q技术" aria-hidden="true">#</a> 6.1.2.2 Hyper-Q技术</h4><figure><img src="'+g+'" alt="figure6-3" tabindex="0" loading="lazy"><figcaption>figure6-3</figcaption></figure><ul><li>Kepler GPU家族中的Hyper-Q技术，使用多个硬件工作队列，从而减少了虚假的依赖<br> 关系。Hyper-Q技术通过在主机和设备之间维持多个硬件管理上的连接，允许多个CPU线<br> 程或进程在单一GPU上同时启动工作。被Fermi架构中虚假依赖关系限制的应用程序，在<br> 不改变任何现有代码的情况下可以看到显著的性能提升。Kepler GPU使用32个硬件工作队<br> 列，每个流分配一个工作队列。如果创建的流超过32个，多个流将共享一个硬件工作队<br> 列。这样做的结果是可实现全流级并发，并且其具有最小的虚假流间依赖关系。图6-3展<br> 示了一个简单的案例，3个流在3个硬件工作队列上。<br> 【这样的话B就不会阻塞流2，流3中的CUDA异步操作】</li></ul><h3 id="_6-1-3-流的优先级" tabindex="-1"><a class="header-anchor" href="#_6-1-3-流的优先级" aria-hidden="true">#</a> 6.1.3 流的优先级</h3><ul><li><p>对计算能力为3.5或更高的设备，可以给流分配优先级。使用下面的函数可以创建一<br> 个具有特定优先级的流：<br><code>cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority);</code></p></li><li><p>这个函数创建了一个具有指定整数优先级的流，并在pStream中返回一个句柄。这个<br> 优先级是与pStream中的工作调度相关的。高优先级流的网格队列可以优先占有低优先级<br> 流已经执行的工作。流优先级不会影响数据传输操作，只对计算内核有影响。如果指定的<br> 优先级超出了设备定义的范围，它会被自动限制为定义范围内的最低值或最高值。对于一<br> 个给定的设备，可以使用以下函数查询优先级的允许范围：<br><code>cudaError_t cudaDeviceGetStreamPriorityRange(int *leastPriority, int *greatestPriority);</code></p></li><li><p>这个函数的返回值存放在leastPriority和greatestPriority中，分别对应于当前设备的最低<br> 和最高优先级。按照惯例，一个较低的整数值表示更高的优先级。如果当前的设备不支持<br> 流优先级，cudaDeviceGetStreamPriorityRange将0返回给这两个参数。<br> 【目前感觉没啥用？】</p></li></ul><h3 id="_6-1-4-cuda事件" tabindex="-1"><a class="header-anchor" href="#_6-1-4-cuda事件" aria-hidden="true">#</a> 6.1.4 CUDA事件</h3><ul><li><p>CUDA中事件本质上是CUDA流中的标记，它与该流内操作流中特定点相关联。可以<br> 使用事件来执行以下两个基本任务：<br> ·同步流的执行<br> ·监控设备的进展</p></li><li><p>CUDA的API提供了在流中任意点插入事件以及查询事件完成的函数。只有当一个给<br> 定CUDA流中先前的所有操作都执行结束后，记录在该流内的事件才会起作用（即完<br> 成）。在默认流中指定的事件，适用于CUDA流中先前所有的操作。<br> 【就是说我们可以在一个stream中加入标记事件】</p></li></ul><h4 id="_6-1-4-1-创建和销毁" tabindex="-1"><a class="header-anchor" href="#_6-1-4-1-创建和销毁" aria-hidden="true">#</a> 6.1.4.1 创建和销毁</h4><ul><li><p>一个事件声明如下：<code>cudaEvent_t event;</code></p></li><li><p>一旦被声明，事件可以使用如下代码进行创建：<code>cudaError_t cudaEventCreate(cudaEvent_t* event);</code></p></li><li><p>使用如下代码销毁一个事件：<code>cudaError_t cudaEventDestroy(cudaEvent_t event);</code></p></li><li><p>当cudaEventDestroy函数被调用时，如果事件尚未起作用，则调用立即返回，当事件<br> 被标记完成时自动释放与该事件相关的资源。</p></li></ul><h4 id="_6-1-4-2-记录事件和计算运行时间" tabindex="-1"><a class="header-anchor" href="#_6-1-4-2-记录事件和计算运行时间" aria-hidden="true">#</a> 6.1.4.2 记录事件和计算运行时间</h4><ul><li><p>事件在流执行中标记了一个点。它们可以用来检查正在执行的流操作是否已经到达了<br> 给定点。它们可以被看作是添加到CUDA流中的操作，当从工作队列中取出时，这个操作<br> 的唯一作用就是通过主机端标志来指示完成的状态。一个事件使用如下函数排队进入<br> CUDA流：<br><code>cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);</code></p></li><li><p>已经排队进入CUDA流中的事件可用于等待或测试在指定流中先前操作的完成情况。<br> 等待一个事件会阻塞主机线程的调用，它可以用下面的函数来执行：<br><code>cudaError_t cudaEventSynchronize(cudaEvent_t event);</code><br> 【就是说我们可以在CPU端进行设置这个事件同步，那么该流中这个事件前的CUDA异步操作完成标志】</p></li><li><p>对于流来说，cudaEventSynchronize函数类似于cudaStreamSynchronize函数，<br> 但cudaEventSynchronize函数允许主机等待<mark>流执行的中间点</mark>。</p></li><li><p>可以使用如下代码测试一个事件是否可以不用阻塞主机应用程序来完成：<br><code>cudaError_t cudaEventQuery(cudaEvent_t event);</code></p></li><li><p>cudaEventQuery函数类似于cudaStreamQuery函数，但这是对于事件来说的。<br> 下面的函数用来计算被两个事件标记的CUDA操作的运行时间：<br><code>cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop);</code></p></li><li><p>此函数返回事件启动和停止之间的运行时间，以毫秒为单位。事件的启动和停止不必<br> 在同一个CUDA流中。请注意，如果在非空流中记录启动事件或停止事件时，返回的时间<br> 可能比预期的要大。这是因为cudaEventRecord函数是异步的，并且不能保证计算的延迟正<br> 好处于两个事件之间。</p></li><li><p>下面的示例代码演示了如何将事件用于时间设备操作：</p></li><li><p>在这里，启动和停止事件被默认放置到空流中。一个时间戳记录空流开始时的启动事<br> 件，另一个时间戳记录空流结束时的停止事件。然后，使用cudaEventElapsedTime函数得<br> 到两个事件之间的运行时间。</p></li></ul>',38),z={class:"hint-container details"},B=n("summary",null,"Click me to view the code!",-1),O=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token comment"},"// create two events"),s(`
cudaEvent_t start`),n("span",{class:"token punctuation"},","),s(" stop"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaEventCreate"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("start"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaEventCreate"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// record start event on the default stream"),s(`
`),n("span",{class:"token function"},"cudaEventRecord"),n("span",{class:"token punctuation"},"("),s("start"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// execute kernel"),s(`
kernel`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),s("arguments"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// record stop event on the default stream"),s(`
`),n("span",{class:"token function"},"cudaEventRecord"),n("span",{class:"token punctuation"},"("),s("stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// wait until the stop event completes"),s(`
`),n("span",{class:"token function"},"cudaEventSynchronize"),n("span",{class:"token punctuation"},"("),s("stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// calculate the elapsed time between two events"),s(`
`),n("span",{class:"token keyword"},"float"),s(" time"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaEventElapsedTime"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("time"),n("span",{class:"token punctuation"},","),s(" start"),n("span",{class:"token punctuation"},","),s(" stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// clean up the two events"),s(`
`),n("span",{class:"token function"},"cudaEventDestroy"),n("span",{class:"token punctuation"},"("),s("start"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaEventDestroy"),n("span",{class:"token punctuation"},"("),s("stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),q=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s("yarn add "),n("span",{class:"token operator"},"-"),s("D vuepress"),n("span",{class:"token operator"},"-"),s("theme"),n("span",{class:"token operator"},"-"),s(`hope
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"})])],-1),F=c(`<h3 id="_6-1-5-流同步" tabindex="-1"><a class="header-anchor" href="#_6-1-5-流同步" aria-hidden="true">#</a> 6.1.5 流同步</h3><ul><li><p>在非默认流中，所有的操作对于主机线程都是非阻塞的，因此会遇到需要在同一个流<br> 中运行主机和运算操作同步的情况。【就是说我需要CPU和stream中的CUDA异步操作同步一下】</p></li><li><p>从主机的角度来说，CUDA操作可以分为两大类：<br> ·内存相关操作<br> ·内核启动</p></li><li><p>对于主机来说，内核启动总是异步的。许多内存操作本质上是同步的（如cudaMemcpy），<br> 但是CUDA运行时也为内存操作的执行提供了异步函数。</p></li><li><p>正如前面介绍的，有两种类型的流：<br> ·异步流（非空流）<br> ·同步流（空流/默认流）</p></li><li><p>在主机上非空流是一个异步流，其上所有的操作都不阻塞主机执行。另一方面，被隐<br> 式声明的空流是主机上的同步流。大多数添加到空流上的操作都会导致主机在先前所有的<br> 操作上阻塞，主要的异常是内核启动.</p></li><li><p>非空流可进一步被分为以下两种类型：<br> ·阻塞流<br> ·非阻塞流</p></li><li><p>虽然非空流在主机上是非阻塞的，<mark>但是非空流内的操作可以被空流中的操作所阻塞</mark>。<br> 如果一个非空流是阻塞流，则空流可以阻塞该非空流中的操作。如果一个非空流是非阻塞<br> 流，则它不会阻塞空流中的操作。在下面的部分中，将介绍如何使用阻塞流和非阻塞流。</p></li></ul><h4 id="_6-1-5-1-阻塞流和非阻塞流" tabindex="-1"><a class="header-anchor" href="#_6-1-5-1-阻塞流和非阻塞流" aria-hidden="true">#</a> 6.1.5.1 阻塞流和非阻塞流</h4><ul><li><p>使用cudaStreamCreate函数创建的流是阻塞流，这意味着在这些流中操作执行可以被<br> 阻塞，一直等到空流中先前的操作执行结束。空流是隐式流，在相同的CUDA上下文中它<br> 和其他所有的阻塞流同步。一般情况下，当操作被发布到空流中，在该操作被执行之前，<br> CUDA上下文会等待所有先前的操作发布到所有的阻塞流中。此外，任何发布到阻塞流中<br> 的操作，会被挂起等待，直到空流中先前的操作执行结束才开始执行。</p></li><li><p>例如，下面的代码中，在stream_1中启动核函数kernel_1，在空流中启动kernel_2，在<br> stream_2中启动kernel_3：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream_1<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream_2<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>这段代码的结果是，直到核函数kernel_1执行结束，kernel_2才会在GPU上开始执行，<br> kernel_2执行结束后，kernel_3才开始执行。请注意，从主机的角度来看，每一个内核启动<br> 仍然是异步和非阻塞的。【空流在中间把 stream_2给阻塞了】</p></li><li><p>CUDA运行时提供了一个定制函数，它是关于空流的非空流行为，代码如下：<br><code>cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags);</code></p></li><li><p>flags参数决定了所创建流的行为。flags的有效值如下所示：<br><code>cudaStreamDefault: default stream creation flag (blocking)</code><br><code>cudaStreamNonBlocking: asynchronous stream creation flag (non-blocking)</code></p></li><li><p>指定cudaStreamNonBlocking使得非空流对于空流的阻塞行为失效。在前面的例子中，<br> 如果stream_1和stream_2都使用cudaStreamNonBlocking进行创建，那么所有核函数的执行都<br> 不会被阻塞，都不用等待其他核函数执行结束。</p></li></ul><h4 id="_6-1-5-2-隐式同步" tabindex="-1"><a class="header-anchor" href="#_6-1-5-2-隐式同步" aria-hidden="true">#</a> 6.1.5.2 隐式同步</h4><ul><li><p>CUDA包括两种类型的主机-设备同步：显式和隐式。在前面已经介绍了许多执行显<br> 式同步的函数，如cudaDeviceSynchronize，cudaStreamSynchronize以及cudaEventSynchronize函数。<br> 这些函数被主机显式调用，使得在设备上任务执行和主机线程同步。在应<br> 用程序的逻辑点中，可以手动插入显式同步调用。</p></li><li><p>前文中也已经介绍了隐式同步的例子。例如，调用cudaMemcpy函数，可以隐式同步<br> 设备和主机，这是由于主机的应用程序在数据传输完成之前会被阻塞。然而，由于此函数<br> 的主要目的不是同步，因此其同步的产生是隐式的。理解隐式同步是很重要的，因为无意<br> 中调用隐式同步主机和设备的函数，可能会导致意想不到的性能下降。</p></li><li><p>隐式同步在CUDA编程中特别吸引编程人员的注意，因为带有隐式同步行为的运行时<br> 函数可能会导致不必要的阻塞，这种阻塞通常发生在设备层面。许多与内存相关的操作意<br> 味着在当前设备上所有先前的操作上都有阻塞，例如：<br> ·锁页主机内存分配<br> ·设备内存分配<br> ·设备内存初始化<br> ·同一设备上两个地址之间的内存复制<br> ·一级缓存/共享内存配置的修改</p></li></ul><h4 id="_6-1-5-3-显式同步" tabindex="-1"><a class="header-anchor" href="#_6-1-5-3-显式同步" aria-hidden="true">#</a> 6.1.5.3 显式同步</h4><ul><li><p>CUDA运行时在网格级支持显式同步CUDA程序的几种方法：<br> ·同步设备<br> ·同步流<br> ·同步流中的事件<br> ·使用事件跨流同步</p></li><li><p>使用下述函数可以阻塞一个主机线程直到设备完所有先前的任务：<br><code>cudaError_t cudaDeviceSynchronize(void);</code></p></li><li><p>这个函数使主机线程等待直到所有和当前设备相关的计算和通信完成。因为这是一个<br> 比较重要的同步函数，所以应该尽量少使用该函数，以免拖延主机运行。</p></li><li><p>使用cudaStreamSynchronize函数可以阻塞主机线程直到流中所有的操作完成为止，使<br> 用cudaStreamQuery函数可以完成非阻塞测试，两个函数代码如下：<br><code>cudaError_t cudaStreamSynchronize(cudaStream_t stream);</code><br><code>cudaError_t cudaStreamQuery(cudaStream_t stream);</code></p></li><li><p>使用下述的cudaEventSynchronize函数和cudaEventQuery函数，CUDA事件也可以用于<br> 细粒度阻塞和同步：<br><code>cudaError_t cudaEventSynchronize(cudaEvent_t event);</code><br><code>cudaError_t cudaEventQuery(cudaEvent_t event);</code></p></li><li><p>此外，cudaStreamWaitEvent函数提供了一个使用CUDA事件引入流间依赖关系比较灵<br> 活的方法：<code>cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event);</code></p></li><li><p>在流中执行任何排队的操作之前，并且在cudaStreamWaitEvent调用之后，cudaStreamWaitEvent函数能使指定流等待指定事件。该事件可能与同一个流相关，也可能与不同的流相关。在后者的情况下，这个函数执行跨流同步，如图6-4所示。在这里，流2发布的等待可以确保在流1创建的事件是满足依赖关系的，然后继续。</p></li></ul><figure><img src="`+_+`" alt="figure6-4" tabindex="0" loading="lazy"><figcaption>figure6-4</figcaption></figure><h4 id="_6-1-5-4-可配置事件" tabindex="-1"><a class="header-anchor" href="#_6-1-5-4-可配置事件" aria-hidden="true">#</a> 6.1.5.4 可配置事件</h4><ul><li><p>CUDA运行时提供了一种方式来定制事件的行为和性能，代码如下：<br><code>cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);</code></p></li><li><p>有效的标志包括下面4个：<br> cudaEventDefault<br> cudaEventBlockingSync<br> cudaEventDisableTiming<br> cudaEventInterprocess</p></li><li><p>其中，cudaEventBlockingSync指定使用cudaEventSynchronize函数同步事件会阻塞调用<br> 的线程。cudaEventSynchronize函数的默认操作是围绕事件进行的，使用CPU周期不断检查<br> 事件的状态。将标志设置成cudaEventBlockingSync，调用的线程在另一个将要休眠的线程<br> 或进程上运行，而不是放弃核心，直到事件满足依赖关系。如果其他有用的工作可以被执<br> 行，那么这样会减少CPU周期的浪费，但是这也会使事件满足依赖关系以及激活调用线程<br> 之间的延迟被加长。</p></li><li><p>设置cudaEventDisableTiming表明创建的事件只能用来进行同步，不需要记录时序数<br> 据。除去时间戳花费的总开销提高了调用cudaStreamWaitEvent和cudaEventQuery函数调用<br> 的性能。</p></li><li><p>标志设置为cudaEventInterprocess表明创建的事件可能被用作进程间事件</p></li></ul><h2 id="_6-2-并发内核执行" tabindex="-1"><a class="header-anchor" href="#_6-2-并发内核执行" aria-hidden="true">#</a> 6.2 并发内核执行</h2><ul><li>前面已经解释了流、事件和同步的概念以及API，接下来用几个例子来演示一下。第<br> 一个示例演示了如何使用多个流并发运行多个核函数。这个简单的例子将介绍并发内核执<br> 行的几个基本问题，包括以下几个方面：<br> ·使用深度优先或广度优先方法的调度工作<br> ·调整硬件工作队列<br> ·在Kepler设备和Fermi设备上避免虚假的依赖关系<br> ·检查默认流的阻塞行为<br> ·在非默认流之间添加依赖关系<br> ·检查资源使用是如何影响并发的</li></ul><h3 id="_6-2-1-非空流中的并发内核" tabindex="-1"><a class="header-anchor" href="#_6-2-1-非空流中的并发内核" aria-hidden="true">#</a> 6.2.1 非空流中的并发内核</h3><ul><li>在本节中，将使用NVIDIA的可视化性能分析器（nvvp）可视化并发核函数执行。在<br> 该例子中使用的核函数包括在设备上仿真有用工作的虚拟计算。这确保了内核驻留在GPU<br> 中的时间足够长，以使重叠在可视化性能分析器中更加明显。注意这个例子使用了多个相<br> 同的核函数（被称为kernel_1，kernel_2，…）：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">kernel_1</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">double</span> sum <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">;</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 sum <span class="token operator">=</span> sum <span class="token operator">+</span> <span class="token function">tan</span><span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">tan</span><span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>这样做是为了在nvvp中更容易将不同内核的执行进行可视化。</p></li><li><p>首先必须要创建一组非空流。这组非空流中，发布每个流中的内核启动应该在GPU上<br> 同时运行，但是应不存在由于硬件资源限制而导致的虚假依赖关系。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>cudaStream_t <span class="token operator">*</span>streams <span class="token operator">=</span> <span class="token punctuation">(</span>cudaStream_t <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>n_streams <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cudaStream_t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span> <span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">cudaStreamCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>使用一个循环遍历所有的流，这样内核在每个流中都可以被调度：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>dim3 <span class="token function">block</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
dim3 <span class="token function">grid</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_4<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>这些内核启动的执行配置被指定为单一线程块中的单一线程，以保证有足够的GPU资<br> 源能并发运行所有的内核。因为每个内核启动相对于主机来说都是异步的，所以可以通过<br> 使用单一主机线程同时调度多个内核到不同的流中。</p></li><li><p>在本例中，为了计算运行时间，也创建了两个事件：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>cudaEvent_t start<span class="token punctuation">,</span> stop<span class="token punctuation">;</span>
<span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventCreate</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在启动所有的内核循环前，启动事件就已经被记录在默认流中了。而在所有的内核启<br> 动后，停止事件也被记录在默认流中。【像是在时空长河中插入了标记；插入到了默认流】</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>start<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token punctuation">}</span>
<span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在同步停止事件后可以计算运行时间：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">cudaEventSynchronize</span><span class="token punctuation">(</span>stop<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaEventElapsedTime</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>elapsed_time<span class="token punctuation">,</span> start<span class="token punctuation">,</span> stop<span class="token punctuation">)</span><span class="token punctuation">;</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这个示例代码可以从Wrox.com的simpleHyperqDepth.cu文件中下载。它可以通过nvcc来<br> 编译，在Tesla K40上运行时，输出如下结果：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleHyperq
<span class="token operator">&gt;</span> Using Device <span class="token number">0</span><span class="token operator">:</span> Tesla K40c with num_streams<span class="token operator">=</span><span class="token number">4</span> 
<span class="token operator">&gt;</span> Compute Capability <span class="token number">3.5</span> hardware with <span class="token number">15</span> multi<span class="token operator">-</span>processors
Measured time <span class="token keyword">for</span> parallel execution <span class="token operator">=</span> <span class="token number">0.079</span>s
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>NVIDIA的可视化性能分析器（nvvp）包含在CUDA工具包中。nvvp便于收集性能指标<br> （类似于nvprof），也同样便于结果的可视化。在这个例子中，nvvp可以用来显示内核的<br> 时间轴。下面的命令将首先从simpleHyperq的一个样本执行中收集执行数据，然后可视化<br> 并发内核执行：<code>$ nvvp ./simpleHyperq</code></p></li><li><p>图6-5显示了在Tesla K40中通过nvvp生成的时间轴。随着时间进度条向右移动，每种<br> 颜色对应不同内核的执行，并且每行对应不同的流。正如所期望的，在K40上可以看到4<br> 个并发内核在4个不同的流中执行。【主要原因还是资源管够】<br><img src="`+h+`" alt="figure6-5" loading="lazy"></p></li><li><p>还有一点就是如何使用事件对时间进行计时的呢？毕竟事件是被插入到了默认流中的；【？】</p></li></ul><h3 id="_6-2-2-fermi-gpu上的虚假依赖关系" tabindex="-1"><a class="header-anchor" href="#_6-2-2-fermi-gpu上的虚假依赖关系" aria-hidden="true">#</a> 6.2.2 Fermi GPU上的虚假依赖关系</h3><ul><li>为了演示虚假的依赖关系，可以在Fermi设备上运行相同的代码。在Tesla M2090上<br> simpleHyperq函数的输出如下：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleHyperq
<span class="token operator">&gt;</span> Using Device <span class="token number">0</span><span class="token operator">:</span> Tesla M2090 with num_streams<span class="token operator">=</span><span class="token number">4</span>
<span class="token operator">&gt;</span> GPU does <span class="token operator">not</span> support HyperQ
<span class="token operator">&gt;</span> CUDA kernel runs will have limited concurrency
<span class="token operator">&gt;</span> Compute Capability <span class="token number">2.0</span> hardware with <span class="token number">16</span> multi<span class="token operator">-</span>processors
Measured time <span class="token keyword">for</span> parallel execution <span class="token operator">=</span> <span class="token number">0.342</span>s
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>simpleHyperq告诉我们，Fermi设备不支持Hyper-Q，而且内核最终会限制并发一起运行。</p></li><li><p>图6-6显示了与图6-5相同应用程序的时间轴，但不同的是它运行在Fermi GPU上。因<br> 为在Fermi设备上有虚假的依赖关系，所以4个流不能同时启动，这是由共享硬件工作队列<br> 造成的。为什么流i＋1能够在流i开始其最后任务时开始它的第一个任务呢？因为两个任<br> 务是在不同的流中，所以它们之间没有依赖关系。当流i的最后一个任务被启动时，CUDA<br> 运行时从工作队列中调度下一个任务，这是流i＋1的第一个任务。因为每个流的第一个任<br> 务不依赖于之前的任何任务，并且有可用的SM，所以它可以立即启动。之后，调度流i＋<br> 1的第二个任务，然而它对第一个任务的依赖却阻止它被执行，这就会导致任务执行再次<br> 被阻塞。【这个前面的一个阻塞后面的全部，可以理解明白】<br> 【但是为什么同一个stream中，两个任务会出现依赖呢？又不是输入输出的依赖，<br> 难道仅仅是在同一个stream中就会出现这样？应该就是这样！】<br> 【所以叫做虚假的依赖关系】<br><img src="`+f+'" alt="figure6-6" loading="lazy"></p></li></ul><br><ul><li><p>这种虚假的依赖关系是由主机调度内核的顺序引起的。该应用程序使用深度优先的方<br> 法，在下一个流启动前，在该流中启动全系列的操作。利用深度优先方法得到的工作队列<br> 中的任务顺序如图6-7所示。由于所有流被多路复用到一个硬件工作队列中，所以前面的<br> 流就连续阻塞了后面的流。<br><img src="'+y+`" alt="figure6-7" loading="lazy"></p></li><li><p>在Fermi GPU上，为了避免虚假的依赖关系，可以用广度优先的方法从主机中调度工作：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code> <span class="token comment">// dispatch job with breadth first way </span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> 
    kernel_4<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>采用广度优先顺序可以确保工作队列中相邻的任务来自于不同的流（如图6-8所<br> 示）。因此，任何相邻的任务对之间都不会再有虚假的依赖关系，从而得以实现并发内核<br> 执行。【所以虚假的依赖关系就是：同一个流中的CUDA异步操作因为在同一个stream中必须顺序】<br><img src="`+C+`" alt="figure6-8" loading="lazy"></p></li><li><p>从Wrox.com中可以下载simpleHyperqBreadth.cu文件，其中包含完整的示例代码，在<br> Fermi M2090 GPU上编译并且运行，结果输出如下：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleHyperqBreadth
<span class="token operator">&gt;</span> Using Device <span class="token number">0</span><span class="token operator">:</span> Tesla M2090 with num_streams <span class="token number">4</span>
<span class="token operator">&gt;</span> GPU does <span class="token operator">not</span> support HyperQ
<span class="token operator">&gt;</span> CUDA kernel runs will have limited concurrency
<span class="token operator">&gt;</span> Compute Capability <span class="token number">2.0</span> hardware with <span class="token number">16</span> multi<span class="token operator">-</span>processors
Measured time <span class="token keyword">for</span> parallel execution <span class="token operator">=</span> <span class="token number">0.105</span>s
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>要注意，此处的执行时间相比采用深度优先的方法已提高了3倍。内核启动调度可以<br> 用nvvp来证实。图6-9展示了用广度优先方法的内核执行时间轴：所有流同步启动。<br><img src="`+A+`" alt="figure6-9" loading="lazy"></li></ul><h3 id="_6-2-3-使用openmp的调度操作" tabindex="-1"><a class="header-anchor" href="#_6-2-3-使用openmp的调度操作" aria-hidden="true">#</a> 6.2.3 使用OpenMP的调度操作</h3><ul><li><p>前面的示例中，使用单一的主机线程将异步CUDA操作调度到多个流中。本节的示例<br> 将使用多个主机线程将操作调度到多个流中，并使用一个线程来管理每一个流。</p></li><li><p>OpenMP是CPU的并行编程模型，它使用编译器指令来识别并行区域。支持OpenMP指<br> 令的编译器可以将它们用作如何并行化应用程序的提示。用很少的代码，在主机上就可以<br> 实现多核并行。</p></li><li><p>在使用OpenMP的同时使用CUDA，不仅可以提高便携性和生产效率，而且还可以提<br> 高主机代码的性能。在simpleHyperQ例子中，我们使用了一个循环调度操作，与此不同，<br> 我们使用了OpenMP线程调度操作到不同的流中，具体方法如下所示：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">omp_set_num_threads</span><span class="token punctuation">(</span>n_streams<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">pragma</span> <span class="token expression">omp parallel</span></span>
<span class="token punctuation">{</span>
 <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token function">omp_get_thread_num</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 kernel_4<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>OpenMp函数omp_set_num_threads用来指定在OpenMP并行区域里要用到的CPU核心的<br> 数量。编译器指令#pragma omp parallel将花括号之间的代码标记为并行部分。<br> omp_get_thread_num函数为每个主机线程返回唯一一个线程ID，将该ID用作streams数组中<br> 的索引，用来创建OpenMP线程和CUDA流间的一对一映射。【奇妙】<br> 【但是好像并不一定能够保证，并且每个线程并不一定同步了执行代码指令】</p></li><li><p>从Wrox.com上下载simpleHyperqOpenmp.cu文件。用nvcc进行编译，使用-Xcom-piler选<br> 项将标识传递给支持OpenMP的主机编译器。<br><code>$ nvcc -O3 -Xcompiler -fopenmp simpleHyperqOpenmp.cu -o simpleHyperqOpenmp -lgomp</code></p></li><li><p>在Kepler 40设备上测试simpleHyperqOpenmp，与之前没有OpenMP的simpleHyperQ测<br> 试产生相同的性能：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>simpleHyperqOpenmp
CUDA_DEVICE_MAX_CONNECTIONS <span class="token operator">=</span> <span class="token number">32</span>
<span class="token operator">&gt;</span> Using Device <span class="token number">0</span><span class="token operator">:</span> Tesla K40c with num_streams <span class="token number">4</span>
<span class="token operator">&gt;</span> Compute Capability <span class="token number">3.5</span> hardware with <span class="token number">15</span> multi<span class="token operator">-</span>processors
<span class="token operator">&gt;</span> grid <span class="token number">1</span> block <span class="token number">1</span>
Measured time <span class="token keyword">for</span> parallel execution <span class="token operator">=</span> <span class="token number">0.079</span>s
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>什么时候从OpenMP中调度并行CUDA操作是有用的？在一般情况下，如果每个流在<br> 内核执行之前、期间或之后有额外的工作待完成，那么它可以包含在同一个OpenMP并行<br> 区域里，并且跨流和线程进行重叠。这样做更明显地说明了每个OpenMP线程中的主机工<br> 作与同一个线程中启动的流CUDA操作是相关的，并且可以为了优化性能简化代码的书写。<br> 【确实是个奇妙的结合：CPU的每个thread 负责 一个 stream 并且消除虚假的依赖关系】</li><li>在第10章中将介绍关于OpenMP和CUDA的详细内容，具体内容参见10.4节。</li></ul><h3 id="_6-2-4-用环境变量调整流行为" tabindex="-1"><a class="header-anchor" href="#_6-2-4-用环境变量调整流行为" aria-hidden="true">#</a> 6.2.4 用环境变量调整流行为</h3><ul><li><p>支持Hyper-Q的GPU在主机和每个GPU之间维护硬件工作队列，消除虚假的依赖关<br> 系。Kepler设备支持的硬件工作队列的最大数量是32。然而，默认情况下并发硬件连接的<br> 数量被限制为8。由于每个连接都需要额外的内存和资源，所以设置默认的限制为8，减少<br> 了不需要全部32个工作队列的应用程序的资源消耗。可以使用<br> CUDA_DEVICE_MAX_CONNECTIONS环境变量来调整并行硬件连接的数量，对于Kepler<br> 设备而言，其上限是32。</p></li><li><p>有几种设置该环境变量的方法。在Linux中，可以根据shell的版本，通过以下代码进<br> 行设置，对于Bash和Bourne Shell，其代码如下：<code>export CUDA_DEVICE_MAX_CONNECTIONS=32</code></p></li><li><p>对于C-Shell，其代码如下：<code>setenv CUDA_DEVICE_MAX_CONNECTIONS 32</code></p></li><li><p>这个环境变量也可以直接在C主机程序中进行设定：<code>setenv(&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;, &quot;32&quot;, 1);</code></p></li><li><p>每个CUDA流都会被映射到单一的CUDA设备连接中。如果流的数量超过了硬件连接<br> 的数量，多个流将共享一个连接。当多个流共享相同的硬件工作队列时，可能会产生虚假<br> 的依赖关系。【所以只有多个stream共享相同的硬件工作队列的时候才会产生虚假的依赖关系】<br> 【所以一个设备连接一个硬件工作队列？好像是】</p></li><li><p>在支持Hyper-Q技术但是没有足够硬件连接的平台上，要检查CUDA流的行为，需要<br> 将simpleHyperqDepth示例修改为使用8个CUDA流：<code>#define NSTREAM 8</code></p></li><li><p>并将CUDA设备连接的数量设置为4：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// set up max connectioin</span>
<span class="token keyword">char</span><span class="token operator">*</span> iname <span class="token operator">=</span> <span class="token string">&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;</span><span class="token punctuation">;</span> 
<span class="token function">setenv</span> <span class="token punctuation">(</span>iname<span class="token punctuation">,</span> <span class="token string">&quot;4&quot;</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>Kepler GPU上使用nvvp运行修改的程序：<code>$ nvvp ./simpleHyperqDepth</code></p></li><li><p>图6-10展示了8个流，但是只有4路并发。因为现在只有4个设备连接，两个流共享一<br> 个队列。采用深度优先的方法调度内核，导致了分配在同一工作队列中的两个流之间出现<br> 了虚假的依赖关系，这与在Fermi GPU上使用深度优先顺序时的结果类似。<br><img src="`+U+'" alt="figure6-10" loading="lazy"></p></li><li><p>下一步，使用相同的设置，检查使用广度优先方法的行为。如图6-11所示，现在8个<br> 流都是同步运行。用广度优先顺序调度内核去除了虚假的依赖关系。<br><img src="'+D+'" alt="figure6-11" loading="lazy"></p></li></ul><h3 id="_6-2-5-gpu资源的并发限制" tabindex="-1"><a class="header-anchor" href="#_6-2-5-gpu资源的并发限制" aria-hidden="true">#</a> 6.2.5 GPU资源的并发限制</h3><ul><li><p>有限的内核资源可以抑制应用程序中可能出现的内核并发的数量。在之前的例子中，<br> 启动内核时只有一个线程，以避免并发时任何的硬件限制。因此，每个内核执行只需要少<br> 量的设备计算资源。<br><code>kernel_1&lt;&lt;&lt;1, 1, 0, streams[i]&gt;&gt;&gt;();</code></p></li><li><p>在实际应用中，内核启动时通常会创建多个线程。通常，会创建数百或数千个线程。<br> 有了这么多线程，可用的硬件资源可能会成为并发的主要限制因素，因为它们阻止启动符<br> 合条件的内核。为了在活动中观察到这个行为，可以在simpleHyperqBreadth例子中改变执<br> 行配置，在每个块中使用多个线程，在每个网格中使用更多的块：<br><code>dim3 block(128);</code><br><code>dim3 grid (32);</code></p></li><li><p>也应该将使用的CUDA流的数量增加到16：<code>#define NSTREAM 16</code></p></li><li><p>重新编译后，在Kepler设备上使用nvvp查看simpleHyperqBreadth的行为。<code>$ nvvp ./simpleHyperqBreadth</code></p></li><li><p>如图6-12所示，图中只实现了8路并发，即使CUDA设备连接的数量被设置为32。因<br> 为GPU无法分配足够的资源来执行所有符合条件的内核，所以并发性是有限的<br> 【所以目前来看做项目的话基本不会那么多的并发，可能尝试较少的stream】<br><img src="'+E+`" alt="figure6-12" loading="lazy"></p></li></ul><h3 id="_6-2-6-默认流的阻塞行为" tabindex="-1"><a class="header-anchor" href="#_6-2-6-默认流的阻塞行为" aria-hidden="true">#</a> 6.2.6 默认流的阻塞行为</h3><ul><li>为了说明默认流在非空流中是如何阻塞操作的，在simpleHyperqDepth.cu中，将深度<br> 优先调度循环改为在默认流中调用kernel_3。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code> <span class="token comment">// dispatch job with depth first ordering</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_4<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>因为第三个内核在默认流中被启动，所以在非空流上所有之后的操作都会被阻塞，直<br> 到默认流中的操作完成。图6-13显示了这段代码运行的时间轴，它是在Tesla K40上使用<br> nvvp得到的。这个时间轴显示了每个kernel_3启动是如何阻止所有其他阻塞流中进一步执<br> 行的。<br><img src="`+w+`" alt="figure6-13" loading="lazy"></li></ul><h3 id="_6-2-7-创建流间依赖关系" tabindex="-1"><a class="header-anchor" href="#_6-2-7-创建流间依赖关系" aria-hidden="true">#</a> 6.2.7 创建流间依赖关系</h3><ul><li><p>在理想情况下，流之间不应该有非计划之内的依赖关系（即虚假的依赖关系）。然<br> 而，在复杂的应用程序中，引入流间依赖关系是很有用的，它可以在一个流中阻塞操作直<br> 到另一个流中的操作完成。事件可以用来添加流间依赖关系。</p></li><li><p>假如我们想让一个流中的工作在其他所有流中的工作都完成后才开始执行，那么就可<br> 以使用事件来创建流之间的依赖关系。首先，将标志设置为cudaEventDisableTiming，创建<br> 同步事件，代码如下：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>cudaEvent_t <span class="token operator">*</span>kernelEvent <span class="token operator">=</span> <span class="token punctuation">(</span>cudaEvent_t <span class="token operator">*</span><span class="token punctuation">)</span><span class="token function">malloc</span><span class="token punctuation">(</span>n_streams <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>cudaEvent_t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token function">cudaEventCreateWithFlags</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>kernelEvent<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> cudaEventDisableTiming<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>接下来，使用cudaEventRecord函数，在每个流完成时记录不同的事件。然后，使用<br> cudaStreamWaitEvent使最后一个流（<code>即streams[n_streams-1]</code>）等待其他所有流：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// dispatch job with depth first way</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">&lt;</span>n_streams<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    kernel_1<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_2<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_3<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    kernel_4<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaEventRecord</span><span class="token punctuation">(</span>kernelEvent<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> streams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaStreamWaitEvent</span><span class="token punctuation">(</span>streams<span class="token punctuation">[</span>n_streams<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> kernelEvent<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>从Wrox.com上可以下载simpleHyperqDependence.cu文件的完整代码。图6-14表示了<br> nvvp内核时间轴。要注意，第四个流，即<code>streams[n_streams-1]</code>，在其他所有流完成后才能<br> 开始启动工作。<br><img src="`+S+`" alt="figure6-14" loading="lazy"></li></ul><h2 id="_6-3-重叠内核执行和数据传输" tabindex="-1"><a class="header-anchor" href="#_6-3-重叠内核执行和数据传输" aria-hidden="true">#</a> 6.3 重叠内核执行和数据传输</h2><ul><li><p>在前一节中，已经介绍了如何在多个流中并发执行多个内核。在本节中，将学习如何<br> 并发执行内核和数据传输。重叠内核和数据传输表现出不同的行为，并且需要考虑一些与<br> 并发内核执行相比不同的因素。</p></li><li><p>Fermi GPU和Kepler GPU有两个复制引擎队列：一个用于将数据传输到设备，另一个<br> 用于从设备中将数据提取出来。因此，最多可以重叠两个数据传输，<mark>并且只有当它们的方<br> 向不同并且被调度到不同的流时才能这样做</mark>，否则，所有的数据传输都将是串行的。在决<br> 定如何使用内核计算最佳地重叠数据传输时，记住这一点是很重要的。</p></li><li><p>在应用程序中，还需要检验数据传输和内核执行之间的关系，<br> 从而可以区分以下两种情况：</p><ul><li>如果一个内核使用数据A，那么对A进行数据传输必须要安排在内核启动前，且必须位于相同的流中。</li><li>如果一个内核完全不使用数据A，那么内核执行和数据传输可以位于不同的流中。</li></ul></li><li><p>在第二种情况下，实现内核和数据传输的并发执行是很容易的：将它们放置在不同的<br> 流中，这就已经向运行时表示了并发地执行它们是安全的。然而，在第一种情况下，要实<br> 现数据传输和内核执行之间的重叠会更复杂，因为内核依赖数据作为输入。当内核和传输<br> 之间存在依赖关系时，可以使用向量加法示例来检验如何实现重叠数据传输和内核执行。</p></li></ul><h3 id="_6-3-1-使用深度优先调度重叠" tabindex="-1"><a class="header-anchor" href="#_6-3-1-使用深度优先调度重叠" aria-hidden="true">#</a> 6.3.1 使用深度优先调度重叠</h3><ul><li>我们已经非常熟悉向量加法内核了：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">sumArrays</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> N<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>idx <span class="token operator">&lt;</span> N<span class="token punctuation">)</span> 
    <span class="token comment">//本节中唯一增加的变动就是这个内核计算被n_repeat增强了，以延长内核的执行时间，</span>
    <span class="token comment">//从而使它在nvvp中的计算和通信重叠更容易被可视化。</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n_repeat<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        C<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><br><ul><li><p>实现向量加法的CUDA程序，其基本结构包含3个主要步骤：<br> ·将两个输入向量从主机复制到设备中<br> ·执行向量加法运算<br> ·将单一的输出向量从设备返回主机中</p></li><li><p>从这些步骤中也许不能明显看出计算和通信是如何被重叠的。为了在向量加法中实现<br> 重叠，需要将输入和输出数据集划分成子集，并将来自一个子集的通信与来自于其他子集<br> 的计算进行重叠。具体对向量加法来说，需要将两个长度为N的向量加法问题划分为长度<br> 为N/M的向量相加的M个子问题。因为这里的每个子问题都是独立的，所以每一个都可以<br> 被安排在不同的CUDA流中，这样它们的计算和通信就可以重叠了。</p></li><li><p>在第2章的向量加法程序中，数据传输是通过同步复制函数实现的。要重叠数据传输<br> 和内核执行，必须使用异步复制函数。因为异步复制函数需要固定的主机内存，所以首先<br> 需要使用cudaHostAlloc函数，在固定主机内存中修改主机数组的分配：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>gpuRef<span class="token punctuation">,</span> nBytes<span class="token punctuation">,</span> cudaHostAllocDefault<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>hostRef<span class="token punctuation">,</span> nBytes<span class="token punctuation">,</span> cudaHostAllocDefault<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,71),K={class:"hint-container details"},R=n("summary",null,"Click me to view the code!",-1),Q=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token comment"},"// 接下来，需要在NSTREAM流中平均分配该问题的任务。"),s(`
`),n("span",{class:"token comment"},"// 每一个流要处理的元素数量使用以下代码定义："),s(`
`),n("span",{class:"token keyword"},"int"),s(" iElem "),n("span",{class:"token operator"},"="),s(" nElem "),n("span",{class:"token operator"},"/"),s(" NSTREAM"),n("span",{class:"token punctuation"},";"),s(`

`),n("span",{class:"token comment"},"//现在，可以使用一个循环来为几个流同时调度iElem个元素的通信和计算"),s(`
`),n("span",{class:"token keyword"},"for"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" i "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(" i "),n("span",{class:"token operator"},"<"),s(" NSTREAM"),n("span",{class:"token punctuation"},";"),s(),n("span",{class:"token operator"},"++"),s("i"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
    `),n("span",{class:"token keyword"},"int"),s(" ioffset "),n("span",{class:"token operator"},"="),s(" i "),n("span",{class:"token operator"},"*"),s(" iElem"),n("span",{class:"token punctuation"},";"),s(`
    `),n("span",{class:"token function"},"cudaMemcpyAsync"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("h_A"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(" iBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},","),s(" stream"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    `),n("span",{class:"token function"},"cudaMemcpyAsync"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("d_B"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("h_B"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(" iBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},","),s(" stream"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    sumArrays`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s("stream"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("d_B"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s("iElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    `),n("span",{class:"token function"},"cudaMemcpyAsync"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("gpuRef"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},"["),s("ioffset"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(" iBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},","),s(" stream"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(` 
`),n("span",{class:"token punctuation"},"}"),s(`

`),n("span",{class:"token comment"},`/*
由于这些内存复制和内核启动对主机而言是异步的，因此全部的工作负载都可以毫无
阻塞地在流之间进行分配。通过将数据传输和该数据上的计算放置在同一个流中，输入向
量、内核计算以及输出向量之间的依赖关系可以被保持。
*/`),s(`


`),n("span",{class:"token comment"},"// 为了进行对比，此例还使用了一个阻塞实现来计算基准性能："),s(`
sumArrays`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_B"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),W=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s("从Wrox"),n("span",{class:"token punctuation"},"."),s("com的simpleMultiAddDepth"),n("span",{class:"token punctuation"},"."),s(`cu文件中可以下载这个示例的完整代码。编译
后，用nvvp显示副本和内核的时间轴：
$ nvvp `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`simpleMultiAddDepth

`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),V=c('<figure><img src="'+P+'" alt="figure6-15.png" tabindex="0" loading="lazy"><figcaption>figure6-15.png</figcaption></figure><ul><li><p>图6-15显示了Tesla K40设备典型的时间轴。图中使用了8个硬件工作队列和4个CUDA<br> 流来重叠内核执行和数据传输。相对于阻塞的默认流执行，该流执行实现了近40%的性能<br> 提升。图6-15显示了以下3种重叠：<br> ·不同流中内核的互相重叠<br> ·内核与其他流中的数据传输重叠<br> ·在不同流以及不同方向上的数据传输互相重叠</p></li><li><p>图6-15还呈现了以下两种阻塞行为：<br> ·内核被同一流中先前的数据传输所阻塞<br> ·从主机到设备的数据传输被同一方向上先前的数据传输所阻塞</p></li><li><p>虽然从主机到设备的数据传输是在4个不同的流中执行的，但时间轴显示它们是按顺<br> 序执行的，因为实际上它们是通过相同的复制引擎队列来执行的。</p></li></ul><figure><img src="'+x+`" alt="figure6-16.png" tabindex="0" loading="lazy"><figcaption>figure6-16.png</figcaption></figure><ul><li><p>接下来，可以尝试将硬件工作队列的数量减少至一个，然后重新运行，测试一下其性<br> 能。图6-16显示了在Tesla K40设备上产生的时间轴。要注意，图6-16（1个工作队列）和<br> 图6-15（8个工作队列）之间没有显著差异。因为每个流只执行单一的一个内核，所以减<br> 少工作队列的数目并没有增加虚假依赖关系，同样，现存的虚假依赖关系（由主机到设备<br> 的复制队列引起的）也没有减少。</p></li><li><p>减少K40中工作队列的数目，可以创造一个类似于Fermi GPU的环境：一个工作队列<br> 和两个复制队列。如果在Fermi GPU上运行相同的测试，会发现虚假的依赖关系是确实存<br> 在的。这是由Kepler的工作调度机制导致的，在网格管理单元（Grid Management Unit，<br> GMU）中实现。GMU负责对发送到GPU中的工作进行管理和排序。通过对GMU的分析有<br> 助于减少虚假的依赖关系。</p></li></ul><h4 id="网格管理单元" tabindex="-1"><a class="header-anchor" href="#网格管理单元" aria-hidden="true">#</a> 网格管理单元</h4><ul><li><p>Kepler引入了一个新的网格管理和调度控制系统，即网格管理单元（GMU）。</p></li><li><p>GMU可以暂停新网格的调度，使网格排队等待且暂停网格直到它们准备好执行，这<br> 样就使运行时变得非常灵活强大，动态并行就是一个很好的例子。</p></li><li><p>在Fermi设备上，网格直接从流队列被传到CUDA工作分配器（CUDA Work<br> Distributor，CWD）中。在Kepler设备上，网格被发送到GMU上，GMU对在GPU上执行的<br> 网格进行管理和优先级排序。</p></li><li><p>GMU创建了多个硬件工作队列，从而减少或消除了虚假的依赖关系。通过GMU，流<br> 可以作为单独的工作流水线。即使GMU被限制只能创建一个单一的硬件工作队列，根据<br> 以上测试结果证实，通过GMU进行的网格依赖性分析也可以帮助消除虚假的依赖关系。</p></li></ul><h3 id="_6-3-2-使用广度优先调度重叠" tabindex="-1"><a class="header-anchor" href="#_6-3-2-使用广度优先调度重叠" aria-hidden="true">#</a> 6.3.2 使用广度优先调度重叠</h3><ul><li><p>先前的例子表明，当采用广度优先的方式调度内核时，Fermi GPU可以实现最好的效<br> 果。现在，将在重叠数据传输和计算内核中，检验广度优先排序产生的效果。</p></li><li><p>下面的代码演示了使用广度优先的方法来调度流间的计算和通信：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// initiate all asynchronous transfers to the device</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> NSTREAM<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">int</span> ioffset <span class="token operator">=</span> i <span class="token operator">*</span> iElem<span class="token punctuation">;</span>
    <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_A<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>h_A<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span>cudaMemcpyHostToDevice<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_B<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>h_B<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span>cudaMemcpyHostToDevice<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// launch a kernel in each stream</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> NSTREAM<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">int</span> ioffset <span class="token operator">=</span> i <span class="token operator">*</span> iElem<span class="token punctuation">;</span>
    sumArrays<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>d_A<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>d_B<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>d_C<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span>iElem<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token comment">// queue asynchronous transfers from the device</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> NSTREAM<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">int</span> ioffset <span class="token operator">=</span> i <span class="token operator">*</span> iElem<span class="token punctuation">;</span>
    <span class="token function">cudaMemcpyAsync</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>gpuRef<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token operator">&amp;</span>d_C<span class="token punctuation">[</span>ioffset<span class="token punctuation">]</span><span class="token punctuation">,</span> iBytes<span class="token punctuation">,</span>cudaMemcpyDeviceToHost<span class="token punctuation">,</span> stream<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
 <span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>从Wrox.com的simpleMultiAddBreadth.cu文件中可以下载完整的示例代码。图6-17显示<br> 了在K40设备上只使用一个硬件工作队列时的时间轴。与深度优先的方法相比它没有明显<br> 的差异，因为Kepler的双向调度机制有助于消除虚假的依赖关系。但如果在Fermi设备上<br> 运行相同的测试，在整体性能方面会发现，使用广度优先方法不如使用深度优先方法。由<br> 主机到设备复制队列上的争用导致的虚假依赖关系，在主机到设备间的传输完成前，将阻<br> 止所有的内核启动。<br><img src="`+M+'" alt="figure6-17" loading="lazy"></p></li><li><p>因此，对于Kepler设备而言，在大多数情况下无须关注其工作调度顺序。而在Fermi<br> 设备上，要注意这些问题，并且对不同的调度方案做出评估，使工作负载找到最佳的任务<br> 调度顺序。</p></li></ul><h2 id="_6-4-重叠gpu和cpu执行" tabindex="-1"><a class="header-anchor" href="#_6-4-重叠gpu和cpu执行" aria-hidden="true">#</a> 6.4 重叠GPU和CPU执行</h2><ul><li><p>相对而言，实现GPU和CPU执行重叠是比较简单的，因为所有的内核启动在默认情况<br> 下都是异步的。因此，只需简单地启动内核，并且立即在主机线程上实现有效操作，就能<br> 自动重叠GPU和CPU执行。</p></li><li><p>本节的示例主要包括两个部分：<br> ·内核被调度到默认流中<br> ·等待GPU内核时执行主机计算</p></li><li><p>使用以下简单的内核实现一个向量与标量的加法：</p></li></ul>',12),$={class:"hint-container details"},X=n("summary",null,"Click me to view the code!",-1),L=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s("__global__ "),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token function"},"kernel"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),s(" g_data"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"float"),s(" value"),n("span",{class:"token punctuation"},")"),s(`
`),n("span",{class:"token punctuation"},"{"),s(`
    `),n("span",{class:"token keyword"},"int"),s(" idx "),n("span",{class:"token operator"},"="),s(" blockDim"),n("span",{class:"token punctuation"},"."),s("x "),n("span",{class:"token operator"},"*"),s(" blockIdx"),n("span",{class:"token punctuation"},"."),s("x "),n("span",{class:"token operator"},"+"),s(" threadIdx"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},";"),s(`
    g_data`),n("span",{class:"token punctuation"},"["),s("idx"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"+="),s(" value"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`

`),n("span",{class:"token comment"},"// 本例中使用了3个CUDA操作（两个复制和一个内核启动）。"),s(`
`),n("span",{class:"token comment"},"//记录一个停止事件，以标记所有CUDA操作的完成。"),s(`
`),n("span",{class:"token function"},"cudaMemcpyAsync"),n("span",{class:"token punctuation"},"("),s("d_a"),n("span",{class:"token punctuation"},","),s(" h_a"),n("span",{class:"token punctuation"},","),s(" nbytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
kernel`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),s("d_a"),n("span",{class:"token punctuation"},","),s(" value"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaMemcpyAsync"),n("span",{class:"token punctuation"},"("),s("h_a"),n("span",{class:"token punctuation"},","),s(" d_a"),n("span",{class:"token punctuation"},","),s(" nbytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaEventRecord"),n("span",{class:"token punctuation"},"("),s("stop"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`

`),n("span",{class:"token comment"},`/*
所有这些操作与主机都是异步的，它们都被绑定到默认流中。
最后的cudaMemcpyAsync函数一旦被发布，控制权将立即返回到主机。
一旦控制权返回给主机，主机就可以做任何有用的计算，而不必再依赖内核输出。
在下面的代码段中，主机只是简单迭代，等待所有的CUDA操作完成时计数器加一。
在每次迭代中，主机线程查询停止事件。一旦事件完成，主机线程继续。
*/`),s(`
`),n("span",{class:"token keyword"},"unsigned"),s(),n("span",{class:"token keyword"},"long"),s(),n("span",{class:"token keyword"},"int"),s(" counter "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token keyword"},"while"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token function"},"cudaEventQuery"),n("span",{class:"token punctuation"},"("),s("stop"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"=="),s(" cudaErrorNotReady"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 counter`),n("span",{class:"token operator"},"++"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`

`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),j=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token comment"},"//从Wrox.com中可以下载asyncAPI.cu代码示例。以下是在Tesla K40上使用nvprof的输出。"),s(`
`),n("span",{class:"token comment"},"// 在等待GPU操作完成时，主机线程执行14606次迭代。"),s(`
$ nvprof `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`asyncAPI
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"22813"),n("span",{class:"token operator"},"=="),s(" NVPROF is profiling process "),n("span",{class:"token number"},"22813"),n("span",{class:"token punctuation"},","),s(" command"),n("span",{class:"token operator"},":"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`asyncAPI
`),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("asyncAPI running on CUDA device "),n("span",{class:"token punctuation"},"["),s("Tesla K40c"),n("span",{class:"token punctuation"},"]"),s(`
CPU executed `),n("span",{class:"token number"},"14606"),s(" iterations "),n("span",{class:"token keyword"},"while"),s(" waiting "),n("span",{class:"token keyword"},"for"),s(` GPU to finish
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"22813"),n("span",{class:"token operator"},"=="),s(" Profiling application"),n("span",{class:"token operator"},":"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`asyncAPI
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"22813"),n("span",{class:"token operator"},"=="),s(" Profiling result"),n("span",{class:"token operator"},":"),s(`
`),n("span",{class:"token function"},"Time"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"%"),n("span",{class:"token punctuation"},")"),s(` Time Calls Avg Min Max Name
 `),n("span",{class:"token number"},"48.89"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"10.661"),s("ms "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"10.661"),s("ms "),n("span",{class:"token number"},"10.661"),s("ms "),n("span",{class:"token number"},"10.661"),s("ms "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy HtoD"),n("span",{class:"token punctuation"},"]"),s(`
 `),n("span",{class:"token number"},"46.04"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"10.041"),s("ms "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"10.041"),s("ms "),n("span",{class:"token number"},"10.041"),s("ms "),n("span",{class:"token number"},"10.041"),s("ms "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy DtoH"),n("span",{class:"token punctuation"},"]"),s(`
 `),n("span",{class:"token number"},"3.25"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"709.82u"),s("s "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"709.82u"),s("s "),n("span",{class:"token number"},"709.82u"),s("s "),n("span",{class:"token number"},"709.82u"),s("s "),n("span",{class:"token function"},"kernel"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),s(`
 `),n("span",{class:"token number"},"1.82"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"396.13u"),s("s "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"396.13u"),s("s "),n("span",{class:"token number"},"396.13u"),s("s "),n("span",{class:"token number"},"396.13u"),s("s "),n("span",{class:"token punctuation"},"["),s("CUDA memset"),n("span",{class:"token punctuation"},"]"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),J=c('<ul><li>这段代码演示了如何在CUDA编程中使用异步操作和事件来管理主机和设备之间的数据传输以及内核执行。将重新解释并简化代码的逻辑： <ol><li><strong>CUDA内核函数</strong> <code>kernel</code> 是一个简单的函数，它将一个值加到设备内存中的数组 <code>g_data</code> 的每个元素上。</li><li><strong>异步数据传输</strong>：使用 <code>cudaMemcpyAsync</code> 进行主机到设备（<code>cudaHostToDevice</code>）和设备到主机（<code>cudaDeviceToHost</code>）的内存复制。这些复制操作是异步的，意味着它们会立即返回控制权给CPU，而不需要等待数据传输完成。</li><li><strong>内核执行</strong>：通过调用 <code>kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_a, value);</code> 来在GPU上执行 <code>kernel</code> 函数。这里的 <code>grid</code> 和 <code>block</code> 定义了执行的线程块的布局。</li><li><strong>事件记录</strong>：<code>cudaEventRecord(stop);</code> 创建并记录一个事件，这个事件标志着所有先前排队的CUDA操作（在这个例子中是内核执行和内存复制）的完成。</li><li><strong>等待事件完成</strong>：在主机代码中，使用一个循环 <code>while (cudaEventQuery(stop) == cudaErrorNotReady) { counter++; }</code> 来不断检查事件 <code>stop</code> 是否完成。如果事件还没有完成，<code>cudaEventQuery</code> 将返回 <code>cudaErrorNotReady</code>，并且循环会继续。一旦事件完成，循环结束。</li><li><strong>计数器</strong>：在等待事件完成的过程中，计数器 <code>counter</code> 被用来模拟主机在等待期间可以做的其他工作。每次循环迭代，计数器增加。</li></ol></li><li>简单来说，这段代码的目的是展示如何异步地在GPU上执行计算（通过内核）和数据传输，同时主机CPU可以继续执行其他任务而不是等待GPU完成。使用事件来同步确保当所有GPU操作完成后，主机代码才继续执行。</li><li>这种模式对于提高GPU和CPU的并行效率非常重要，因为它允许它们同时工作而不是相互等待。在实际应用中，这种技术可以用来最大化计算资源的利用率。</li></ul><h2 id="_6-5-流回调" tabindex="-1"><a class="header-anchor" href="#_6-5-流回调" aria-hidden="true">#</a> 6.5 流回调</h2><ul><li><p>流回调是另一种可以到CUDA流中排列等待的操作。一旦流回调之前所有的流操作全<br> 部完成，被流回调指定的主机端函数就会被CUDA运行时所调用。此函数由应用程序提<br> 供，并允许任意主机端逻辑插入到CUDA流中。流回调是另一种CPU和GPU同步机制。回<br> 调功能十分强大，因为它们是第一个GPU操作的例子，此操作用于在主机系统上创建工<br> 作，这与在本书中阐述这一点的CUDA概念完全相反。</p></li><li><p>流回调函数是由应用程序提供的一个主机函数，并在流中使用以下的API函数注册：<br><code>cudaError_t cudaStreamAddCallback(cudaStream_t stream,</code><br><code>cudaStreamCallback_t callback, void *userData, unsigned int flags);</code></p></li><li><p>此函数为提供的流添加了一个回调函数。<mark>在流中所有先前排队的操作完成后，回调函<br> 数才能在主机上执行</mark>。每使用cudaStreamAddCallback一次，只执行一次回调，并阻塞队列<br> 中排在其后面的工作，直到回调函数完成。当它被CUDA运行时调用时，回调函数会通过<br> 调用它的流，并且会有错误代码来表明是否有CUDA错误的发生。还可以使用<br> cudaStreamAddCallback的userData参数，指定传递给回调函数的应用程序数据。flags参数<br> 在后面将会使用，目前没有任何意义；因此，必须将它设置为零。在所有流中先前的全部<br> 工作都完成后，排在空流中的回调队列才被执行。</p></li><li><p>对于回调函数有两个限制：<br> ·从回调函数中不可以调用CUDA的API函数<br> ·在回调函数中不可以执行同步</p></li><li><p>一般来说，对互相关联或与其他CUDA操作相关的回调顺序做任何假设都是有风险<br> 的，可能都会导致代码不稳定。</p></li><li><p>下面的代码示例在4个流都执行4个内核后，为每个流的末尾添加回调函数my_callback。<br> 只有当每个流中的所有工作都完成后，回调函数才开始在主机上运行。</p></li></ul>',3),Y={class:"hint-container details"},Z=n("summary",null,"Click me to view the code!",-1),nn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token keyword"},"void"),s(" CUDART_CB "),n("span",{class:"token function"},"my_callback"),n("span",{class:"token punctuation"},"("),s("cudaStream_t stream"),n("span",{class:"token punctuation"},","),s(" cudaError_t status"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),s("data"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(` 
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"callback from stream %d\\n"'),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),s("data"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`


`),n("span",{class:"token comment"},"//为每个流添加流回调的代码如下："),s(`
 `),n("span",{class:"token keyword"},"for"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" i "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(" i "),n("span",{class:"token operator"},"<"),s(" n_streams"),n("span",{class:"token punctuation"},";"),s(" i"),n("span",{class:"token operator"},"++"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
    stream_ids`),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"="),s(" i"),n("span",{class:"token punctuation"},";"),s(`
    kernel_1`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" streams"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(` 
    kernel_2`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" streams"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    kernel_3`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" streams"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    kernel_4`),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" streams"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
    `),n("span",{class:"token function"},"cudaStreamAddCallback"),n("span",{class:"token punctuation"},"("),s("streams"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},","),s(" my_callback"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},"("),s("stream_ids "),n("span",{class:"token operator"},"+"),s(" i"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),sn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token comment"},"//从Wrox.com可以下载simpleCallback.cu文件。下面是在Tesla K40 GPU上得到的示例输出："),s(`
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`callback 
`),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("callback Starting"),n("span",{class:"token punctuation"},"."),n("span",{class:"token punctuation"},"."),n("span",{class:"token punctuation"},"."),s(`
`),n("span",{class:"token operator"},">"),s(" Using Device "),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(` Tesla K40c
`),n("span",{class:"token operator"},">"),s(" Compute Capability "),n("span",{class:"token number"},"3.5"),s(" hardware with "),n("span",{class:"token number"},"15"),s(" multi"),n("span",{class:"token operator"},"-"),s(`processors
`),n("span",{class:"token operator"},">"),s(" CUDA_DEVICE_MAX_CONNECTIONS "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"8"),s(`
`),n("span",{class:"token operator"},">"),s(" with streams "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"4"),s(`
callback from stream `),n("span",{class:"token number"},"0"),s(`
callback from stream `),n("span",{class:"token number"},"1"),s(`
callback from stream `),n("span",{class:"token number"},"2"),s(`
callback from stream `),n("span",{class:"token number"},"3"),s(`
Measured time `),n("span",{class:"token keyword"},"for"),s(" parallel execution "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0.104"),s(`s
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),an=c('<h2 id="_6-6-总结" tabindex="-1"><a class="header-anchor" href="#_6-6-总结" aria-hidden="true">#</a> 6.6 总结</h2><ul><li><p>流的概念是CUDA编程模型的一个基本组成部分。允许高级CUDA操作在独立的流中<br> 排队执行，CUDA流可以实现粗粒度并发。因为CUDA支持异步操作和大多数版本的运行<br> 时函数，所以它可以在多个CUDA流之间调度计算和通信。</p></li><li><p>从概念上讲，如果CUDA操作之间存在依赖关系，则它们必须在同一个流中被调度。<br> 例如，为了确保应用程序的准确无误，内核必须在同一个流中被调度，并在它使用的任何<br> 数据传输后进行。另外，没有依赖关系的操作可以在任意的流中被调度。在CUDA中，通<br> 常可以使用3种不同类型的重叠方案来隐藏计算或通信延迟：</p><ul><li>在设备上重叠多个并发的内核</li><li>重叠带有传入或传出设备数据传输的CUDA内核</li><li>重叠CPU执行和GPU执行</li></ul></li><li><p>为了充分利用设备，并确保最大的并发性，还需要注意以下问题：</p><ul><li>平衡内核资源需求和并发资源需求。在设备上一次启动过多的计算任务，可能会导<br> 致内核串行，这会使得硬件资源的工作块变得可用。但是，也需要确保设备没有被充分利<br> 用，一直有工作在排队等待执行。</li><li>如果可能的话，避免使用默认流执行异步操作。<mark>放置在默认流中的操作可能会阻塞<br> 其他非默认CUDA流的进展。</mark></li><li>在Fermi设备上，从深度优先和广度优先两方面考虑主机的调度。这个选择可以通过<br> 消除共享硬件工作队列上的虚假依赖关系，显著影响其性能。</li><li>要注意隐式同步的函数，并且充分利用它们和异步函数来避免性能的降低。</li></ul></li><li><p>此外，本章还介绍了CUDA可视化性能分析器（nvvp）在可视化GPU执行中的作用。<br> nvvp允许确认操作重叠的条件，并且易于多个流行为的可视化。</p></li></ul><h2 id="_6-7-习题" tabindex="-1"><a class="header-anchor" href="#_6-7-习题" aria-hidden="true">#</a> 6.7 习题</h2>',3),tn=c("<li><p>1.描述“CUDA流”的定义。哪些操作可以置于CUDA流中？在应用程序中使用流的主<br> 要优点是什么？</p></li><li><p>2.事件是如何与流相关联的？举一个例子，在该例中CUDA事件是有用的，并且其能<br> 实现单独用流无法有效实现的逻辑。</p></li><li><p>3.什么因素能造成GPU上虚假的依赖关系？在Fermi和Kepler架构上产生这些虚假依赖<br> 关系的原因有什么区别？Hyper-Q是如何限制虚假的依赖关系的？</p></li><li><p>4.描述显式同步和隐式同步的差异。可以创建隐式主机和设备同步点的具体CUDA<br> API函数的例子有哪些？</p></li><li><p>5.在CUDA流中执行任务时，使用深度优先和广度优先的方法有哪些不同？特别是<br> Fermi结构如何通过广度优先序列的方法获得益处？</p></li><li><p>6.列出不同类型的CUDA重叠。描述要实现每种CUDA重叠所需的技术。</p></li><li><p>7.使用nvvp代码画出在Fermi设备上运行simpleHyperqBreadth预期的时间轴：<br> 假设使用了32个流，解释你所画的时间轴。<br><code>$ nvvp ./simplehHyperqBreadth</code></p></li><li><p>8.画出在Kepler设备上，使用如下命令产生的时间轴：<br> 假设使用了32个流，解释你所画的时间轴。<br><code>$ nvvp ./simpleHyperDepth</code></p></li>",8),en={href:"http://9.xn--simpleCallback-u84x8292c.cu",target:"_blank",rel:"noopener noreferrer"},pn=n("br",null,null,-1);function on(cn,ln){const e=i("router-link"),l=i("CodeTabs"),u=i("ExternalLinkIcon");return k(),d("div",null,[H,T,m(" more "),n("nav",I,[n("ul",null,[n("li",null,[t(e,{to:"#简单介绍主要是基础"},{default:a(()=>[s("简单介绍主要是基础")]),_:1})]),n("li",null,[t(e,{to:"#第6章-流和并发"},{default:a(()=>[s("第6章 流和并发")]),_:1})]),n("li",null,[t(e,{to:"#_6-1-流和事件概述"},{default:a(()=>[s("6.1 流和事件概述")]),_:1}),n("ul",null,[n("li",null,[t(e,{to:"#_6-1-1-cuda流"},{default:a(()=>[s("6.1.1 CUDA流")]),_:1})]),n("li",null,[t(e,{to:"#_6-1-2-流调度"},{default:a(()=>[s("6.1.2 流调度")]),_:1})]),n("li",null,[t(e,{to:"#_6-1-3-流的优先级"},{default:a(()=>[s("6.1.3 流的优先级")]),_:1})]),n("li",null,[t(e,{to:"#_6-1-4-cuda事件"},{default:a(()=>[s("6.1.4 CUDA事件")]),_:1})]),n("li",null,[t(e,{to:"#_6-1-5-流同步"},{default:a(()=>[s("6.1.5 流同步")]),_:1})])])]),n("li",null,[t(e,{to:"#_6-2-并发内核执行"},{default:a(()=>[s("6.2 并发内核执行")]),_:1}),n("ul",null,[n("li",null,[t(e,{to:"#_6-2-1-非空流中的并发内核"},{default:a(()=>[s("6.2.1 非空流中的并发内核")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-2-fermi-gpu上的虚假依赖关系"},{default:a(()=>[s("6.2.2 Fermi GPU上的虚假依赖关系")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-3-使用openmp的调度操作"},{default:a(()=>[s("6.2.3 使用OpenMP的调度操作")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-4-用环境变量调整流行为"},{default:a(()=>[s("6.2.4 用环境变量调整流行为")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-5-gpu资源的并发限制"},{default:a(()=>[s("6.2.5 GPU资源的并发限制")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-6-默认流的阻塞行为"},{default:a(()=>[s("6.2.6 默认流的阻塞行为")]),_:1})]),n("li",null,[t(e,{to:"#_6-2-7-创建流间依赖关系"},{default:a(()=>[s("6.2.7 创建流间依赖关系")]),_:1})])])]),n("li",null,[t(e,{to:"#_6-3-重叠内核执行和数据传输"},{default:a(()=>[s("6.3 重叠内核执行和数据传输")]),_:1}),n("ul",null,[n("li",null,[t(e,{to:"#_6-3-1-使用深度优先调度重叠"},{default:a(()=>[s("6.3.1 使用深度优先调度重叠")]),_:1})]),n("li",null,[t(e,{to:"#_6-3-2-使用广度优先调度重叠"},{default:a(()=>[s("6.3.2 使用广度优先调度重叠")]),_:1})])])]),n("li",null,[t(e,{to:"#_6-4-重叠gpu和cpu执行"},{default:a(()=>[s("6.4 重叠GPU和CPU执行")]),_:1})]),n("li",null,[t(e,{to:"#_6-5-流回调"},{default:a(()=>[s("6.5 流回调")]),_:1})]),n("li",null,[t(e,{to:"#_6-6-总结"},{default:a(()=>[s("6.6 总结")]),_:1})]),n("li",null,[t(e,{to:"#_6-7-习题"},{default:a(()=>[s("6.7 习题")]),_:1})])])]),N,n("details",z,[B,t(l,{id:"423",data:[{id:"demo1"},{id:"demo2"}],"tab-id":"shell"},{title0:a(({value:p,isActive:o})=>[s("demo1")]),title1:a(({value:p,isActive:o})=>[s("demo2")]),tab0:a(({value:p,isActive:o})=>[O]),tab1:a(({value:p,isActive:o})=>[q]),_:1})]),F,n("details",K,[R,t(l,{id:"1012",data:[{id:"demo1"},{id:"demo2"}],"tab-id":"shell"},{title0:a(({value:p,isActive:o})=>[s("demo1")]),title1:a(({value:p,isActive:o})=>[s("demo2")]),tab0:a(({value:p,isActive:o})=>[Q]),tab1:a(({value:p,isActive:o})=>[W]),_:1})]),V,n("details",$,[X,t(l,{id:"1130",data:[{id:"demo1"},{id:"demo2"}],"tab-id":"shell"},{title0:a(({value:p,isActive:o})=>[s("demo1")]),title1:a(({value:p,isActive:o})=>[s("demo2")]),tab0:a(({value:p,isActive:o})=>[L]),tab1:a(({value:p,isActive:o})=>[j]),_:1})]),J,n("details",Y,[Z,t(l,{id:"1224",data:[{id:"demo1"},{id:"demo2"}],"tab-id":"shell"},{title0:a(({value:p,isActive:o})=>[s("demo1")]),title1:a(({value:p,isActive:o})=>[s("demo2")]),tab0:a(({value:p,isActive:o})=>[nn]),tab1:a(({value:p,isActive:o})=>[sn]),_:1})]),an,n("ul",null,[tn,n("li",null,[n("p",null,[n("a",en,[s("9.参考simpleCallback.cu"),t(u)]),s("，把回调点设定在第二个内核启动后。使用nvvp运行并且观"),pn,s(" 察差异。")])])])])}const kn=r(G,[["render",on],["__file","F-第六章.html.vue"]]);export{kn as default};
