import{_ as i}from"./plugin-vue_export-helper-c27b6911.js";import{r,o,c as h,d as m,a as e,e as t,w as l,b as s,f as n}from"./app-2a2d189a.js";const p="/assets/figure1-44d4084b.png",c="/assets/algorithm1-ce84749d.png",u="/assets/algorithm2-eac2973f.png",d="/assets/figure2-d54d60b1.png",g="/assets/algorithm3-7c1eb810.png",f="/assets/figure3-451a0579.png",y="/assets/figure4-cc86e546.png",k="/assets/figure5-2fc5705f.png",N="/assets/table1-10dea11b.png",b={},v=e("h1",{id:"i-onlineknng",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-onlineknng","aria-hidden":"true"},"#"),s(" I-OnlineKNNG")],-1),w=e("p",null,"I-OnlineKNNG",-1),x=e("div",{class:"hint-container info"},[e("p",{class:"hint-container-title"},"相关信息"),e("ul",null,[e("li",null,"可能这里要有一些 关于 其它的对于这篇文章的解读"),e("li",null,"我们需要一些链接？"),e("li",null,"paper:"),e("li",null,"code：")])],-1),G={class:"table-of-contents"},q=n('<h2 id="no-0-摘要" tabindex="-1"><a class="header-anchor" href="#no-0-摘要" aria-hidden="true">#</a> No.0 摘要</h2><h2 id="这个no-1-ok" tabindex="-1"><a class="header-anchor" href="#这个no-1-ok" aria-hidden="true">#</a> 这个No.1 == OK</h2><h2 id="no-1-introduction" tabindex="-1"><a class="header-anchor" href="#no-1-introduction" aria-hidden="true">#</a> No.1 INTRODUCTION</h2>',3),M=e("ul",null,[e("li",null,[e("p",null,"dataset S, 数据集S，")]),e("li",null,[e("p",null,"each sample, 每个样本")]),e("li",null,[e("p",null,"a metric, 一个度量标准")]),e("li",null,[e("p",null,"the time complexity， 时间复杂度")]),e("li",null,[e("p",null,"目前许多存在的方法的问题：")]),e("li",null,[e("p",null,"1-数据维度比较小：on low-dimensional data")]),e("li",null,[e("p",null,"2-数据规模比较小：less than one million")]),e("li",null,[e("p",null,"3-特殊的度量规则：under specific metric i.e., l 2-norm")]),e("li",null,[e("p",null,"in the generic metric spaces：在一般度量空间中")]),e("li",null,[e("p",null,[s("Thanks to the introduction of NN-Descent in [4], the construction time complexity has been reduced from "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"O"),e("mo",{stretchy:"false"},"("),e("msup",null,[e("mi",null,"n"),e("mn",null,"1.94")]),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},"O(n^{1.94})")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"O"),e("span",{class:"mopen"},"("),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.94")])])])])])])])]),e("span",{class:"mclose"},")")])])]),s(" [6] to "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"O"),e("mo",{stretchy:"false"},"("),e("msup",null,[e("mi",null,"n"),e("mn",null,"1.14")]),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},"O(n^{1.14})")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"O"),e("span",{class:"mopen"},"("),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.14")])])])])])])])]),e("span",{class:"mclose"},")")])])]),s("for data with low dimensionality (e.g., 5) [4]")])]),e("li",null,[e("p",null,"一个潜在的问题：")]),e("li",null,[e("p",null,"大多数方法都是为固定的数据集构建近似的k-NN图。在实践中，数据集随时发生变化是很正常的。")]),e("li",null,[e("p",null,"不幸的是，对于大多数现有的方法[4]，[8]-[10]，数据集被假定为固定的。对数据集的任何更新都会调用k-NN图的完整重建。")]),e("li",null,[e("p",null,"the k-NN graph should be updated dynamically.")]),e("li",null,[e("p",null,"It is more convenient if it is allowed to simply insert/remove the samples into/from the existing k-NN graph. Nevertheless, it is complicated to update the k-NN graph with the support of conventional indexing structure such as locality-sensitive hashing(LSH) [11], R-Tree [12] or k-d tree [13].")]),e("li",null,[e("p",null,"如果允许简单地将样本 插入/从现有的k-NN图中 移除，则会更方便。然而，在传统索引结构的支持下，如位置敏感哈希(LSH)[11]、R-Tree[12]或k-d tree[13]，更新k-NN图是比较复杂的。")]),e("li",null,[e("p",null,"a generic approximate k-NN graph construction approach is presented， 通用的方法")]),e("li",null,[e("p",null,"图的构造问题被视为一个k-NN搜索任务。")]),e("li",null,[e("p",null,"graph construction problem is treated as a k-NN search task. The approximate k-NN graph is incrementally built by invoking each sample to query against the graph under construction. After one round of k-NN search, the query sample is joined into the graph along with the discovered top-k nearest neighbors. The k-NN lists of samples (already in the graph) that are visited during the search are accordingly updated. The NN search basically follows the hill-climbing strategy [14]. In order to achieve high performance in terms of both efficiency and quality, two major innovations are proposed.")]),e("li",null,[e("p",null,"上面这个要好好看看，是增量过程；")]),e("li",null,[e("p",null,"给定一个数据集(S)，(k)近邻图是指为数据集中的每个样本保留前(k)个最近邻的结构。它是流形学习[1]-[3]、计算机视觉、机器学习、多媒体信息检索[4]和视频标注[5]中的关键数据结构。由于它所起的基础作用，它已经被研究了几十年。基本上，给定一个度量标准，(k)近邻图的构建就是为每个数据样本找到前(k)个最近邻。当以暴力方式构建时，时间复杂度为(O(d·n^2))，其中(d)是维度，(n)是数据集的大小。由于大数据问题在各种情况下都很普遍，(d)和(n)都可能非常大。")]),e("li",null,[e("p",null,[s("given a dataset "),e("em",null,"S"),s(" , the "),e("em",null,"k"),s("-NN graph refers to the structure that keeps top-"),e("em",null,"k"),s(" nearest neighbors for each sample in the dataset. It is the key data structure in the manifold learning [1]–[3], computer vision, machine learning, multimedia information retrieval [4], and video annotation [5]. Due to the fundamental role it plays, it has been studied for several decades. Basically, given a metric, the construction of "),e("em",null,"k"),s("-NN graph is to find the top-"),e("em",null,"k"),s(" nearest neighbors for each data sample. When it is built in"),e("br"),s(" brute-force way, the time complexity is "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"d"),e("msup",null,[e("mi",null,"n"),e("mn",null,"2")])]),e("annotation",{encoding:"application/x-tex"},"d n^2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])])])])])])])])]),s("), where "),e("em",null,"d"),s(" is the dimension and "),e("em",null,"n"),s(" is the size of dataset. Due to the prevalence of big data issues in various contexts, both "),e("em",null,"d"),s(" and "),e("em",null,"n"),s(" could be very large.")])])],-1),A=e("p",null,[s("-"),e("code",null," 尽管近年来已经取得了许多进展，但近似 k 近邻图构建中潜在的主要问题仍然具有挑战性。首先，许多现有的方法仅在低维数据上表现良好。它们被假设处理的数据规模通常小于一百万。此外，大多数方法是在特定的度量标准下设计的，即\\(l_2\\)范数。只有最近的一些工作[4]、[6]、[7]旨在在通用度量空间中解决这个问题。由于在[4]中引入了 NN-Descent，对于低维数据（例如，维度为 5）[4]，构建时间复杂度从\\(O(n^{1.94})\\)降低到了\\(O(n^{1.14})\\)。")],-1),_=e("ul",null,[e("li",null,[e("p",null,[s("Despite numerous progress has been made in recent years, the major issues latent in the approximate "),e("em",null,"k"),s("-NN graph construction still remain challenging. First of all, many existing approaches perform well only on low-dimensional data. The scale of data they are assumed to cope with is usually less than one million. Moreover, most of approaches are designed under specific metric i.e., "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"l"),e("mn",null,"2")]),e("mo",null,"−"),e("mi",null,"n"),e("mi",null,"o"),e("mi",null,"r"),e("mi",null,"m")]),e("annotation",{encoding:"application/x-tex"},"l_2-norm")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8444em","vertical-align":"-0.15em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.3011em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0197em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])]),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),e("span",{class:"mbin"},"−"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.4306em"}}),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),e("span",{class:"mord mathnormal"},"m")])])]),s(". Only recent few works [4], [6], [7] aim to address this issue in the generic metric spaces. Thanks to the introduction of NN-Descent in [4], the construction time complexity has been reduced from "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"n"),e("mn",null,"1.94")])]),e("annotation",{encoding:"application/x-tex"},"n^{1.94}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.94")])])])])])])])])])])]),s(") [6] to "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"n"),e("mn",null,"1.14")])]),e("annotation",{encoding:"application/x-tex"},"n^{1.14}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.14")])])])])])])])])])])]),s(") for data with low dimensionality (e.g., "),e("em",null,"5"),s(") [4].")])]),e("li",null,[e("p",null,"除了上述主要问题之外，许多现有的方法仍然面临另一个潜在问题。即，大多数方法被设计用于为固定的数据集构建近似 k 近邻图。在实际中，数据集不时地发生变化并不罕见。对于大规模多媒体搜索任务来说尤其如此。例如，Flickr 中的照片和视频每天都在增加。如果采用 k 近邻图来支持基于内容的搜索和浏览。新的项目在上传后的下一分钟就应该可以被搜索到。电子购物也有类似的情况。新产品在网站上开始销售后，应该立即可以通过内容进行检索和浏览。")]),e("li",null,[e("p",null,[s("Besides the aforementioned major issues, many existing approaches still face another potential problem. Namely, most of the approaches are designed to build approximate "),e("em",null,"k"),s("-NN graph for a fixed dataset. In practice, it is not unusual that the dataset changes from time to time. This is particularly the case for large-scale multimedia search tasks. For example, the photos and videos in Flickr grow daily. Given "),e("em",null,"k"),s("-NN graph is adopted to support the content-based search and browse. The new items should be searchable in the next minute after being uploaded. Similar scenario is expected for e-shopping. The new products should be ready for retrieval and browsing by content right after being put on sale in the website.")])]),e("li",null,[e("p",null,"除了上述主要问题之外，许多现有的方法仍然面临另一个潜在问题。即，大多数方法被设计用于为固定的数据集构建近似 k 近邻图。在实际中，数据集不时地发生变化并不罕见。对于大规模多媒体搜索任务来说尤其如此。例如，Flickr 中的照片和视频每天都在增加。如果采用 k 近邻图来支持基于内容的搜索和浏览。新的项目在上传后的下一分钟就应该可以被搜索到。电子购物也有类似的情况。新产品在网站上开始销售后，应该立即可以通过内容进行检索和浏览。")]),e("li",null,[e("p",null,[s("In these scenarios, one would expect the "),e("em",null,"k"),s("-NN graph that works behind should be updated dynamically. Unfortunately, for most of the existing approaches [4], [8]–[10], the dataset is assumed to be fixed. Any update on the dataset invokes a complete reconstruction on the "),e("em",null,"k"),s("-NN graph.")])]),e("li",null,[e("p",null,[s("As a consequence, the aggregated costs are very high even the dataset is in small-scale. It is more convenient if it is allowed to simply insert/remove the samples into/from the existing "),e("em",null,"k"),s("-NN graph. Nevertheless, it is complicated to update the "),e("em",null,"k"),s("-NN graph with the support of conventional indexing structure such as locality-sensitive hashing (LSH) [11], R-Tree [12] or k-d tree [13].")])]),e("li",null,[e("p",null,"另一个与近似 k 近邻图构建密切相关的问题是最近邻搜索（NN 搜索），它也源于广泛的应用。最近邻搜索问题定义如下：给定一个查询向量（(q\\in R^d)），以及在同一维度下数据集中的(n)个候选样本。要求根据给定的度量(m(\\cdot,\\cdot))返回与查询最接近的样本。")]),e("li",null,[e("p",null,[s("Another problem that is closely related to approximate "),e("em",null,"k"),s("-NN graph construction is the nearest neighbor search (NN search), which also arises from a wide range of applications. The nearest neighbor search problem is defined as follows. Given a query vector ("),e("em",null,"q"),s(),e("em",null,"∈"),s(),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mi",null,"d")])]),e("annotation",{encoding:"application/x-tex"},"R^d")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"d")])])])])])])])])])]),s("), and "),e("em",null,"n"),s(" candidates in "),e("em",null,"S"),s(" that are under the same dimensionality. It is required to return the sample(s) that are the closest to the query according to a given metric "),e("em",null,"m"),s("("),e("em",null,"·**,"),s(),e("em",null,"·"),s(").")])]),e("li",null,[e("p",null,"在本文中，提出了一种通用的近似 k 近邻图构建方法。在一个统一的框架下解决了 k 近邻图构建和最近邻搜索问题。将图构建问题视为一个 k 近邻搜索任务。通过调用每个样本对正在构建的图进行查询，逐步构建近似 k 近邻图。在一轮 k 近邻搜索后，查询样本与发现的前 k 个最近邻一起加入到图中。在搜索过程中访问的样本（已经在图中）的 k 近邻列表相应地进行更新。最近邻搜索基本上遵循爬山策略[14]。为了在效率和质量方面实现高性能，提出了两项主要创新。")]),e("li",null,[e("p",null,[s("In this paper, a generic approximate "),e("em",null,"k"),s("-NN graph construction approach is presented. The issues of "),e("em",null,"k"),s("-NN graph construction and NN search are addressed under a unified framework. The graph construction problem is treated as a "),e("em",null,"k"),s("-NN search task. The approximate "),e("em",null,"k"),s("-NN graph is incrementally built by invoking each sample to query against the graph under construction. After one round of "),e("em",null,"k"),s("-NN search, the query sample is joined into the graph along with the discovered top-"),e("em",null,"k"),s(" nearest neighbors. The "),e("em",null,"k"),s("-NN lists of samples (already in the graph) that are visited during the search are accordingly updated. The NN search basically follows the hill-climbing strategy [14]. In order to achieve high performance in terms of both efficiency and quality, two major innovations are proposed.")]),e("ul",null,[e("li",null,[s("Restricted recursive neighborhood propagation (RRNP) is proposed to introduce the newly coming sample to its most likely neighbors, which enhances the quality of approximate "),e("em",null,"k"),s("-NN graph considerably.")]),e("li",null,"In order to boost the search performance, a lazy graph diversification (LGD) scheme is proposed. It helps to avoid unnecessary distance computations during the hill-climbing while involving no additional computations.")])]),e("li",null,[e("p",null,"这种方法的优点有几个方面。首先，在线构建避免了当前大多数近似 k 近邻图构建方法所遭受的重复距离计算。这使得它比经典的 NN-Descent 算法[4]更高效。其次，在线图构建特别适用于数据集动态变化的场景。此外，如[15]、[16]所示，在 k 近邻图的支持下，可以实现高效的最近邻搜索。因此，在我们的解决方案中，两个相关问题得到了共同解决。与最先进的最近邻搜索方法[8]、[15]、[16]相比，它在保持在线 k 近邻图的同时，显示出相似甚至略高的搜索效率。优点是它允许用户浏览紧密相关内容（即 k 个最近邻）之间的超链接。此外，我们的方法对距离度量没有特定要求，因此是一种通用解决方案，这在我们的实验中得到了证实。")]),e("li",null,[e("p",null,[s("The advantages of this approach are several folds. Firstly, the online construction avoids repetitive distance computations that most of the current approximate "),e("em",null,"k"),s("-NN graph construction approaches suffer from. This makes it more efficient than the classic NN-Descent algorithm [4]. Secondly, the online graph construction is particularly suitable for the scenario that the dataset is dynamically changing. Moreover, with the support of "),e("em",null,"k"),s("-NN graph, efficient NN search is achievable as demonstrated in [15], [16]. As a result, two related issues have been jointly addressed in our solution. Compared to the state-of-the-art NN search approaches [8], [15], [16], it shows similar or even slightly higher search efficiency while maintaining an online "),e("em",null,"k"),s("-NN graph. The advantage is that it allows the user to browse over hyperlinks between closely related contents (i.e., "),e("em",null,"k"),s(" nearest neighbors). Furthermore, our approach has no specification on the distance measure, it is therefore a generic solution, which is confirmed in our experiments")])]),e("li",null,[e("p",null,"本文的其余部分组织如下。在第二部分中，对近似 k 近邻图构建和近似 k 近邻搜索的研究工作进行了简要回顾。第三部分提出了一种最近邻搜索算法，近似 k 近邻图构建方法基于此算法构建。第四部分提出了一种在线近似 k 近邻图构建方法和增强方案。第五部分提出了一种基于在线图的更高效的最近邻搜索方法。第六部分介绍了关于所提出的 k 近邻图构建和最近邻搜索有效性的实验研究。第七部分对本文进行了总结。")]),e("li",null,[e("p",null,[s("The remainder of this paper is organized as follows. In Section II, a brief review of the research works on the approximate "),e("em",null,"k"),s("-NN graph construction and approximate "),e("em",null,"k"),s("-NN search is presented. Section III presents an NN search algorithm upon which the approximate "),e("em",null,"k"),s("-NN graph construction approach is built. Section IV presents an online approximate "),e("em",null,"k"),s("-NN graph construction approach and the enhancement schemes. A more efficient NN search approach based on the online graph is presented in Section V. The experimental studies about the effectiveness of proposed "),e("em",null,"k"),s("-NN graph construction and NN search are presented in Section VI. Section VII concludes the paper.")])])],-1),T=n('<h2 id="这个no-2-ok" tabindex="-1"><a class="header-anchor" href="#这个no-2-ok" aria-hidden="true">#</a> 这个No.2 == OK</h2><h2 id="no-2-related-works" tabindex="-1"><a class="header-anchor" href="#no-2-related-works" aria-hidden="true">#</a> No.2 RELATED WORKS</h2><h3 id="a-k-nn-search" tabindex="-1"><a class="header-anchor" href="#a-k-nn-search" aria-hidden="true">#</a> A. k-NN Search</h3>',3),L=e("ul",null,[e("li",null,[e("p",null,"关于 k 近邻搜索问题的早期研究可以追溯到 20 世纪 70 年代，当时在文件系统上进行最近邻搜索的需求出现了。在那个时候，要处理的数据维度非常低，通常是一维的。这个问题通过 B 树[17]及其变体 B+树[17]得到了很好的解决，基于此，最近邻搜索的时间复杂度可以低至 O(log(n))。B 树不能自然地扩展到一维以上的情况。为了处理多维数据中的最近邻搜索，设计了更复杂的索引结构。具有代表性的结构有 k-d 树[13]、R 树[12]和 R*树[18]。对于 k-d 树，每次选择一个轴向量将数据集均匀地分成两部分。通过反复进行这种二分操作，超空间被划分成嵌入式的层次子空间。最近邻搜索是通过遍历一个或几个分支来探测最近邻。与一维情况下的 B 树不同，这种划分方案不能排除最近邻位于这些候选子空间之外的可能性。因此，不可避免地要对树中的大量分支进行广泛的探测。出于这个原因，使用 k-d 树等进行最近邻搜索可能非常慢。最近的索引结构 FLANN[19]、[20]使用层次 k 均值和多个 k-d 树对空间进行划分。虽然高效，但仍能观察到次优结果。")]),e("li",null,[e("p",null,[s("The early study about the "),e("em",null,"k"),s("-NN search issue could be traced back to the "),e("em",null,"1970 s"),s(" when the need for NN search on the file system arises. In those days, the data to be processed are in very low dimensions, typically 1D. This problem is well-addressed by B-Tree [17] and its variant "),e("em",null,"B"),s("+-tree [17], based on which the NN search time complexity could be as low as "),e("em",null,"O"),s("("),e("em",null,"log"),s("("),e("em",null,"n"),s(")).")])]),e("li",null,[e("p",null,"B-tree is not naturally extensible to more than 1D case. More sophisticated indexing structures were designed to handle NN search in multi-dimensional data. Representative structures are k-d-tree [13], R-tree [12] and R*-tree [18].")]),e("li",null,[e("p",null,"For k-d tree, pivot vector is selected each time to split the dataset evenly into two. By applying this bisecting repeatedly, the hyper-space is partitioned into embedded hierarchical subspaces.")]),e("li",null,[e("p",null,"The NN search is performed by traversing over one or several branches to probe the nearest neighbors. Unlike B-tree in 1D case, the partition scheme does not exclude the possibility that nearest neighbor resides outside of these candidate subspaces."),e("ul",null,[e("li",null,"这个 是 高纬度 的kd-tree 不行的关键原因；")])]),e("li",null,[e("p",null,[s("Therefore, extensive probing over a large number of branches in the tree becomes inevitable. For this reason, NN search with k-d tree and the like could be very slow. Recent indexing structure FLANN [19], [20] partitions space with hierarchical "),e("em",null,"k"),s("-means and multiple k-d trees. Although efficient, sub-optimal results are observed.")])]),e("li",null,[e("p",null,"对于所有上述的树划分方法，另一个主要缺点在于它们对内存的巨大需求。一方面，为了支持快速比较，所有的候选向量都被加载到内存中。另一方面，用于索引的树节点也占用了相当数量的额外内存。总体而言，内存消耗通常是参考集大小的几倍。")]),e("li",null,[e("p",null,"For all the aforementioned tree partitioning methods, another major disadvantage lies in their heavy demand in memory. On the one hand, in order to support fast comparison, all the candidate vectors are loaded into the memory. On the other hand, the tree nodes that are used for indexing also take up considerable amount of extra memory. Overall, the memory consumption is usually several times bigger than the size of reference set.")]),e("li",null,[e("p",null,"为了降低内存复杂度，提出了基于量化的方法[21]-[25]来压缩参考向量[21],[26]。在最近的工作[27],[28]中，由于更好的索引结构和在 GPU 上的高效计算，基于量化的方法的性能得到了提升。对于所有基于量化的方法，它们有两个共同点。首先，候选向量都通过向量（或子向量）量化进行压缩。其次，在查询和压缩后的候选向量之间进行最近邻搜索。查询和候选之间的距离通过查询和用于量化的词汇之间的距离来近似。由于对参考向量进行了大量压缩，很难实现高搜索质量。此外，这些类型的方法仅适用于具有(l_p)范数的度量空间。")]),e("li",null,[e("p",null,[s("In order to reduce the memory complexity, quantization based approaches [21]–[25] are proposed to compress the reference vectors [21], [26]. In recent works [27], [28], the performance of quantization based approaches is boosted due to the better indexing structure and the efficient computation on GPU. For all the quantization based methods, they share two things in common. Firstly, the candidate vectors are all compressed via vector (or sub-vector) quantization. Secondly, NN search is conducted between the query and the compressed candidate vectors. The distance between query and candidates is approximated by the distance between query and vocabulary words that are used for quantization. Due to the heavy compression on the reference vectors, high search quality is hardly achievable. Furthermore, these types of approaches are only suitable for metric spaces of "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"l"),e("mi",null,"p")]),e("mo",null,"−"),e("mi",null,"n"),e("mi",null,"o"),e("mi",null,"r"),e("mi",null,"m")]),e("annotation",{encoding:"application/x-tex"},"l_p-norm")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.9805em","vertical-align":"-0.2861em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0197em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"p")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.2861em"}},[e("span")])])])])]),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),e("span",{class:"mbin"},"−"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.4306em"}}),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),e("span",{class:"mord mathnormal"},"m")])])])])]),e("li",null,[e("p",null,"除了上述方法之外，已经有几次尝试将局部敏感哈希（LSH）[11],[29]应用于最近邻搜索。一般来说，搜索阶段涉及两个步骤。即，步骤1收集与查询具有相同或相似哈希键的候选。步骤2在查询和所有这些选定的候选之间进行详尽的比较，以找出最近邻。在最近的工作中，还提出了通过标量量化进行哈希以用于大规模图像搜索[30]。然而，如果期望高搜索质量，计算成本仍然很高。此外，设计适用于各种度量的哈希函数并非易事。")]),e("li",null,[e("p",null,"Apart from the above approaches, several attempts have been made to apply LSH [11], [29] on NN search.")]),e("li",null,[e("p",null,"In general, there are two steps involved in the search stage.")]),e("li",null,[e("p",null,[s("Namely,"),e("em",null,"step 1"),s(" collects the candidates that share the same or similar hash keys as the query.")])]),e("li",null,[e("p",null,[e("em",null,"Step 2"),s(" performs an exhaustive comparison between the query and all these selected candidates to find out the nearest neighbor.")])]),e("li",null,[e("p",null,"In recent work, hashing by scalar quantization is also proposed for large-scale image search [30]. Whereas computational cost remains high if one expects high search quality. Additionally, the design of hash functions that are feasible for various metrics is non-trivial.")]),e("li",null,[e("p",null,"最近，基于图的方法得到了广泛的探索[7],[8],[14],[31]-[33]。尽管它们在细节上彼此不同，但它们都是基于一种爬山过程构建的。搜索[14]从一组随机种子（向量空间中的随机位置）开始。它通过最佳优先搜索在近似 k 近邻图或相对 k 近邻图（预先构建）上进行迭代遍历。在最近邻图的引导下，搜索过程在每一轮中更接近真实的最近邻，直到找不到更好的候选为止。[8],[14],[16],[32],[33]中的方法遵循类似的搜索过程。它们之间的主要区别在于用于支持搜索的图。根据最近的研究[15]，这些基于图的方法在各种类型的数据上表现出比其他类型方法更优越的性能。")]),e("li",null,[e("p",null,"Recently, the graph-based approaches have been extensively explored [7], [8], [14], [31]–[33]. Although they are different from each other in details, all of them are built upon a hill-climbing procedure.")]),e("li",null,[e("p",null,"The search [14] starts from a group of random seeds (random locations in the vector space). It traverses iteratively over an approximate k-NN graph or a relative k-NN graph (built-in advance) by the best-first search. Guided by the NN graph, the search procedure ascends closer to the true nearest neighbor in each round until no better candidates could be found.")]),e("li",null,[e("p",null,"Approaches in [8], [14], [16], [32], [33] follow similar search procedure. The major difference between them lies in the graph used to support the search. According to recent study [15], these graph-based approaches demonstrate superior performance over other types of approaches across a variety of types of data.")])],-1),I=e("h3",{id:"b-approximate-k-nn-graph-construction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#b-approximate-k-nn-graph-construction","aria-hidden":"true"},"#"),s(" B. Approximate k-NN Graph Construction")],-1),O=e("ul",null,[e("li",null,[e("p",null,"近似 k 近邻图构建的方法大致可以分为两类。像[10],[34]这样的方法基本上遵循分治策略。第一步，将样本划分为若干个小的子集。在每个子集中进行详尽的比较。建立一个子集中任意两个样本之间的接近关系（即 k 近邻图中的边）。第二步，收集这些接近关系来构建 k 近邻图。为了提高性能，第一步用不同的划分重复几次。产生的接近关系用于更新 k 近邻图。由于很难设计出一种适用于各种通用空间的划分方案，所以它们通常只在(l_2)空间中有效。 另一类近似 k 近邻图构建方法，典型的如 NN-Descent[4]避免了这样的缺点。NN-Descent 中的图构建从一个随机的 k 近邻图开始。基于“邻居的邻居很可能是邻居”的原则，k 近邻图通过在每个样本的邻域中调用样本之间的比较来演化。比较中产生的更好的接近关系用于更新一个样本的邻域。这种方法被证明是通用且高效的。从本质上讲，它可以被看作是批量执行爬山算法[4]。最近，在文献[8]中也出现了源自上述方法的混合方案。")]),e("li",null,[e("p",null,[s("The approaches for approximate "),e("em",null,"k"),s("-NN graph construction can be roughly grouped into two categories.")])]),e("li",null,[e("p",null,[s("Approaches such as [10], [34] basically follow the divide-and-conquer strategy. On the first step, samples are partitioned into a number of small subsets. Exhaustive comparisons are carried out within each subset. The closeness relations ("),e("em",null,"viz."),s(", edges in the "),e("em",null,"k"),s("-NN graph) between any two samples in one subset are established.")])]),e("li",null,[e("p",null,[s("In the second step, these closeness relations are collected to build the "),e("em",null,"k"),s("-NN graph. To enhance the performance, the first step is repeated several times with different partitions. The produced closeness relations are used to update the "),e("em",null,"k"),s("-NN graph. Since it is hard to design a partition scheme that is feasible for various generic spaces, they are generally only effective in "),e("em",null,"l"),s("2-space.")])]),e("li",null,[e("p",null,[s("Another category of approximate "),e("em",null,"k"),s("-NN graph construction, typically NN-Descent [4] avoids such disadvantages. The graph construction in NN-Descent starts from a random "),e("em",null,"k"),s("-NN graph. Based on the principle “neighbor’s neighbor is likely to be the neighbor,” the "),e("em",null,"k"),s("-NN graph evolves by invoking comparison between samples in each sample’s neighborhood. Better closeness relations that are produced in the comparison are used to update the neighborhood of one sample. This approach turns out to be generic and efficient. Essentially, it can be viewed as performing hill-climbing batchfully [4]. Recently, the mixture scheme derived from the above approaches is also seen in the literature [8].")])]),e("li",null,[e("p",null,"值得注意的是，[7]、[15]、[16]中提出的方法不是近似 k 近邻图构建算法。这些图主要是为 k 近邻搜索而构建的。在这些方法中，为了比较效率，故意省略了应该在一个样本的 k 近邻列表中的样本。同时保持与远程邻居的链接[16]。因此，这些方法构建的图并不是真正意义上的 k 近邻图。这种类型的图很难支持除 k 近邻搜索之外的任务。可导航小世界图（NSW）[32]主要是为了支持快速最近邻搜索而设计的，它可以被看作是一种在线图构建方法。然而，由于其搜索策略不佳，导致图的质量较低，因此它不适合用于近似 k 近邻图的构建。")]),e("li",null,[e("p",null,[s("It is worth noting that approaches proposed in [7], [15], [16] are not approximate "),e("em",null,"k"),s("-NN graph construction algorithms. The graphs are built primarily for "),e("em",null,"k"),s("-nearest neighbor search. In these approaches, the samples which should be in the "),e("em",null,"k"),s("-NN list of one sample are deliberately omitted for comparison efficiency.While the links to the remote neighbors are maintained [16]. As a consequence, graphs constructed by these approaches are not "),e("em",null,"k"),s("-NN graph in the real sense.")])]),e("li",null,[e("p",null,[s("Such kind of graphs are hardly supportive for tasks beyond "),e("em",null,"k"),s("-NN search. Navigable small-world graph (NSW) [32] is primarily designed to support fast NN search, it could be viewed as an online graph construction approach. However, it is not suitable for approximate "),e("em",null,"k"),s("-NN graph construction for its poor search strategy, which leads to low graph quality.")])]),e("li",null,[e("p",null,"在上述大多数方法中，一个潜在的问题是构建过程必须记录样本对之间已经进行的比较，以避免重复比较。然而，其空间复杂度可能高达(O(n^2))。否则，即使采用特定的采样方案，重复比较也不可避免[4]。在本文中，近似(k)近邻图的构建和(k)近邻搜索被共同解决。近似(k)近邻图的构建是通过将每个样本作为查询，对正在构建的近似(k)近邻图进行查询来进行的。由于每次查询对于正在构建的图来说都是新的，数据集中的两个样本最多比较一次。避免了重复比较。")]),e("li",null,[e("p",null,"In most of the aforementioned approaches, one potential issue is that the construction procedure has to keep records on the comparisons that have been made between sample pairs to avoid repetitive comparisons.")]),e("li",null,[e("p",null,[s("However, its space complexity could be as high as "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"O"),e("mn",null,"2")])]),e("annotation",{encoding:"application/x-tex"},"O^2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"O"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])])])])])])])])]),s("). Otherwise the repetitive comparisons are inevitable even by adopting specific sampling schemes [4]. In this paper, the approximate "),e("em",null,"k"),s("-NN graph construction and "),e("em",null,"k"),s("-NN search are addressed jointly. The approximate "),e("em",null,"k"),s("-NN graph construction is undertaken by invoking each sample as a query to query against the approximate "),e("em",null,"k"),s("-NN graph that is under construction. Since the query is new to the graph under construction each time, two samples in the dataset are compared at most once.The repetitive comparisons are avoided.")])])],-1),S=n('<h2 id="这个no-3-ok" tabindex="-1"><a class="header-anchor" href="#这个no-3-ok" aria-hidden="true">#</a> 这个No.3 == OK</h2><h2 id="no-3-nn-search-on-the-k-nn-graph" tabindex="-1"><a class="header-anchor" href="#no-3-nn-search-on-the-k-nn-graph" aria-hidden="true">#</a> No.3 NN SEARCH ON THE K-NN GRAPH</h2><ul><li><p>这里的话 是 在 K-NN G 上进行的search； 而不是 之前的index上进行 search</p></li><li><p>在我们介绍图构建方法之前，先介绍构建方法所基于的最近邻搜索。我们的搜索过程与 HNSW [16] 中提出的过程在很大程度上相似。然而，与 HNSW 不同的是，我们的搜索过程是在一个平面的(k)-近邻图上进行的，而不是在一个分层的相对邻域图上进行。与 HNSW 中的单层搜索相比，我们还进一步引入了一些修改。在平面的(k)-近邻图中，一个样本的(k)个最近邻及其反向邻居都被保留。在下面，在我们介绍搜索过程之前，先讨论这个图的结构。</p></li><li><p>Before we introduce our graph construction approach, the NN search, on which the construction approach is based, is presented.</p><ul><li>就是说图的构造knng是search进行的，并且不是index而是knng。</li></ul></li><li><p>Our search procedure is largely similar as the procedure proposed in HNSW [16]. Whereas unlike HNSW, our search procedure is undertaken on a flat <em>k</em>-NN graph instead of a hierarchical relative neighborhood graph.</p><ul><li>基本search都要说这个，但是 这个是 flat king，然后hnsw 是 hrng</li></ul></li><li><p>Compared to the single-layer search in HNSW, a few modifications are further introduced. In the flat <em>k</em>-NN graph, both the <em>k</em> nearest neighbors of one sample and its reverse neighbors are kept. In the following, the structure of this graph is discussed before we present the search procedure.</p></li></ul><figure><img src="'+p+'" alt="figure1" tabindex="0" loading="lazy"><figcaption>figure1</figcaption></figure>',4),R=e("ul",null,[e("li",null,[e("p",null,"给定(k)-近邻图(G)，(G[i])返回样本(i)的(k)近邻列表。相应地，(\\overline{G})是(G)的反向(k)-近邻图，它只不过是图(G)的重新组织。(\\overline{G}[i])保存样本(i)出现在其(k)近邻列表中的样本的 ID。注意，(\\overline{G}[i])的大小不一定是(k)，并且(\\overline{G}[i])和(G[i])之间会有重叠。图(G)和(\\overline{G})的图示见图 1。在我们的实现中，(G[i])和(\\overline{G}[i])的并集保存在一个动态数组（而不是链表）中。前(k)个元素是(k)个最近邻（即(G[i])）。其余元素是样本(i)的反向近邻，它们不在(k)个最近邻的覆盖范围内。为了表述清晰，这些邻域中的样本仍然称为(G[i])和(\\overline{G}[i])。在(k)-近邻图(G)及其反向图(\\overline{G})的支持下，最近邻搜索算法如算法 1 所示。")]),e("li",null,[e("p",null,[s("Given "),e("em",null,"k"),s("-NN graph "),e("em",null,"G"),s(", "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"G[i]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])])]),s(" returns a "),e("em",null,"k"),s("-NN list of sample "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"i")]),e("annotation",{encoding:"application/x-tex"},"i")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6595em"}}),e("span",{class:"mord mathnormal"},"i")])])]),s(".Accordingly, "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"G"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8833em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8833em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G")])]),e("span",{style:{top:"-3.8033em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" is the reverse "),e("em",null,"k"),s("-NN graph of "),e("em",null,"G"),s(", which is nothing more than a re-organization of graph"),e("em",null,"G"),s(".")])]),e("li",null,[e("p",null,[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G[i]}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.2em","vertical-align":"-0.25em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.95em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])]),e("span",{style:{top:"-3.87em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.25em"}},[e("span")])])])])])])]),s(" keeps ID of samples that sample "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"i")]),e("annotation",{encoding:"application/x-tex"},"i")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6595em"}}),e("span",{class:"mord mathnormal"},"i")])])]),s(" appears in their "),e("em",null,"k"),s("-NN lists. Noticed that the size of "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G[i]}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.2em","vertical-align":"-0.25em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.95em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])]),e("span",{style:{top:"-3.87em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.25em"}},[e("span")])])])])])])]),s(" is not necessarily "),e("em",null,"k"),s(" and there would be overlappings between "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"G[i]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])])]),s(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G[i]}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.2em","vertical-align":"-0.25em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.95em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])]),e("span",{style:{top:"-3.87em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.25em"}},[e("span")])])])])])])]),s(".")]),e("ul",null,[e("li",null,"这个就是说 反向的话 并不是k个 并且 正向和反向是有可能重叠的；")])]),e("li",null,[e("p",null,[s("An illustration of graphs "),e("em",null,"G"),s(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"G"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8833em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8833em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G")])]),e("span",{style:{top:"-3.8033em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" are seen in Fig. 1. In our implementation, the union of "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"G[i]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])])]),s(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G[i]}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.2em","vertical-align":"-0.25em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.95em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])]),e("span",{style:{top:"-3.87em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.25em"}},[e("span")])])])])])])]),s(" are kept in a dynamic array (instead of linked list). The first "),e("em",null,"k"),s(" elements are the "),e("em",null,"k"),s(" nearest neighbors (namely "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"G[i]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])])]),s("). The remaining elements are the reverse neighbors of sample "),e("em",null,"i"),s(" that are outside the coverage of "),e("em",null,"k"),s(" nearest neighbors.")]),e("ul",null,[e("li",null,"这个话是说 用vector存储，前k个是正向的knn，剩下的就是反向的，并且是去重复的；")])]),e("li",null,[e("p",null,[s("For presentation clarity, these samples in the neighborhood are still referred to as "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"G[i]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])])]),s(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mrow",null,[e("mi",null,"G"),e("mo",{stretchy:"false"},"["),e("mi",null,"i"),e("mo",{stretchy:"false"},"]")]),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G[i]}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.2em","vertical-align":"-0.25em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.95em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G"),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mclose"},"]")])]),e("span",{style:{top:"-3.87em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.25em"}},[e("span")])])])])])])]),s(".. With the support of "),e("em",null,"k"),s("-NN graph "),e("em",null,"G"),s(" and its reverse graph "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"G"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8833em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8833em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G")])]),e("span",{style:{top:"-3.8033em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(", the NN search algorithm is presented in Alg. 1.")])])],-1),D=n('<figure><img src="'+c+'" alt="algorithm1" tabindex="0" loading="lazy"><figcaption>algorithm1</figcaption></figure><ul><li><p>算法1总体上是一个具有多个起始种子的爬山过程[14]。查询(q)首先与(p)个种子样本进行比较。比较后的样本保存在两个优先队列(Q)和(R)中。(Q)主要维护查询(q)的前(k)个最近邻。检索到的邻居根据它们与查询的距离按升序排序。当一个更近的样本加入到(Q)中时，(Q)中的最后一个样本将被替换掉。与(Q)不同，(R)的大小不是固定的。在每次迭代中（第11 - 27行），算法访问与查询(q)最接近的样本(r\\in R)的邻域。根据与查询的新的近邻样本相应地更新(Q)和(R)。迭代继续，直到(R)为空或(Q)不能被更新。在比较中，距离函数(m(\\cdot,\\cdot))可以是在输入数据集上定义的任何度量。很明显，这是一个通用的搜索算法。算法1与HNSW[16]中的单层最近邻搜索算法的主要区别在两个方面。首先，它从多个随机种子开始，这使得它比HNSW具有更稳定的性能。此外，保留了访问过的样本以及它们与查询的相应距离。这些距离将在后面用于辅助在线(k)-近邻图的多样化。</p></li><li><p>Alg. 1 in general is a hill-climbing procedure [14] with multiple starting seeds. The query <em>q</em> is firstly compared to <em>p</em> seed samples. The compared samples are kept in two priority queues <em>Q</em> and <em>R</em>. <em>Q</em> basically maintains the top-<em>k</em> nearest neighbors of the query <em>q</em>.</p><ul><li>通过算法1看出，R 像是候选邻居集合；</li></ul></li><li><p>The retrieved neighbors are sorted in ascending order according to their distance to the query.1 As a closer sample is joined into Q, the rear sample in <em>Q</em> will be swapped out. Unlike <em>Q</em>, the size of <em>R</em> is not fixed. In each iteration (<em>Line 11 – 27</em>), the algorithm visits the neighborhood of the closest sample <em>r</em> <em>∈</em> <em>R</em> to the query <em>q</em>.</p></li><li><p><em>Q</em> and <em>R</em> are updated accordingly with new close samples to the query. The iteration continues until <em>R</em> is empty or <em>Q</em> cannot be updated. In the comparison, the distance function <em>m</em>(<em>·,</em> <em>·</em>) could be any metric defined on the input dataset. It is clear to see this is a generic search algorithm.</p></li><li><p>The major differences between Alg. 1 and the single-layer NN search algorithm from HNSW [16] are in two aspects.</p><ul><li>Firstly, it starts from multiple random seeds, which leads to more stable performance over HNSW.</li><li>Moreover, the visited samples and the corresponding distances to the query are kept. These distances will be used to assist the online <em>k</em>-NN graph diversification later.这个表示 已经访问过的存储到了V并且距离存储到了D；可详细看算法1；可用于后续的在线的图的散化并且；</li></ul></li></ul><h2 id="这个no-4-ok" tabindex="-1"><a class="header-anchor" href="#这个no-4-ok" aria-hidden="true">#</a> 这个No.4 == OK</h2><h2 id="no-4-online-approximate-k-nn-graph-construction" tabindex="-1"><a class="header-anchor" href="#no-4-online-approximate-k-nn-graph-construction" aria-hidden="true">#</a> No.4 ONLINE APPROXIMATE <em>K</em>-NN GRAPH CONSTRUCTION</h2>',4),z=e("ul",null,[e("li",null,[e("p",null,"算法1中最近邻搜索算法的前提条件是 k 近邻图 G 及其反向 k 近邻图 G'。在本节中，我们将展示如何基于最近邻搜索算法本身构建近似的 k 近邻图及其反向图。此外，提出了一种称为受限递归邻域传播的策略来提高近似 k 近邻图的质量。而且，提出了一种在线图多样化方案。与[7]、[15]中的方法相比，它不涉及额外的距离计算，同时比算法1能实现更高效的最近邻搜索。")]),e("li",null,[e("p",null,[s("The prerequisites(前提条件) for the NN search algorithm in Alg. 1 are the "),e("em",null,"k"),s("-NN graph "),e("em",null,"G"),s(" and its reverse "),e("em",null,"k"),s("-NN graph "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"G"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline G")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8833em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8833em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G")])]),e("span",{style:{top:"-3.8033em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(".")])]),e("li",null,[e("p",null,[s("we are going to show how an approximate "),e("em",null,"k"),s("-NN graph and its reverse graph are built based on NN search algorithm itself.")])]),e("li",null,[e("p",null,[s("Additionally, a strategy called "),e("em",null,"restricted recursive neighborhood"),s(),e("em",null,"propagation"),s(" is proposed to enhance the quality of approximate "),e("em",null,"k"),s("-NN graph. 这个是说 提高图的质量")])]),e("li",null,[e("p",null,"Moreover, an online graph diversification scheme is proposed. Compared to the approaches in [7], [15], it involves no additional distance computations while leading to more efficient NN search than Alg. 1.这个是说散化提高search性能并且是在线的；")])],-1),F=n('<h3 id="a-approximate-k-nn-graph-construction-by-search" tabindex="-1"><a class="header-anchor" href="#a-approximate-k-nn-graph-construction-by-search" aria-hidden="true">#</a> <em>A. Approximate k-NN Graph Construction by Search</em></h3><ul><li><p>思考： 在线的更新的话肯定要考虑到hnsw的模式；就是以search代construction</p></li><li><p>在算法1中，搜索从参考集的几个随机位置开始，并沿着几条路径移动。在每次迭代中，搜索朝着查询的更近邻域移动。在理想情况下，会找到前(k)个最近邻。一方面，在搜索后这个查询的前(k)个最近邻是已知的。另一方面，参考集中的一些样本被引入了一个新邻居（即这个查询）。因此，(k)-近邻图可以被扩充以包括这个查询样本。</p></li><li><p>In Alg. 1, the search starts from several random locations of the reference set and moves along several trails. The search moves towards the closer neighborhood of a query in each iteration. In the ideal case, top-<em>k</em> nearest neighbors will be discovered.</p></li><li><p>On the one hand, the top-<em>k</em> nearest neighbors of this query are known after the search. On the other hand, some samples in the reference set are introduced with a new neighbor (namely the query). As a result, the <em>k</em>-NN graph could be augmented to include this query sample. //？？？？？？就是说 对于目前这个query，不仅可以补充自己的k近邻，其k近邻也可以补充其k近邻通过尝试加入这个query到自己</p></li><li><p>受此观察的启发，构思了在线(k)-近邻图构建算法。首先，从(S)的一个小子集上详尽地构建一个初始图。在本文中，(S)的大小固定为 64。之后，将剩余的每个样本都视为查询，按照算法 1 的流程在(k)-近邻图上进行查询。每次搜索后，查询样本的(k)-近邻列表被加入到图中。在搜索过程中被访问过的样本的(k)-近邻列表也相应地进行更新。构建算法的一般过程总结在算法 2 中。</p></li><li><p>Motivated by this observation, the online <em>k</em>-NN graph construction algorithm is conceived.</p></li><li><p>First, an initial graph is built exhaustively from a small subset of <em>S</em>. The size of <em>S</em> is fixed to <em>64</em> in this paper.</p><ul><li>这个的意思是 先找64个点进行暴力 建立knng小的；以及反向；</li></ul></li><li><p>After that, each of the remaining samples is treated as a query to query against the <em>k</em>-NN graph following the flow of Alg. 1.</p></li><li><p>The <em>k</em>-NN list of a query sample is joined into the graph after each search.</p></li><li><p>The <em>k</em>-NN lists of samples which have been visited during the search are accordingly updated. The general procedure of the construction algorithm is summarized in Alg. 2.</p></li></ul><figure><img src="'+u+'" alt="algorithm2" tabindex="0" loading="lazy"><figcaption>algorithm2</figcaption></figure>',3),W=e("ul",null,[e("li",null,[e("p",null,"在算法2中，近似(k)-近邻图的构建过程基本上是对最近邻搜索算法和图更新函数的重复调用。函数(InsertG(\\cdot))负责在图(G)和(\\overline{G})中向(r)的(k)-近邻列表中插入一条边。(InsertG(\\cdot))内部的主要操作涉及对(r)的(k)-近邻列表和反向(k)-近邻列表的更新。如果一个更近的样本被加入，那么(k)-近邻列表末尾的一个样本将被删除。保留(k)-近邻的距离以便列表始终可以被排序。")]),e("li",null,[e("p",null,[s("In Alg. 2, the procedure of approximate "),e("em",null,"k"),s("-NN graph construction is basically a repetitive calling of the NN search algorithm and graph update function.")])]),e("li",null,[e("p",null,[s("Function "),e("em",null,"InsertG"),s("("),e("em",null,"·"),s(") is responsible for inserting an edge into "),e("em",null,"k"),s("-NN list of "),e("em",null,"r"),s(" in graphs "),e("em",null,"G"),s(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"G"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overline{G}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8833em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8833em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"G")])]),e("span",{style:{top:"-3.8033em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(". The major operations inside "),e("em",null,"InsertG"),s("("),e("em",null,"·"),s(") involve the update of "),e("em",null,"k"),s("-NN list and the reverse "),e("em",null,"k"),s("-NN list of "),e("em",null,"r"),s(". 所以就是说这个的是不涉及q的knn的；")])]),e("li",null,[e("p",null,[s("A sample in the rear of a "),e("em",null,"k"),s("-NN list is deleted if a closer sample is joined in. Distances of "),e("em",null,"k"),s("-NNs are kept to allow the list to be sorted all the time.")])]),e("li",null,[e("p",null,"尽管在算法2中输入数据集的大小是固定的，但对于一个开放集合显然是可行的，在开放集合中，新的样本可以不时地加入。正如在实验部分（第六节）中将揭示的那样，算法2已经表现得非常好。在下面，将分别提出两种新颖的方案来进一步提升图构建和最近邻搜索的性能。")]),e("li",null,[e("p",null,"Although the size of input dataset is fixed in Alg. 2, it is apparently feasible for an open set, for which new samples are allowed to join in from time to time. // 因为是从一个固定的数据集一点一点的输入的；")]),e("li",null,[e("p",null,"As will be revealed in the experiments (Section VI), Alg. 2 already performs very well. In the following, two novel schemes are presented to further boost the performance in the graph construction and NN search respectively.")])],-1),H=n('<h3 id="b-restricted-recursive-neighborhood-propagation" tabindex="-1"><a class="header-anchor" href="#b-restricted-recursive-neighborhood-propagation" aria-hidden="true">#</a> <em>B. Restricted Recursive Neighborhood Propagation</em></h3><figure><img src="'+d+'" alt="figure2" tabindex="0" loading="lazy"><figcaption>figure2</figcaption></figure><ul><li><p>在算法2的最后几步中，只要查询样本q在每个r的k近邻范围内，就将其插入到每个r的邻域中。注意，只有与q足够接近的少数r才会被考虑。在r的邻域中，可能存在一些在爬山搜索期间未与q进行比较的样本（如图2（a）中虚线多边形内的空心圆圈所示）。基于“邻居的邻居很可能也是邻居”的原则，这些未访问的样本很可能是样本q的近邻。因此，将q和这些未访问的样本相互插入到对方的邻域中是合理的。在此插入之后，q有可能被引入以遇到更多未访问的邻居。结果，这种插入可以递归地进行，直到不再遇到新的未访问邻居为止。此外，这种传播被限制在样本的紧密邻域内。例如，如图2（b）所示，只有当m(q,p1)小于p1与其第k个邻居之间的距离时，才传播p1的邻域。这个操作被称为受限递归邻域传播（RRNP）。我们发现这个操作进一步提高了k近邻图的质量，同时只引入了较小的计算开销。</p></li><li><p>In the last steps of Alg. 2, the query sample <em>q</em> is inserted to the neighborhood of each <em>r</em> as long as <em>q</em> is in their <em>k</em>-NN range. Noticed that only a few <em>r</em>s that are sufficiently close to <em>q</em> will be considered.</p><ul><li>这个是说q 与 r 是已经计算过distance了；</li></ul></li><li><p>In the neighborhood of <em>r</em>, it is possible that there are some samples that were not compared to <em>q</em> during the hill-climbing search (shown as empty circles inside the dashed polygon of Fig. 2(a)).</p><ul><li>是因为 优先队列是有限的大小，所以总有点被 没有被比较；但是耗费又不大能提高性能？</li></ul></li><li><p>Based on the principle that “neighbor’s neighbor is likely to be the neighbor,” these unvisited samples are likely to be close neighbors of sample <em>q</em>. Therefore, it is reasonable to insert <em>q</em> and these unvisited samples into the neighborhoods of each other.</p><ul><li>就是说那些 r 因为那些r 本来就很少，如果 将q插入到r 的话 就可以将q与r的其他未访问的给进行连接；</li></ul></li><li><p>After this insertion, it is possible that <em>q</em> is introduced to meet with more unvisited neighbors. As a result, such kind of insertion could be undertaken recursively until no new unvisited neighbors are encountered.</p></li><li><p>In addition, such kind of propagation is restricted to the close neighborhood of a sample. For example, as shown in Fig. 2(b), <em>p</em>1’s neighborhood is propagated only if <em>m</em>(<em>q, p</em>1) is smaller than the distance from <em>p</em>1 to its <em>k</em>-th neighbor.</p></li><li><p>This operation is called <em>restricted recur**sive neighborhood propagation</em> (RRNP). We find this operation further improves the quality of <em>k</em>-NN graph while only inducing minor computational overhead.</p></li><li><p>RRNP的过程如图2所示，并在算法3（第9 - 24行）中展示。在算法3中，W是一个用于待传播样本的工作队列。函数Push(·)在队列末尾插入一个新元素，函数Pop(·)移除并返回队列中的下一个元素。首先，将一个样本r推入W中。之后，满足条件的W中样本的所有邻居都将被推入队列，并且相应地更新k近邻图。这个操作将重复进行，直到队列为空，这意味着没有可用于传播的样本了。</p></li><li><p>The procedure of RRNP is illustrated in Fig. 2 and shown in Alg. 3 (<em>Line 9 – 24</em>). In Alg. 3, <em>W</em> is a working queue for the samples to be propagated. Function <em>Push</em>(<em>·</em>) inserts a new element at the end of the queue and function <em>Pop</em>(<em>·</em>) removes and returns the next element in the queue.</p></li><li><p>Firstly, a sample <em>r</em> is pushed into <em>W</em>. After that, all neighbors of the sample(s) in <em>W</em> that meet the conditions will be pushed into the queue, and the <em>k</em>-NN graph is updated accordingly. This operation will be repeated until the queue is empty, which means that there is no sample available for propagation[传播].</p></li></ul><figure><img src="'+g+'" alt="algorithm3" tabindex="0" loading="lazy"><figcaption>algorithm3</figcaption></figure><ul><li><p>尽管RRNP在概念上与文献[35]中提出的邻域传播相似，但它们在两个方面本质上是不同的。首先，RRNP在传播方面没有优先级。一个邻域中的所有未访问样本以随机顺序与查询进行比较。此外，这种传播被限制在紧密邻域（在样本的k近邻列表半径内）。文献[35]中提出的邻域传播更像是NN-Descent的迷你版本。</p></li><li><p>Although RRNP is conceptually similar to neighborhood propagation proposed in [35], they are essentially different in two aspects. Firstly, there is no priority in terms of propagation in RRNP. All the unvisited samples in one neighborhood are compared with the query in random order. Furthermore, such kind of propagation is restricted to the close neighborhood (within the radius of a sample’s <em>k</em>-NN list). The neighborhood propagation proposed in [35] is more like a mini version of NN-Descent.</p></li><li><p>所以上面这些就是 增加了质量；</p></li></ul><h3 id="c-lazy-graph-diversification" tabindex="-1"><a class="header-anchor" href="#c-lazy-graph-diversification" aria-hidden="true">#</a> <em>C. Lazy Graph Diversification</em></h3><figure><img src="'+f+'" alt="figure3" tabindex="0" loading="lazy"><figcaption>figure3</figcaption></figure><ul><li><p>在算法1中，当扩展样本<em>r</em>时，<em>r</em>邻域中的所有样本都将与查询进行比较。根据最近的研究[7]、[15]、[16]，当<em>r</em>邻域中的样本彼此非常接近时，在扩展过程中无需将它们全部进行比较。对这些接近的样本进行扩展很可能会引导爬山过程到达相同的局部区域。<em>k</em>近邻列表中的样本彼此之间比它们与<em>r</em>之间更接近的现象被称为“遮挡”[7]。遮挡的示意图如图3所示。在该图中，样本<em>b</em>和<em>e</em>被样本<em>a</em>遮挡。很容易看出，一个样本只能被比它更接近<em>r</em>的样本遮挡。根据[7]、[15]、[16]，当在扩展<em>r</em>时不考虑像<em>b</em>和<em>e</em>这样的样本时，爬山过程将更加高效。</p></li><li><p>In Alg. 1, when expanding sample <em>r</em>, all the samples in the neighborhood of <em>r</em> will be compared to the query. According to recent studies [7], [15], [16], when samples in the neighborhood of <em>r</em> are very close to each other, it is no need to compare to all of them during the expansion.</p><ul><li>散化了 但是knng?</li></ul></li><li><p>The expansion on these close samples most likely guides the climbing process to the same local region.</p><ul><li>这个的真正的意思？？ 爬山 相同的区域？是这样</li></ul></li><li><p>The phenomenon that samples in the <em>k</em>-NN list are closer to each other than they are to <em>r</em> is called as “occlusion” [7]. An illustration of occlusion is shown in Fig. 3. In the illustration, samples <em>b</em> and <em>e</em> are occluded by sample <em>a</em>. It is easy to see one sample can only be occluded by samples which are closer to <em>r</em> than that of it. According to [7], [15], [16], the hill-climbing will be more efficient when samples like <em>b</em> and <em>e</em> are not considered when expanding <em>r</em>.</p><ul><li>这里讲述的 散化 也是 为了 search过程中进行的；</li><li>因为 建立knng，也是在search最真实的knn；平常的search也是；所以是需要进行散化的中间过程；</li></ul></li><li><p>为了知道在一个 k 近邻列表中的样本是否相互遮挡，需要对 k 近邻列表中的样本进行两两比较[7]、[15]、[16]。这对于在线构建过程（即算法 2）是不可行的。首先，k 近邻列表是动态变化的，两两距离不能简单地计算并一直保存以供使用。其次，一旦有新样本加入，更新两两遮挡关系的成本太高。这将导致这个新样本与其他样本进行完全比较。此外，与 HNSW 不同，被遮挡的样本不能简单地从 k 近邻列表中删除，因为我们的主要目标是构建尽可能精确的近似 k 近邻图。</p></li><li><p>In order to know whether samples in a <em>k</em>-NN list are occluded by each other, the pair-wise comparisons between samples in the <em>k</em>-NN list are required [7], [15], [16]. This is unfeasible for an online construction procedure (i.e., Alg. 2).</p><ul><li>先思考下 在线的构建： 肯定是有一些的knng的，然后 来加入的点流，那么进行searchknng；建立自己的knn，然后再更新之前的点的knn； 那么 效果图还是一个 knn； 那么search的时候 是在knng上进行的</li></ul></li><li><p>First of all, <em>k</em>-NN lists are dynamically changing, pair-wise distances cannot be simply computed and kept for use all the way. Secondly, it is too costly to update the pair-wise occlusion relations as long as a new sample is joined in. It would induce a complete comparison between this new sample and the rest.</p></li><li><p>Moreover, unlike HNSW the occluded samples cannot be simply removed from a <em>k</em>-NN list since our primary goal is to build an approximate <em>k</em>-NN graph as precise as possible.</p><ul><li>这点；是为了建立一个knng；</li></ul></li><li><p>在本文中，提出了一种名为懒惰图多样化（LGD）的新方案，用于在在线图构建过程中识别样本之间的遮挡。首先，引入一个遮挡因子λ作为附加在 k 近邻列表中每个样本的属性。当新查询的 k 近邻列表加入到图中时，列表中所有样本的λ都初始化为 0。在后续阶段，当另一个新样本加入到这个 k 近邻列表中时，因子λ将被更新。</p></li><li><p>In this paper, a novel scheme called <em>lazy graph diversification</em> (LGD) is proposed to identify the occlusions between samples during the online graph construction.</p></li><li><p>Firstly, an occlusion factor <em>λ</em> is introduced as the attribute attached to each sample in a <em>k</em>-NN list. <em>λ</em>s of all the samples in the list are initialized to <em>0</em> when the <em>k</em>-NN list of a new query is joined into the graph. Factor <em>λ</em> will be updated when another new sample is joined into this <em>k</em>-NN list at the later stages.</p></li><li><p>让我们考虑将一个新样本 <em>q</em> 插入到样本 <em>r</em> 的 <em>k</em> 近邻列表中。为了更新 <em>r</em> 的邻居的 <em>λ</em> 值，我们应该知道所有邻居到 <em>r</em> 的距离以及 <em>q</em> 和列表中其他邻居之间的距离。到 <em>r</em> 的距离已经是已知的。而查询（<em>q</em>）和其他邻居之间的距离是未知的。我们不进行 <em>q</em> 和其他邻居之间的高成本比较，而是利用记录在变量 <em>D</em> 中的距离。即，在最近邻搜索（算法 1）期间 <em>q</em> 和所有已访问样本之间的距离。在 <em>D</em> 的支持下，<em>k</em> 近邻列表中所有样本的遮挡因子 <em>λ</em> 通过以下三个规则进行更新。</p><ul><li>- <strong>规则 1</strong>：对于在 q 之前排序的样本，λ保持不变； - <strong>规则 2</strong>：如果在 q 之前排序的一个样本到 q 的距离比 q 到 r 的距离更近，那么样本 q 的λ值增加 1； - <strong>规则 3</strong>：如果在 q 之后排序的一个样本到 q 的距离小于 q 到 r 的距离，那么该样本的λ值增加 1。</li></ul></li><li><p>Let’s consider a new sample <em>q</em> to be inserted into sample <em>r</em>’s <em>k</em>-NN list. In order to update <em>λ</em>s of <em>r</em>’s neighbors, we should know the distances of all the neighbors to <em>r</em> and the distances between <em>q</em> and other neighbors in the list. The distances to <em>r</em> are already known. While the distances between the query and the rest neighbors are unknown.</p></li><li><p>Instead of performing a costly comparison between <em>q</em> and the rest neighbors, we make use of distances that are recorded in variable <em>D</em>. Namely, the distances between <em>q</em> and all the visited samples during the NN search (Alg. 1). With the support of <em>D</em>, occlusion factor <em>λ</em> of all the samples in the <em>k</em>-NN list is updated with following three rules.</p></li><li><p><strong>Rule 1</strong>: <em>λ</em> is kept unchanged for samples ranked before <em>q</em>;</p></li><li><p><strong>Rule 2</strong>: <em>λ</em> of sample <em>q</em> is incremented by <em>1</em> if a sample ranked before <em>q</em> is closer to <em>q</em> than <em>q</em> to <em>r</em> ;</p></li><li><p><strong>Rule 3</strong>: <em>λ</em> of a sample ranked after <em>q</em> is incremented by <em>1</em> if its distance to <em>q</em> is smaller than <em>q</em> to <em>r</em>.</p></li><li><p>每个样本到 q 的默认距离设置为无穷大。未被访问的邻居的λ值不会根据规则 1 和规则 3 进行更新。这是合理的，因为未被访问的邻居应该离 q 足够远，否则根据“邻居的邻居也可能是邻居”的原则，它们已经被访问过了。由于只有在爬山算法收敛后我们才能得到所有可能的距离（q 与图中的样本之间的距离），所以将 q 插入到 r 的 k 近邻列表中以及更新列表中的因子λ的操作被推迟到最近邻搜索结束时。图 4 展示了由最近邻搜索形成的一条路径。在 r 的 k 近邻区域中，应用了 LGD 操作。</p></li><li><p>The default distance of each sample to <em>q</em> is set to <em>∞</em>. The <em>λ</em>s of not-being-visited neighbors are not updated according to <em>Rule</em> <em>1</em> and <em>Rule 3</em>. This is reasonable because the not-being-visited neighbors should be sufficiently far away from <em>q</em>, otherwise they</p></li></ul><p>are already being visited according to the principle “a neighbor of a neighbor is also likely to be a neighbor”.</p><ul><li>Since we have all the possible distances (between <em>q</em> and samples in the graph) only after the hill-climbing converges, the operations of inserting <em>q</em> into <em>k</em>-NN list of <em>r</em> and updating factor <em>λ</em>s in the list are postponed to the end of NN search. Fig. 4 illustrates a trail that is formed by the NN search. In the <em>k</em>-nearest neighborhood of <em>r</em>, the LGD operations are applied.</li></ul><figure><img src="'+y+'" alt="figure4" tabindex="0" loading="lazy"><figcaption>figure4</figcaption></figure><ul><li><p>我们的方法与[15]不同，图的多样化是以一种懒惰的方式进行的，即在 k 近邻列表中不涉及详尽的比较。因此，这个方案被称为懒惰图多样化（LGD）。用于计算遮挡因子的三个规则被称为 LGD 规则。我们的方法也与[7]、[16]中提出的方法不同，在[7]、[16]中，被遮挡的样本（λ&gt;0）被简单地忽略。在我们的情况下，这是不可行的，因为它偏离了 k 近邻图构建的目标。相反，遮挡因子λ作为遮挡程度的指示器。如果一个样本的λ高于平均水平λ，它就被视为被遮挡。在快速最近邻搜索期间，这种样本将不会被访问，这将在第五节中详细阐述。算法 3 中第 25 行的函数 ApplyLGD(·)负责在插入一个样本时实现 LGD 规则。</p></li><li><p>Our approach is different from [15], the graph diversification is undertaken in a lazy way in the sense no exhaustive comparison is involved within the <em>k</em>-NN list. This scheme is therefore called <em>lazy graph diversification</em> (LGD).</p></li><li><p>The three rules used to calculate the occlusion factor are called as <em>LGD rules</em>. Our approach is also different from the way proposed in [7], [16], in which the occluded samples (<em>λ</em> <em>&gt;</em> 0) are simply omitted. This is infeasible in our case as it deviates from the goal of <em>k</em>-NN graph construction.</p></li><li><p>Alternatively, the occlusion factor <em>λ</em> works as an indicator of the degree of occlusion. If one sample’s <em>λ</em> is above the average level <em>λ</em>, it is viewed as being occluded. Such kinds of samples will not be visited during the fast NN search, which will be elaborated in Section V. Function <em>ApplyLGD(<strong>·</strong>)</em> in Alg. 3, <em>Line 25</em> is responsible to fulfill <em>LGD rules</em> as one sample is inserted.</p></li></ul><h3 id="d-sample-removal-from-k-nn-graph" tabindex="-1"><a class="header-anchor" href="#d-sample-removal-from-k-nn-graph" aria-hidden="true">#</a> <em>D. Sample Removal From k-NN Graph</em></h3>',13),C=e("ul",null,[e("li",null,[e("p",null,"在实际应用中，我们应该允许样本从 k 近邻图中被剔除。一个很好的用例是为一个电子购物网站的产品照片维护一个 k 近邻图，在这种情况下，过时的产品应该下架。我们的方法支持从 k 近邻图中动态删除样本。如果图是由算法 2 构建的，删除操作就像从其反向邻居的 k 近邻列表中删除该样本并释放其自己的 k 近邻列表一样简单。如果图是由算法 3 构建的，在删除样本之前，必须更新位于同一 k 近邻列表中的样本的遮挡因子。幸运的是，并非列表中的所有样本都需要考虑。根据 LGD 规则 3，只需要考虑排在这个样本之后的样本。更新操作平均涉及 k²/2 次距离计算。由于 k 是一个小常数，时间成本比在图上执行一次查询要低得多。")]),e("li",null,[e("p",null,[s("In practice, we should allow samples to be dropped out from the "),e("em",null,"k"),s("-NN graph. A good use case is to maintain a "),e("em",null,"k"),s("-NN graph for product photos for an e-shopping website, where old-fashioned products should be withdrawn from sale.")])]),e("li",null,[e("p",null,[s("The removal of samples dynamically from the "),e("em",null,"k"),s("-NN graph is supported in our approach. If the graph is built by Alg. 2, the removal operation is as easy as deleting the sample from the "),e("em",null,"k"),s("-NN lists of its reverse neighbors and releasing its own "),e("em",null,"k"),s("-NN list.")])]),e("li",null,[e("p",null,[s("If the graph is built by Alg. 3, before the sample is deleted, the occlusion factors of the samples living in the same "),e("em",null,"k"),s("-NN list have to be updated. Fortunately, not all the samples in the list should be considered. According to "),e("em",null,"LGD Rule 3"),s(", only samples ranked after this sample should be considered. The update operations involve "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"k"),e("mn",null,"2")]),e("mi",{mathvariant:"normal"},"/"),e("mn",null,"2")]),e("annotation",{encoding:"application/x-tex"},"k^2 /2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"2")])])])])])])]),e("span",{class:"mord"},"/2")])])]),s(" times distance computations on average. Given "),e("em",null,"k"),s(" is a small constant, the time cost is much lower than fulfilling a query on the graph.")])]),e("li",null,[e("p",null,"其他的 k 近邻图构建方法[32]、[32]、[36]或其他基于图的最近邻搜索索引结构[15]、[16]不能方便地支持动态样本删除。在 HNSW（Hierarchical Navigable Small World）[16]中，样本删除操作可能会导致索引结构崩溃。而在[15]中，这个问题甚至没有被考虑到。")]),e("li",null,[e("p",null,[s("Dynamic sample removal is not conveniently supported by other "),e("em",null,"k"),s("-NN graph construction approaches [32], [32], [36] or other graph-based NN search indexing structures [15], [16].")])]),e("li",null,[e("p",null,"In HNSW [16], the sample removal operation may lead to the collapse of the indexing structure. While this issue is not even considered in [15].")])],-1),P=e("h3",{id:"e-complexity-and-optimality-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#e-complexity-and-optimality-analysis","aria-hidden":"true"},"#"),s(),e("em",null,"E. Complexity and Optimality Analysis")],-1),E=e("ul",null,[e("li",null,[e("p",null,"对于 OLGraph（算法 2）和 OLGraph+（算法 3），邻接表结构是支持搜索和动态更新所必需的。必须为 k 个近邻和反向近邻的 ID、遮挡因子以及距离分配内存。因此，索引的内存消耗上限约为 20·k·n 字节，其中 n 是数据集的规模。由于该结构是近邻和反向近邻的并集，实际的内存消耗将远低于这个上限。")]),e("li",null,[e("p",null,"For both OLGraph (Alg. 2) and OLGraph+ (Alg. 3), the adjacency list structure is required to support the search and dynamic update.")]),e("li",null,[e("p",null,[s("Memory for IDs, occlusion factors, and distances of "),e("em",null,"k"),s(" neighbors and reverse neighbors must be allocated. As a result, the upper bound of memory consumption of index is around "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mn",null,"20"),e("mo",{separator:"true"},"⋅"),e("mi",null,"n"),e("mo",{separator:"true"},"⋅"),e("mi",null,"k")]),e("annotation",{encoding:"application/x-tex"},"20·n·k")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord"},"20"),e("span",{class:"mpunct"},"⋅"),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mpunct"},"⋅"),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k")])])]),s(" Bytes, where "),e("em",null,"n"),s(" is the scale of the dataset. Since the structure is the union of neighbors and reverse neighbors, the real memory consumption will be much lower than this bound.")])]),e("li",null,[e("p",null,"从本质上讲，NN-Descent、OLGraph 和 OLGraph+通过 k 近邻图提供的路径来避免穷举比较。当路径能快速引导查询找到其真正的近邻时，该路径是有效的。这在很大程度上取决于内在的数据维度[4]，而数据维度因数据集的不同而有所变化。")]),e("li",null,[e("p",null,[s("Essentially NN-Descent, OLGraph, and OLGraph+ avoid exhaustive comparison via the routings supplied by the "),e("em",null,"k"),s("-NN graph. The routing is effective when it guides the query quickly to its true neighbors. This is largely determined by the intrinsic data dimension [4], which varies from one dataset to another.")])]),e("li",null,[e("p",null,"为了研究 OLGraph+的时间复杂度，对四个具有不同数据维度和规模的真实世界数据集进行了一项实证研究。在每个数据集上，我们研究了在不同数据规模下构建 k 近邻图所需的平均比较次数。如图 5 所示，每个数据集所需的平均比较次数至少比数据集的大小低一个数量级。这基本上表明，当数据规模在百万级别时，OLGraph+的时间复杂度在[n1.5,n1.9]范围内。对于内在维度较低的数据，如 SIFT 和 YFCC，OLGraph+的时间复杂度更低。对于 GloVe 数据集，其时间复杂度接近 O(n1.66)，因为它的内在维度高达 39.5[37]。然而，与穷举法相比，OLGraph+在扫描率方面节省了 98%的比较次数，这在后面的实验中将会说明，因此仍然非常高效。与 NN-Descent 相比，OLGraph+避免了潜在的重复比较，因为每个样本对于图中已有的样本来说都是新的。此外，由于局部图差异（LGD），OLGraph+跳过了对邻域中被遮挡样本的比较。总的来说，它更高效。")]),e("li",null,[e("p",null,[s("In order to investigate the time complexity of OLGraph+, an empirical study is conducted on four real-world datasets with varying data dimensions and scales. On each dataset, we investigate the average number of comparisons required to construct "),e("em",null,"k"),s("-NN graphs in different data scales.")])]),e("li",null,[e("p",null,[s("As shown in Fig. 5, the number of average comparisons required for each dataset is at least one order of magnitude lower than the size of the dataset. This basically indicates that the time complexity of OLGraph+ is in the range of "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mo",{stretchy:"false"},"["),e("msup",null,[e("mi",null,"n"),e("mn",null,"1.5")]),e("mo",{separator:"true"},","),e("msup",null,[e("mi",null,"n"),e("mn",null,"1.9")]),e("mo",{stretchy:"false"},"]")]),e("annotation",{encoding:"application/x-tex"},"[n^{1.5},n^{1.9}]")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1.0641em","vertical-align":"-0.25em"}}),e("span",{class:"mopen"},"["),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.5")])])])])])])])]),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.9")])])])])])])])]),e("span",{class:"mclose"},"]")])])]),s(" when the data scale is on the million level. The time complexity of OLGraph+ is lower on data with lower intrinsic dimensions, e.g. SIFT and YFCC.")])]),e("li",null,[e("p",null,[s("For the dataset GloVe, its time complexity is close to "),e("em",null,"O"),s("("),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"n"),e("mn",null,"1.66")])]),e("annotation",{encoding:"application/x-tex"},"n^{1.66}")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8141em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"n"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8141em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},[e("span",{class:"mord mtight"},"1.66")])])])])])])])])])])]),s(") as its intrinsic dimension is as high as "),e("em",null,"39.5"),s(" [37]. However, compared to the exhaustive approach, OLGraph+ is still very efficient in the sense it saves up "),e("em",null,"98%"),s(" comparisons in terms of its scanning rate, which will be illustrated in the later experiments. Compared to NN-Descent, OLGraph+ avoids potential repetitive comparisons as each sample is new to the samples already in the graph. In addition, OLGraph+ skips the comparison to the occluded samples in the neighborhood due to LGD. In general, it is more efficient.")])])],-1),B=e("figure",null,[e("img",{src:k,alt:"figure5",tabindex:"0",loading:"lazy"}),e("figcaption",null,"figure5")],-1),j=e("ul",null,[e("li",null,[e("p",null,"在 OLGraph（算法 2）和 OLGraph+（算法 3）中，构建都是从一个小规模的 100%质量的 k 近邻图开始。搜索过程每次将一个新样本的 k 近邻列表添加到图中。同时，当新样本恰好位于已插入样本的邻域中时，已插入样本的 k 近邻列表可能会被更新。因此，这对于图的构建和最近邻搜索来说是一个双赢的局面。有效的搜索过程会返回高质量的 k 近邻列表。而高质量的 k 近邻图会为爬山过程提供良好的指导。")]),e("li",null,[e("p",null,[s("In both OLGraph (Alg. 2) and OLGraph+ (Alg. 3), the construction starts from a small-scale "),e("em",null,"k"),s("-NN graph of "),e("em",null,"100%"),s(" quality. The search process appends a "),e("em",null,"k"),s("-NN list of a new sample to the graph each time. Meanwhile, the "),e("em",null,"k"),s("-NN lists of the already inserted samples will be possibly updated when the new sample happens to be in their neighborhoods.")])]),e("li",null,[e("p",null,[s("It is, therefore, a win-win situation for both graph construction and NN-search. Effective search procedure returns high-quality "),e("em",null,"k"),s("-NN list. While high-quality "),e("em",null,"k"),s("-NN graph gives a good guidance for the hill-climbing process.")])]),e("li",null,[e("p",null,"除了近邻列表大小 k 之外，OLGraph 和 OLGraph+中还涉及另一个参数，即种子数量 p。通常，近邻列表大小 k 不应小于内在数据维度 d∗[37]，而内在数据维度 d∗小于或等于数据维度 d。种子数量通常设置为不大于 k。当 d 非常高（即几百到几千）且 d∗接近 d 时，如果将 k 设置为接近 d，那么构建过程可能会很慢。在这种情况下，必须在 k 近邻图的质量和构建效率之间进行权衡。")]),e("li",null,[e("p",null,[s("Besides the size of NN list "),e("em",null,"k"),s(", there is another parameter in volved in OLGraph and OLGraph+. Namely, the number of seeds "),e("em",null,"p"),s(". Usually, the size of NN list "),e("em",null,"k"),s(" should be no less than the intrinsic data dimension "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"d"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"d∗")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord"},"∗")])])]),s(" [37], which is less than or equal to the data dimension "),e("em",null,"d"),s(". The number of seeds is usually set to be no bigger than "),e("em",null,"k"),s(". When "),e("em",null,"d"),s(" is very high (i.e., several hundred to thousand) and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"d"),e("mo",null,"∗")]),e("annotation",{encoding:"application/x-tex"},"d∗")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord"},"∗")])])]),s(" is close to "),e("em",null,"d"),s(", the construction process could be slow when "),e("em",null,"k"),s(" is set to be close to "),e("em",null,"d"),s(". In such a situation, a trade-off has to be made between the quality of "),e("em",null,"k"),s("-NN graph and the efficiency of the construction.")])])],-1),K=e("h2",{id:"这个no-5-ok",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#这个no-5-ok","aria-hidden":"true"},"#"),s(" 这个No.5 == OK")],-1),Q=e("h2",{id:"no-5-fast-nn-search-on-the-diversified-graph",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#no-5-fast-nn-search-on-the-diversified-graph","aria-hidden":"true"},"#"),s(" No.5 FAST NN-SEARCH ON THE DIVERSIFIED GRAPH")],-1),V=e("ul",null,[e("li",null,[e("p",null,"在局部图差异（LGD）规则下，当一个 k 近邻列表及其对应的反向 k 近邻列表中的样本的λ值高于该 k 近邻列表的λ值时，这些样本被认为是被遮挡的，其中λ是列表的平均遮挡因子。这基本上表明列表中的这些样本与其他样本太接近，在爬山过程中无需考虑它们。只要有样本被插入或从列表中移除，一个列表中所有样本的因子就会动态更新。随着新样本加入列表，之前被遮挡的成员可能会变得“可见”。这是我们的方法与 HNSW[16]的本质区别，在 HNSW 中，一旦被遮挡的样本被识别出来，就会被永久移除。")]),e("li",null,[e("p",null,[s("Under "),e("em",null,"LGD rules"),s(", the samples in one "),e("em",null,"k"),s("-NN list and the corresponding reverse "),e("em",null,"k"),s("-NN list are considered as being occluded when their "),e("em",null,"λ"),s(" is higher than "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"λ"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overlineλ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8944em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8944em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"λ")])]),e("span",{style:{top:"-3.8144em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" of the "),e("em",null,"k"),s("-NN list, where "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"λ"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overlineλ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8944em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8944em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"λ")])]),e("span",{style:{top:"-3.8144em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" is the average occlusion factor of the list.")])]),e("li",null,[e("p",null,"This basically indicates these samples in the list are too close to other samples that they are no need to be considered during the hill-climbing.")]),e("li",null,[e("p",null,"The factors of all the samples in one list are updated dynamically as long as samples are inserted/removed from the list. As the new samples are joined in the list, members that are previously occluded may become “visible”. This is the essential difference between our approach and HNSW [16], in which occluded samples are removed permanently once it is identified.")]),e("li",null,[e("p",null,"一旦有了遮挡因子，搜索算法（算法 1）就会相应地进行修改。当查询与 r 的 k 近邻列表中的邻居进行比较时，我们只考虑那些λ值不大于该列表平均遮挡因子λ的样本，其中λ是 k 近邻列表的平均遮挡因子。在多样化的 k 近邻图上进行最近邻搜索与算法 1 大致相似。在修改后的最近邻搜索中，添加了一个条件判断语句（如算法 1 中的第 18 行）来检查一个样本的遮挡因子λ是否高于λ。只有“可见”的样本才会加入到比较中。由于在搜索过程中一个 k 近邻列表中近 50%的样本被跳过，所以有望实现加速。")]),e("li",null,[e("p",null,[s("Once the occlusion factor is available, the search algorithm (Alg. 1) is accordingly modified. When the query is compared to the neighbors in "),e("em",null,"r"),s("’s "),e("em",null,"k"),s("-NN list, we only consider the samples whose "),e("em",null,"λ"),s(" is no greater than the average occlusion factor "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"λ"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overlineλ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8944em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8944em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"λ")])]),e("span",{style:{top:"-3.8144em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" of this list, where "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"λ"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overlineλ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8944em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8944em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"λ")])]),e("span",{style:{top:"-3.8144em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" is the average occlusion factor of the "),e("em",null,"k"),s("-NN list.")])]),e("li",null,[e("p",null,[s("The NN search on the diversified "),e("em",null,"k"),s("-NN graph is largely similar as Alg. 1. In the modified NN search, a conditional judgment statement (like "),e("em",null,"Line 18"),s(" in Alg. 1) is added to check whether the occlusion factor "),e("em",null,"λ"),s(" of a sample is above "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mover",{accent:"true"},[e("mi",null,"λ"),e("mo",{stretchy:"true"},"‾")])]),e("annotation",{encoding:"application/x-tex"},"\\overlineλ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8944em"}}),e("span",{class:"mord overline"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8944em"}},[e("span",{style:{top:"-3em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal"},"λ")])]),e("span",{style:{top:"-3.8144em"}},[e("span",{class:"pstrut",style:{height:"3em"}}),e("span",{class:"overline-line",style:{"border-bottom-width":"0.04em"}})])])])])])])])]),s(" Only the “visible samples are joined in the comparison. Speed-up is expected as nearly "),e("em",null,"50%"),s(" of samples in one "),e("em",null,"k"),s("-NN list are skipped during the search.")]),e("ul",null,[e("li",null,"这里是说 散化的 knng的 search是有一部分不一样的；"),e("li",null,"因为目的是knng，所以部分的邻居是没有用的；但是还是要存着；")])]),e("li",null,[e("p",null,"虽然在局部图差异（LGD）图上进行最近邻搜索比算法 1 高效得多，但它不适合在算法 3 的 k 近邻图构建中采用。在算法 1 中进行比较时考虑的近邻在这个修改后的最近邻搜索中会被忽略。因此，应该相互加入到 k 近邻列表中的样本根本没有被访问到。出于这个原因，当想要将一个样本添加到图中时，算法 3 中建议使用 NNSearch(·)（算法 1）。或者，在进行纯粹的最近邻搜索时，建议使用带有 LGD 检查的最近邻搜索。在实验部分，我们将展示 LGD 策略在最近邻搜索中带来了显著的加速。与最先进的方法相比，它变得具有竞争力。")]),e("li",null,[e("p",null,[s("Although NN search on LGD graph is significantly more efficient than Alg. 1, it is not suitable to be adopted in the "),e("em",null,"k"),s("-NN graph construction in Alg. 3. The close neighbors that are considered in the comparison in Alg. 1 will be ignored in this modified NN search.")])]),e("li",null,[e("p",null,[s("As a consequence, the samples should be joined in "),e("em",null,"k"),s("-NN lists of each other are simply unvisited. For this reason, "),e("em",null,"NNSearch"),s("("),e("em",null,"·"),s(") (Alg. 1) in Alg. 3 is recommended when one wants to add a sample to the graph.")])]),e("li",null,[e("p",null,"Alternatively, NN search with LGD check is recommended as one performs pure NN search. In the experiment section, we will show that the LGD strategy leads to considerable speed-up in the NN search. It becomes competitive in comparison to the state-of-the-art approaches.")]),e("li",null,[e("p",null,"为此，可以想象在我们的在线 k 近邻图框架中有两种模式，即构建模式和纯最近邻搜索模式。在纯最近邻搜索模式下，由于在该过程中不调用 k 近邻列表更新，所以不会调用 ApplyLGD(·)。在构建模式下，调用算法 3。")]),e("li",null,[e("p",null,[s("To this end, one could imagine that there are two modes in our online "),e("em",null,"k"),s("-NN graph framework, namely the construction mode and pure NN search mode. Under the pure NN search mode, "),e("em",null,[s("ApplyLGD("),e("strong",null,"·"),s(")")]),s(" is not called since no "),e("em",null,"k"),s("-NN list update is invoked during the procedure. Under the construction mode, Alg. 3 is invoked.")])]),e("li",null,[e("p",null,"快速的在线 k 近邻图构建和最近邻搜索已被集成到一个框架中。与现有的 k 近邻图构建方法[4]、[32]、[36]相比，快速动态插入、删除以及在 k 近邻图上的最近邻搜索都得到了很好的支持。与其他基于图的最近邻索引结构（如 HNSW[16]和 DPG[15]）相比，没有离线构建阶段。因此，动态维护索引结构的成本比 HNSW[16]或 DPG[15]低得多。此外，与 HNSW 和 DPG 相比，多样化的 k 近邻图和 k 近邻图在一个结构中同时被维护。一方面，它保证了快速的最近邻搜索。另一方面，它允许用户通过 k 个最近邻之间的链接浏览相似的内容。")]),e("li",null,[e("p",null,[s("The fast online "),e("em",null,"k"),s("-NN graph construction and NN search have been integrated into one framework. Compared to the existing "),e("em",null,"k"),s("-NN graph construction approaches [4], [32], [36], fast dynamic insertion, removal as well as NN search on a "),e("em",null,"k"),s("-NN graph are all well supported.")])]),e("li",null,[e("p",null,"Compared to other graph-based NN indexing structures such as HNSW [16] and DPG [15], there is no offline construction stage. As a result, the cost of dynamically maintaining the indexing structure is much lower than either HNSW [16] or DPG [15].")]),e("li",null,[e("p",null,[s("Furthermore, compared to HNSW and DPG, a diversified "),e("em",null,"k"),s("-NN graph and a "),e("em",null,"k"),s("-NN graph are maintained simultaneously in one structure. On the one hand, it guarantees fast NN search. On the other hand, it allows the user to browse over similar contents via the links between "),e("em",null,"k"),s("-nearest neighbors.")])])],-1),U=n('<h2 id="vi-experiments" tabindex="-1"><a class="header-anchor" href="#vi-experiments" aria-hidden="true">#</a> VI. EXPERIMENTS</h2><h2 id="该这个了" tabindex="-1"><a class="header-anchor" href="#该这个了" aria-hidden="true">#</a> 该这个了</h2><ul><li><p>在本节中，对所提出的算法作为近似 k 近邻图构建和最近邻搜索方法的性能进行了研究。在评估中，在流行的评估数据集上报告性能。数据集的简要信息总结在表 I 中。特别地，YFCC1M 的深度特征是从 ResNet-50 的倒数第二层提取的。这些特征通过主成分分析进一步降维到 128 维。</p></li><li><p>In this section, the performance of the proposed algorithms is studied both as an approximate <em>k</em>-NN graph construction and a nearest neighbor search approach. In the evaluation, the performance is reported on popular evaluation datasets. The brief information about the datasets is summarized in Table I. In particular, the deep features for YFCC1M are extracted from the <em>2nd</em> last layer of ResNet-50. The features are further reduced to <em>128</em> dimension via PCA.</p></li></ul><figure><img src="'+N+'" alt="table1" tabindex="0" loading="lazy"><figcaption>table1</figcaption></figure><ul><li><p>在近似 k 近邻图构建任务中，在性能比较中考虑了现有的方法 NN-Descent[4]和 NSW[32]，这两种方法对于各种距离度量都是可行的。在最近邻搜索任务中，与不同类别的代表性方法进行比较，研究了所提出的搜索方法的性能。即，它们是基于图的方法，如 NN-Descent[4]、DPG[15]和 HNSW[16]。考虑了具有代表性的局部敏感哈希方法 SRS[38]。对于基于量化的方法，在比较中纳入了乘积量化器（PQ）[22]和双比特量化（DBQ）[30]。选择 FLANN[19]和 Annoy[39]作为具有代表性的树划分方法。它们都是文献中流行的最近邻搜索库。</p></li><li><p>On the approximate <em>k</em>-NN graph construction task, existing approaches NN-Descent [4] and NSW [32] are considered in the performance comparison, both of which are feasible for various distance metrics.</p></li><li><p>On the nearest neighbor search task, the performance of the proposed search approach is studied in comparison to the representative approaches of different categories. Namely, they are graph-based approaches such as NN-Descent [4], DPG [15] and HNSW [16]. The representative locality-sensitive hash approach SRS [38] is considered. For quantization based approach, product quantizer (PQ) [22] and double-bit quantization (DBQ) [30] are incorporated in the comparison. FLANN [19] and Annoy [39] are selected as the representative tree partitioning approaches. Both of them are popular NN search libraries in the literature.</p></li></ul><h3 id="a-evaluation-protocol" tabindex="-1"><a class="header-anchor" href="#a-evaluation-protocol" aria-hidden="true">#</a> <em>A. Evaluation Protocol</em></h3>',6),X=e("ul",null,[e("li",null,[e("p",null,[s("Eight real-world datasets are adopted to evaluate the performance of both "),e("em",null,"k"),s("-NN graph construction and nearest neighbor search. These datasets are derived from real-world images, text data, or itemset. The top-"),e("em",null,"1"),s(" ("),e("em",null,"recall@1"),s(") and top-"),e("em",null,"10"),s(" ("),e("em",null,"recall@10"),s(") recalls on each dataset are studied under different metrics such as "),e("em",null,"l"),s("2, "),e("em",null,"Cosine"),s(", "),e("em",null,"Jaccard"),s(" and "),e("em",null,"κ"),s("2. Given function "),e("em",null,"R"),s("("),e("em",null,"i, k"),s(")returns the number of truth-positive neighbors at top-"),e("em",null,"k"),s(" NN list of sample "),e("em",null,"i"),s(", the recall at top-"),e("em",null,"k"),s(" on the whole set is given as:")]),e("ul",null,[e("li",null,"Recall 的公示")])]),e("li",null,[e("p",null,[s("Besides "),e("em",null,"k"),s("-NN graph quality, the construction cost is also studied by measuring the scanning rate [4] and time cost of each approach. Given "),e("em",null,"C"),s(" is the total number of distance computations in the construction, the scanning rate is defined as：")]),e("ul",null,[e("li",null,[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("mi",null,"C"),e("mo",null,"="),e("mi",null,"C"),e("mi",{mathvariant:"normal"},"/"),e("mi",null,"n"),e("mo",{stretchy:"false"},"("),e("mi",null,"n"),e("mo",null,"−"),e("mn",null,"1"),e("mo",{stretchy:"false"},")"),e("mi",{mathvariant:"normal"},"/"),e("mn",null,"2")]),e("annotation",{encoding:"application/x-tex"},"C = C /n(n-1)/2")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6833em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),e("span",{class:"mord"},"/"),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),e("span",{class:"mbin"},"−"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord"},"1"),e("span",{class:"mclose"},")"),e("span",{class:"mord"},"/2")])])])])])]),e("li")],-1);function Y(J,Z){const a=r("router-link");return o(),h("div",null,[v,w,m(" more "),x,e("nav",G,[e("ul",null,[e("li",null,[t(a,{to:"#no-0-摘要"},{default:l(()=>[s("No.0 摘要")]),_:1})]),e("li",null,[t(a,{to:"#这个no-1-ok"},{default:l(()=>[s("这个No.1 == OK")]),_:1})]),e("li",null,[t(a,{to:"#no-1-introduction"},{default:l(()=>[s("No.1 INTRODUCTION")]),_:1})]),e("li",null,[t(a,{to:"#这个no-2-ok"},{default:l(()=>[s("这个No.2 == OK")]),_:1})]),e("li",null,[t(a,{to:"#no-2-related-works"},{default:l(()=>[s("No.2 RELATED WORKS")]),_:1}),e("ul",null,[e("li",null,[t(a,{to:"#a-k-nn-search"},{default:l(()=>[s("A. k-NN Search")]),_:1})]),e("li",null,[t(a,{to:"#b-approximate-k-nn-graph-construction"},{default:l(()=>[s("B. Approximate k-NN Graph Construction")]),_:1})])])]),e("li",null,[t(a,{to:"#这个no-3-ok"},{default:l(()=>[s("这个No.3 == OK")]),_:1})]),e("li",null,[t(a,{to:"#no-3-nn-search-on-the-k-nn-graph"},{default:l(()=>[s("No.3 NN SEARCH ON THE K-NN GRAPH")]),_:1})]),e("li",null,[t(a,{to:"#这个no-4-ok"},{default:l(()=>[s("这个No.4 == OK")]),_:1})]),e("li",null,[t(a,{to:"#no-4-online-approximate-k-nn-graph-construction"},{default:l(()=>[s("No.4 ONLINE APPROXIMATE K-NN GRAPH CONSTRUCTION")]),_:1}),e("ul",null,[e("li",null,[t(a,{to:"#a-approximate-k-nn-graph-construction-by-search"},{default:l(()=>[s("A. Approximate k-NN Graph Construction by Search")]),_:1})]),e("li",null,[t(a,{to:"#b-restricted-recursive-neighborhood-propagation"},{default:l(()=>[s("B. Restricted Recursive Neighborhood Propagation")]),_:1})]),e("li",null,[t(a,{to:"#c-lazy-graph-diversification"},{default:l(()=>[s("C. Lazy Graph Diversification")]),_:1})]),e("li",null,[t(a,{to:"#d-sample-removal-from-k-nn-graph"},{default:l(()=>[s("D. Sample Removal From k-NN Graph")]),_:1})]),e("li",null,[t(a,{to:"#e-complexity-and-optimality-analysis"},{default:l(()=>[s("E. Complexity and Optimality Analysis")]),_:1})])])]),e("li",null,[t(a,{to:"#这个no-5-ok"},{default:l(()=>[s("这个No.5 == OK")]),_:1})]),e("li",null,[t(a,{to:"#no-5-fast-nn-search-on-the-diversified-graph"},{default:l(()=>[s("No.5 FAST NN-SEARCH ON THE DIVERSIFIED GRAPH")]),_:1})]),e("li",null,[t(a,{to:"#vi-experiments"},{default:l(()=>[s("VI. EXPERIMENTS")]),_:1})]),e("li",null,[t(a,{to:"#该这个了"},{default:l(()=>[s("该这个了")]),_:1}),e("ul",null,[e("li",null,[t(a,{to:"#a-evaluation-protocol"},{default:l(()=>[s("A. Evaluation Protocol")]),_:1})])])])])]),q,M,A,_,T,L,I,O,S,R,D,z,F,W,H,C,P,E,B,j,K,Q,V,U,X])}const se=i(b,[["render",Y],["__file","OnlineKNNG.html.vue"]]);export{se as default};
