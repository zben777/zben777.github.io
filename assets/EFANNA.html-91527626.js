import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as l,o as p,c as h,d as c,a as e,b as i,e as n,w as t,f as s}from"./app-2a2d189a.js";const d="/assets/Algorithm1-b19036c0.png",u="/assets/table4-1524ed6f.png",m="/assets/Algorithm3-b645b88b.png",g="/assets/figure1-3ee5bcd0.png",N="/assets/table1-b7ea0e00.png",f="/assets/figure2-f64faadb.png",b="/assets/figure3-7f1dee84.png",v="/assets/table2-36ce86dd.png",k="/assets/table3-c458bee4.png",w="/assets/figure4-0c556a3e.png",A="/assets/figure5-60bf74ae.png",x="/assets/figure6-6a660830.png",y="/assets/figure7-8c534785.png",E="/assets/table5-c52fd98a.png",F="/assets/figure8-6de577b3.png",_="/assets/figure9-92ee4569.png",T="/assets/table6-8933d41e.png",S="/assets/figure10-d9db73d5.png",I="/assets/figure11-7b45e0ed.png",z="/assets/table7-83ce98f0.png",q="/assets/table8-82906715.png",G="/assets/figure12-dc54f0af.png",K="/assets/figure13-1be9ac76.png",D={},R=e("h1",{id:"h-efanna",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#h-efanna","aria-hidden":"true"},"#"),i(" H-EFANNA")],-1),M=e("p",null,"H-EFANNA",-1),H={class:"hint-container info"},L=e("p",{class:"hint-container-title"},"相关信息",-1),C=e("li",null,[e("p",null,"可能这里要有一些 关于 其它的对于这篇文章的解读")],-1),W=e("li",null,[e("p",null,"我们需要一些链接？")],-1),O={href:"https://whenever5225.github.io/2019/09/21/efanna1/",target:"_blank",rel:"noopener noreferrer"},B={href:"https://github.com/ZJULearning/efanna",target:"_blank",rel:"noopener noreferrer"},P=s("<li><p>主要思想：将NN-descent的初始化更改为 随机截断树；</p></li><li><p>NN-expansion : a neighbor of a neighbor is also likely to be a neighbor.(converge to local optima)</p></li><li><p>然后就是EFANNA算法 就是 基于这样一个观察， NN-expansion进行搜索 和NN-Descent进行建图 都是对 初始化 比较敏感的；</p><ul><li>在离线构建阶段，EFANNA算法 通过 构造 随机截断kd树进行给 进行初始化，然后通过 NN-Descent进行 不断地细化KNNG；</li><li>在 在线搜索阶段，首先 在 随即截断KD树 找到 较好的候选邻居，然后 不断的进行expansion 通过在KNNG上</li></ul></li><li><p>还有就是EFANNA 的 搜索基本都是建立的 10-NN graph and 16trees;</p></li><li><p>在构图阶段：当维度变高时，KD 树初始化变得不太有效。(KD-tree的本身局限性)</p></li><li><p>不同的Knng的K的大小会导致search性能的变化；</p></li><li><p>当我们搜索 100 近邻时，在 95%召回率水平下，GNNS（随机初始化）的性能优于 EFANNA 和 IEH-ITQ。这实际上是预期的，<br> 因为对于搜索来说良好的初始化需要额外的时间。意思是说在search阶段，如果要返回的100近邻的话 初始化的就很耗费时间了；<br> （当然这个时候的KNNG的K=40）</p></li><li></li><li><p>我们可以发现，随着图的“K”增加，EFANNA 相对于 IEH-ITQ 和 GNNS（kGraph）实现的性能增益变小。这表明良好初始化对最近邻扩展的影响变小。</p></li><li><p>还有就是说 如果 是一个 高K的knng；那么seed的初始化 并不是影响很大了；低K的knng进行初始化的会比较好点；进入点</p></li><li><p>那么这个联想的话可不可以在散化后 进行将 低 出度的 点 进行 与 高出度的 点进行连接？？比如通过遮挡等信息；</p></li><li></li><li><p>还是总的来说就是：EFANNA 对于 高纬度+高KNNG的K+search的高K；</p></li>",13),U={class:"table-of-contents"},Q=s('<h2 id="no-0-摘要" tabindex="-1"><a class="header-anchor" href="#no-0-摘要" aria-hidden="true">#</a> No.0 摘要</h2><ul><li>近似最近邻（ANN）搜索是数据挖掘、机器学习和计算机视觉等许多领域中的一个基本问题。传统基于层次结构(traditional hierarchical structure (tree))的方法的性能随着数据维度的增加而下降，而基于哈希(hashing)的方法在实际中通常缺乏效率。</li><li>最近，基于图的方法引起了相当大的关注。其主要思想是，一个邻居的邻居也很可能是邻居，我们称之为 NN 扩展(<em>NN-expansion</em>.)。这些方法离线(offline)构建一个 k 近邻（kNN）图。</li><li>在在线搜索阶段(at online search stage)，这些方法以某种方式（例如随机选择random selection）找到查询点的候选邻居(find candidate neighbors of a query point)，然后迭代地检查这些候选邻居的邻居以找到更近的邻居。</li><li>尽管取得了一些有希望的结果，但这些方法主要存在两个问题： <ul><li>1）这些方法倾向于收敛(converge)到局部最优解(local optima)。</li><li>2）构建 kNN 图很耗时。</li></ul></li><li>我们发现，当我们为 NN 扩展( NN-expansion)提供良好的初始化(a good initialization)时，这两个问题可以很好地解决。</li><li>在本文中，我们提出了 EFANNA，一种基于 kNN 图的极快近似最近邻搜索算法。EFANNA 很好地结合(combines)了基于层次结构( hierarchical structure)方法和基于最近邻图方法( nearest-neighbor-graph)的优点。</li><li>大量实验表明，EFANNA 在近似最近邻搜索 和近似最近邻图构建方面. 都优于最先进的算法。据我们所知，EFANNA 是迄今为止在近似最近邻图构建和近似最近邻搜索方面最快的算法。基于这项研究的一个库 EFANNA 在 Github 上发布(released)。</li></ul><h2 id="no-1-introduction" tabindex="-1"><a class="header-anchor" href="#no-1-introduction" aria-hidden="true">#</a> No.1 INTRODUCTION</h2><ul><li>最近邻搜索在数据挖掘、机器学习和计算机视觉的许多应用中都起着重要作用。</li><li>在处理稀疏数据(sparse data)（例如文档检索document retrieval）时，可以使用先进的索引结构(index structures)（例如倒排索引IVF:inverted index）来解决这个问题。</li><li>然而，对于具有密集特征(dense features)的数据，找到精确最近邻的成本是 O(N)，其中 N 是数据库中点的数量。当数据集很大时，这非常耗时。所以在实际中人们转向近似最近邻（ANN）搜索[1],[5]。已经做了很多工作来进行具有高精度但低计算复杂度的近似最近邻搜索。</li></ul><br><ul><li><p>在近似最近邻搜索中主要有两种类型的方法。</p></li><li><p>第一类方法是基于层次结构(hierarchical structure(tree))的方法，例如 KD-tree[3],[13]、随机 KD-tree[30]、 K-means tree[15]。当数据的维度相对较低时，这些方法表现得非常好。然而，随着数据维度的增加，其性能会急剧(dramatically)下降[30]。</p></li><li><p>第二类方法是基于哈希(hashing)的方法，例如局部敏感哈希（LSH）[17]、谱哈希(Spectral Hashing)[34]、迭代量化( Iterative Quantization)[36]等等。</p></li><li><p><mark>这一段是讲述Hashing,可以先不看这段落</mark></p></li><li><p>关于各种哈希方法的详细综述请见[33]。这些方法为高维实向量生成二进制代码，同时试图保留原始实向量之间的相似性。因此，所有的实向量都落入不同的哈希桶中（具有不同的二进制代码）。理想情况下，如果相邻向量落入同一个桶或附近的桶中（通过两个二进制代码的汉明距离来衡量），基于哈希的方法可以有效地检索查询点的最近邻。然而，不能保证所有的相邻向量都会落入附近的桶中。为了确保高召回率（返回的点集中真正邻居的数量除以所需邻居的数量），需要检查许多哈希桶（即，在汉明空间中扩大搜索半径），这导致了高计算复杂度。关于详细分析请见[22]。</p></li></ul><br><ul><li><p>最近，基于图的方法引起了相当大的关注[11],[18],[22]。这些方法背后的基本(essential)思想是，一个邻居的邻居也很可能是邻居(*a neighbor of a neighbor is also <em>likely to be a neighbor</em>)，我们称之为 NN 扩展(<em>NN-expansion</em>)。</p><ul><li>所以这篇文章是把 NN-expansion 和 NN-Descent的那个分别区分定义了；</li></ul></li><li><p>这些方法离线(outline)构建一个 k 近邻（kNN）图。在在线(online)搜索阶段，这些方法以某种方式找到查询点的候选邻居( candidate neighbors of a query point)（例如，random selection[11],[18]），然后迭代地检查这些候选邻居的邻居以找到更近的邻居( check the neighbors of these candidate neighbors for closer ones iteratively. )。</p></li><li><p>这种方法的一个问题是NN-expansion很容易收敛到局部最优解( converge to local optima)，导致召回率低(low recall)。</p></li><li><p>[22]试图通过为查询点提供更好的初始化来解决这个问题(providing better initialization for a query point)。</p><ul><li>什么叫 给 查询点 提供更好的初始化？ 是指 进入点更好点？应该是；</li></ul></li><li><p>[22]不是random selection，而是使用基于哈希(hashing)的方法进行初始化。这种方法被命名为迭代扩展哈希( Iterative Expanding Hashing)（IEH），并且比相应的(corresponding)基于哈希的方法取得了显著更好的结果。</p></li></ul><br><ul><li><p>使用基于图的方法的另一个挑战是构建 k 近邻图的高计算成本，特别是当数据库很大时。已经有很多努力致力于降低 k 近邻图构建的时间复杂度。</p></li><li><p>[4],[9],[32],[28]试图加速精确 k 近邻图的构建。然而，在大数据的背景(context)下，这些方法仍然不够高效。最近的研究人员不是构建精确的 k 近邻图，而是尝试高效地构建 <code>近似的 k 近邻图</code>。</p></li><li><p>NN-expansion的思想(<em>NN-expansion</em> idea)再次可以用于构建近似的 k 近邻图。[12]提出了 NN-descent 来高效地构建近似 k 近邻图。也就是说之前的NN-expansion的思想仅仅是应用在了search阶段？？？？？</p></li><li><p>[8],[16],[37],[31]尝试以分治的方式( in a divide-and-conquer manner)构建近似 k 近邻图。他们的算法主要包含三个阶段。</p><ul><li>首先，他们<mark>多次</mark>将整个数据集划分为小的子集。</li><li>其次，他们在子集中进行暴力搜索并得到许多重叠的子图(overlapping subgraphs)。</li><li>最后，他们合并子图(merge the subgraphs)，并使用与 NN-expansion类似的技术来优化refine图。</li></ul></li><li><p>尽管可以高效地构建近似 k 近邻图，但目前还没有关于如果使用近似 k 近邻图而不是精确 k 近邻图，基于图的搜索方法的性能将如何受到影响的正式研究。</p></li></ul><br><ul><li><p>为了解决上述问题，我们在本文中提出了一种新颖的基于图的近似最近邻搜索框架(framework) EFANNA。EFANNA 是两个算法的缩写(abbreviation)：</p><ul><li>极速近似 k 近邻图构建算法(<strong>E</strong>xtremely <strong>F</strong>ast <strong>A</strong>pproximate <em>k</em>-<strong>N</strong>earest <strong>N</strong>eighbor graph construction <strong>A</strong>lgorithm</li><li>基于 kNN 图的极速近似最近邻搜索算法(<strong>E</strong>xtremely <strong>F</strong>ast <strong>A</strong>pproximate <strong>N</strong>earest <strong>N</strong>eighbor search <strong>A</strong>lgorithm based on <em>k</em>NN graph)</li></ul></li><li><p>我们的算法基于一个简单的观察：NN-expansion和 NN-descent 的性能对初始化(initialization)非常敏感(sensitive)。</p></li></ul><br><ul><li><p>我们的 EFANNA 索引包含两部分：</p><ul><li>the multiple randomized hierarchical structures (<em>e.g</em>., randomized truncated KD-tree)</li><li>an approximate <em>k</em>-nearest neighbor graph.</li></ul></li><li><p>在离线阶段，EFANNA 以快速且分层的方式多次将数据集划分为多个子集，生成多个随机化的分层结构。</p><ul><li>这个的话 就是 肯定有重叠了，就是说对一个数据集多次进行了划分，并且 每次的话生成的分层结构数据不一样；因为是随机化</li><li>每一次都是 要求快 并且是 分层</li></ul></li><li><p>然后，EFANNA 通过沿着这些结构自底向上地进行分治(conquering bottom-up along the structures.)来构建近似 k 近邻图。</p><ul><li>这里还没有说明 使用 哪个 分层结构，分层结构还是有很多的；</li></ul></li><li><p>在分治过程中，EFANNA 利用这些结构来定位最接近的可能邻居，并使用这些候选邻居来更新图，这比使用子树中的所有点减少了计算成本。最后，我们像 NN-descent [12]一样优化图，它基于 NN 扩展思想，并使用局部连接、采样和提前终止等技术进行优化 [12]。 When conquering, EFANNA takes advantage of the structures to locate the closest possible neighbors, and use these candidates to update the graph, which reduces the computation cost than using all the points in the subtree. Finally we refine the graph similar to NN-descent [12], which is based on the NN-expansion idea and optimized with techniques like local join, sampling, and early termination [12].</p></li></ul><br><ul><li>在在线搜索阶段，EFANNA 首先在分层结构(hierarchical structures )中进行搜索，以获得给定查询的候选近邻( candidate neighbors)。然后，EFANNA 在近似 k 近邻图上使用 NN-expansion来优化(refines)结果。大量的实验结果表明，我们的方法明显优于最先进的近似最近邻搜索算法。(所以说NN-expansion是在search)</li></ul><br><ul><li>值得强调的是，我们论文的贡献如下： <ul><li>EFANNA 明显优于最先进的近似最近邻搜索算法。特别是，EFANNA 在索引大小(index size)、搜索速度和搜索精度方面优于最流行的近似最近邻搜索库之一 Flann [26]。</li><li>EFANNA 可以构建近似 k 近邻图，在百万规模的数据集上，其速度比暴力构建图快数百倍。考虑到许多无监督和半监督机器学习算法[2]、[29]都是基于最近邻图的。EFANNA 为在大规模数据集上检验所有这些算法的有效性提供了可能。</li><li>我们通过实验结果表明，使用由 EFANNA 构建的低精度近似 k 近邻图，基于图的近似最近邻搜索方法（例如 EFANNA）仍然表现得非常好。这是因为由 EFANNA 构建的近似 k 近邻图中的“错误”邻居实际上是稍微远一点的邻居。这一特性在以前的工作中从未被探索过。This is because the ”error” neighbors of approximate <em>k</em>NN graph constructed by EFANNA are actually neighbors a little farther. This property is never explored in the previous work. <ul><li>这个的原因是？当knn的recall不好的时候 居然 也是 ?难道是 因为莫名奇妙的进行了散化？</li></ul></li></ul></li></ul><br><ul><li><p>本文的其余部分组织如下。在第 2 节中，我们将介绍一些相关工作。我们的 EFANNA 算法在第 3 节中介绍。在第 4 节中，我们将报告实验结果并全面展示 EFANNA 的性能。在第 5 节中，我们将讨论我们的开源库，在第 6 节中，我们将得出结论。</p></li><li><p>When dealing with sparse data (<em>e.g</em>., document retrieval), one can use advanced index structures (<em>e.g</em>., inverted index) to solve this problem.</p></li><li><p>However, for data with dense features, the cost for finding the exact nearest neighbor is <em>O</em>(<em>N</em>), where <em>N</em> is the number of points in the database.</p></li></ul><h2 id="no-2-related-work" tabindex="-1"><a class="header-anchor" href="#no-2-related-work" aria-hidden="true">#</a> <strong>No.2 R</strong>ELATED WORK</h2><ul><li><p>近邻搜索在过去几十年中一直是一个热门话题。由于精确近邻搜索存在内在困难(intrinsic difficulty)，近似近邻（ANN）搜索算法被广泛研究，研究人员期望牺牲一点搜索精度以尽可能降低时间成本( sacrifice a little searching accuracy to lower the time cost as much as possible)。</p></li><li><p>基于层次索引(Hierarchical index)（基于树）的算法，如 KD-tree[13]，在近似最近邻搜索问题上早期取得了成功。然而，当数据维度变高时，它被证明是低效的。</p></li><li><p>为了解决这个限制(limitation)，许多新的基于层次结构的方法[30]、[7]、[27]被提出。Randomized KD-tree[30]和 Kmeans tree[27]被纳入一个著名的开源库 FLANN[25]，该库已经广受欢迎。</p></li><li><p>基于哈希的算法[17]、[34]旨在找到为数据点生成二进制代码的合适方法，并在原始特征空间中保留它们的相似性。这些方法可以被视为用多个超平面划分数据空间，并使用二进制代码表示每个生成的多面体。用不同的约束学习哈希函数将导致数据空间的不同划分。其中最著名的算法之一是局部敏感哈希（LSH）[17]，它本质上基于随机投影[6]。基于不同的约束提出了许多其他变体[34]、[36]、[19]、[21]。并且这些约束反映了他们认为划分数据空间的合适方式。</p></li><li><p>基于哈希的方法和基于树的方法有着相同的目标。它们期望将邻居放入同一个哈希桶（或节点）(hashing bucket (or node))中。然而，这种期望并没有理论保证(theoretical guarantee)。</p></li><li><p>为了提高搜索召回率recall（返回的点中真正的邻居数量除以所需邻居的数量），人们需要检查“附近”的桶或节点。对于高维数据，一个多面体可能有大量的邻居多面体（例如，一个具有 32 位哈希码的桶有 32 个汉明半径距离为 1 的邻居桶），这使得定位真正的邻居变得困难[22]。 one needs to check the “nearby” buckets or nodes. With high dimensional data, one polyhedron may have a large amount of neighbor polyhedrons, (for example, a bucket with 32 bit hashing code has 32 neighbor buckets with 1 hamming radius distance), which makes locating true neighbors hard [22].</p></li><li><p>最近，基于图的技术引起了相当大的关注[11]、[18]、[22]。这些方法的主要思想是邻居的邻居也很可能是邻居，我们称之为 NN-expansion。<em>a neighbor of a neighbor is also likely to be</em> a neighbor*, which we refer as <em>NN-expansion</em>.</p></li><li><p>在离线阶段，他们需要构建一个 k 近邻图，这可以被视为一个大表，记录数据库中每个点的前 k 个最近邻。</p></li><li><p>在在线阶段，给定一个查询点，他们首先为查询分配一些点作为初始候选邻居(as initial candidate neighbors)，然后迭代地检查邻居的邻居以定位更接近的邻居(check the neighbors of the neighbors iteratively to locate closer neighbors.)。</p></li><li><p>图最近邻搜索（GNNS）[18]随机生成初始候选邻居(randomly generate the initial candidate neighbors)，而迭代扩展哈希（IEH）[22]使用哈希算法生成初始候选邻居。</p></li><li><p>由于所有基于图的方法都需要一个 k 近邻图作为索引结构，如何高效地构建一个 k 近邻图成为了一个关键(crucial )问题，尤其是当数据库很大的时候。</p></li><li><p>在构建精确或近似的 k 近邻图方面已经有很多工作。[4]、[9]、[28]、[32]试图快速构建一个精确的 k 近邻图。然而，在大数据的背景下，这些方法仍然不够高效。</p></li><li><p>最近，研究人员试图高效地构建一个近似的 k 近邻图。同样，NN-expansion的思想可以在这里使用。[12]提出了一种 NN-descent算法来高效地构建一个近似的 k 近邻图。</p></li><li><p>NN-descent的基本思想与 NN-expansion类似，但细节不同。NN-descent使用许多技术（例如，局部连接和采样）来有效地细化图。详情请见[12]。The basic idea of NN-descent is similar to NN-expansion but the details are different. NN-descent uses many techniques (<em>e.g</em>., <strong>Local join</strong> and <strong>Sampling</strong>) to efficiently refine the graph. Please see [12] for details.</p></li><li><p>与随机初始化 k 近邻图不同，[8]、[16]、[37]、[31]使用了一些分治方法(some divide-and-conquer methods.)。</p></li><li><p>它们的初始化包含两部分。</p><ul><li>首先，他们多次将整个数据集划分为小的子集。</li><li>其次，他们在子集中进行暴力搜索，并得到许多重叠的子图(overlapping subgraphs)。这些子图可以合并(merged)在一起作为 k 近邻图的初始化。</li><li>然后可以使用类似NN-expansion的技术来细化该图。</li><li>[8]的划分(division )步骤基于谱二分法(spectral bisection)，并且他们提出了两个不同的版本，重叠划分和胶合划分(overlap and glue division)。[37]使用锚图哈希[23]来进行划分。[16]使用递归随机划分(recursive random division)，在子集中与随机采样数据的主方向正交进行划分。[31]使用随机投影树来划分数据集。[16] uses recursive random division, dividing orthogonal to the principle direction of randomly sampled data in subsets. [31] uses random projection trees to partition the datasets.</li></ul></li><li><p>[37]和[31]声称显著优于 NN-descent[12]。然而，根据他们报告的结果以及我们的分析，似乎对 NN-descent[12]存在误解。实际上，NN-descent 与 NN-expansion 有很大不同。[37]和[31]中比较的方法应该是 NN-expansion 而不是 NN-descent。详情请见第 3.3.3 节。</p></li></ul><h2 id="no-3-efanna-algorithms-for-ann-search" tabindex="-1"><a class="header-anchor" href="#no-3-efanna-algorithms-for-ann-search" aria-hidden="true">#</a> No.3 EFANNA ALGORITHMS FOR <strong>ANN</strong> <strong>SEARCH</strong></h2><ul><li><p>我们将在本节中介绍我们的 EFANNA 算法。EFANNA 算法包括离线索引构建部分和在线近似最近邻搜索算法。EFANNA 索引包含两部分：多个层次结构（例如，随机截断 KD 树)(multiple hierarchical structures (<em>e.g</em>., randomized truncated KD-tree))和一个近似 k 近邻图。我们将首先展示如何使用 EFANNA 索引进行在线近似最近邻搜索。然后，我们将展示如何以分治细化的方式(in a divide-conquer-refinement manner.)构建 EFANNA 索引。</p></li><li><p>EFANNA algorithms include two parts:</p><ul><li>offline index building part</li><li>online ANN search algorithm</li></ul></li><li><p>the EFANNA index contains two parts:</p><ul><li>multiple hierarchical structures(<em>e.g</em>., randomized truncated KD-tree) 所以就是说不仅仅是随机截断树只要是多层次结构可能就可以了?；</li><li>an approximate KNN graph</li></ul></li><li><p>how to use EFANNA index to carry out online ANN search?</p></li><li><p>how to build the EFANNA index in a divide-conquer-refinement manner?</p></li></ul><h3 id="_3-1-ann-search-with-efanna-index" tabindex="-1"><a class="header-anchor" href="#_3-1-ann-search-with-efanna-index" aria-hidden="true">#</a> <strong>3.1 ANN search with EFANNA index</strong></h3><figure><img src="'+d+`" alt="Algorithm1" tabindex="0" loading="lazy"><figcaption>Algorithm1</figcaption></figure><ul><li><p>EFANNA 是一种基于图的方法。其主要思想是为最近邻扩展（NN-expansion）提供更好的初始化，以显著提高性能。The multiple hierarchical structures用于初始化，近似 k 近邻图用于最近邻扩展(NN-expansion)。</p></li><li><p>有许多可能的层次结构(hierarchical structures) (<em>e.g</em>.hierarchical clustering [26] or randomized division tree[10]）可以在我们的索引结构中使用。</p></li><li><p>在本文中，我们仅报告使用 randomized <em>truncated</em> KD-tree.的结果。关于这种结构与traditional randomized KD-tree[30]之间差异的细节将在下一小节中讨论。基于这种随机截断 KD 树，我们可以在给定查询 q 的情况下获得初始邻居候选( initial neighbor candidates)。然后，我们使用NN-expansion来细化refine结果，即，根据近似 k 近邻图检查 q 的邻居的邻居，以获得更近的邻居。算法 1 展示了详细的过程。we check the neighbors of <em>q</em>’s neighbors according to the approximate <em>k</em>NN graph to get closer neighbors.</p><ul><li>所以这个算法是 构建 阶段更快了？ 也同样的导致了 search更好了？</li><li>两部分都好了啊；</li></ul></li><li><p>在我们的近似最近邻（ANN）搜索算法中有三个关键参数parameters ：the expansion factor <em>E</em>、the the candidate pool size <em>P</em> 、the iteration number <em>I</em>.。</p></li><li><p>在我们的实验中，我们发现 I = 4 就足够了，因此我们固定 I = 4。可以通过调整参数 E 和 P 来在搜索速度和准确性之间进行权衡( trade-off )。换句话说，较大的 E 和较大的 P 以牺牲(sacrifice )搜索速度为代价来获得高准确性。</p></li></ul><p>在search part by use the EFANNA index</p><ul><li>the main idea is providing better initialization for NN-expansion to improve the performance significantly.</li><li>The multiple hierarchical structures is used for initialization <ul><li>There are many possible hierarchical structures for index structures： <ul><li>hierarchical clustering</li><li>randomized division tree</li><li></li></ul></li></ul></li><li>the approximate <em>k</em>NN graph is used for NN-expansion.</li></ul><h3 id="_3-2-efanna-index-building-algorithms-i-treebuidling" tabindex="-1"><a class="header-anchor" href="#_3-2-efanna-index-building-algorithms-i-treebuidling" aria-hidden="true">#</a> <strong>3.2 EFANNA Index Building Algorithms I : Tree</strong>Buidling</h3><ul><li>算法2</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">//输入:</span>
<span class="token comment">//数据集D;树的数量T;一个叶子结点中的数据点数K</span>
Input<span class="token operator">:</span>
the data set D<span class="token punctuation">,</span> the number of trees T<span class="token punctuation">,</span> the number of points in a leaf node K<span class="token punctuation">.</span>
<span class="token comment">//输出:</span>
<span class="token comment">//随机截断KD树集S</span>
Output<span class="token operator">:</span>
the randomized truncated KD<span class="token operator">-</span>tree set S
<span class="token comment">//构建树的函数BUILDTREE，输入结点和数据点集</span>
function <span class="token function">BUILDTREE</span><span class="token punctuation">(</span>Node<span class="token punctuation">,</span>PointSet<span class="token punctuation">)</span>
    <span class="token comment">//若数据点的个数小于K则直接返回</span>
    <span class="token keyword">if</span> size of PointSet <span class="token operator">&lt;</span> K then
				<span class="token keyword">return</span>
		<span class="token keyword">else</span>
		<span class="token comment">//随机选择一个维度d</span>
        Randomly choose dimension d<span class="token punctuation">.</span>
        <span class="token comment">//计算数据集在该维的平均值mid</span>
				Calculate the mean mid over PointSet on dimension d<span class="token punctuation">.</span> 
        <span class="token comment">//根据平均值把数据集平分为两个子集LeftHalf和RightHalf</span>
				Divide PointSet evenly into two subsets<span class="token punctuation">,</span> LeftHalf <span class="token operator">and</span> RightHalf<span class="token punctuation">,</span> according to mid<span class="token punctuation">.</span>
				<span class="token comment">//递归建立左子树</span>
        <span class="token function">BUILDTREE</span><span class="token punctuation">(</span>Node<span class="token punctuation">.</span>LeftChild<span class="token punctuation">,</span> LeftHalf<span class="token punctuation">)</span>
				<span class="token comment">//递归建立右子树</span>
				<span class="token function">BUILDTREE</span><span class="token punctuation">(</span>Node<span class="token punctuation">.</span>RightChild<span class="token punctuation">,</span>RightHalf<span class="token punctuation">)</span>
		end <span class="token keyword">if</span>
		<span class="token keyword">return</span>
end function
      
<span class="token comment">//迭代T次，建立T棵树</span>
<span class="token keyword">for</span> all i <span class="token operator">=</span> <span class="token number">1</span> to T <span class="token keyword">do</span>
    <span class="token comment">//对数据集D建立第i棵树</span>
	<span class="token function">BUILDTREE</span><span class="token punctuation">(</span>Root<span class="token operator">-</span>i<span class="token punctuation">,</span>D<span class="token punctuation">)</span>
    <span class="token comment">//添加第i棵树根到树集S</span>
	Add Root<span class="token operator">-</span>i to S<span class="token punctuation">.</span>
end <span class="token keyword">for</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>EFANNA 索引的一部分是多个层次结构(a multiple hierarchical structures)。有许多可能的层次结构(hierarchical structures ) (<em>e.g</em>. hierarchical clustering [26] or randomized division tree [10])可以使用。</p></li><li><p>在本文中，我们仅报告使用randomized <em>truncated</em> KD-tree的结果。有关构建随机截断 KD 树的详细信息，请参见算法 2。</p></li><li><p>随机截断 KD 树与传统随机 KD 树之间的唯一区别在于，我们的树中的叶节点有 K 个点（在我们的实验中 K = 10）而不是 1 个点。这种变化使得 EFANNA 中的树构建比传统随机 KD 树快得多。有关详细信息，请参见实验中的表 4。</p></li></ul><figure><img src="`+u+`" alt="table4" tabindex="0" loading="lazy"><figcaption>table4</figcaption></figure><ul><li><p>leaf node in our trees has <em>K</em> (<em>K</em> = 10 in our experiments)points instead of 1. This change makes the tree building in EFANNA much faster than the the traditional randomized KD-tree.</p><ul><li>也就是说 是为了更快的初始化；</li></ul></li><li><p>在这一步中构建的随机截断 KD 树不仅在在线搜索阶段使用，而且在近似 k 近邻图构建阶段也使用。详情请见下一节。</p></li></ul><h3 id="_3-3-efanna-index-building-algorithms-ii-approximate-knn-graph-construction" tabindex="-1"><a class="header-anchor" href="#_3-3-efanna-index-building-algorithms-ii-approximate-knn-graph-construction" aria-hidden="true">#</a> <strong>3.3 EFANNA Index Building Algorithms II : Approxi</strong>mate k<strong>NN Graph Construction</strong></h3><ul><li>EFANNA 索引的另一部分是一个近似 k 近邻图。我们使用与近似最近邻搜索类似的方法(methodology )来高效地构建近似 k 近邻图。</li><li>它包含两个阶段。 <ul><li>在第一阶段，我们将上一部分构建的树视为对数据集的多个重叠划分(regard the trees built in the previous part as multiple overlapping divisions over the data set)，并且我们沿着树结构执行征服步骤( conquering step along the tree structures)以获得一个初始的 k 近邻图。</li><li>在第二阶段，我们使用 NN-descent [12] 来细化 k 近邻图。</li></ul></li></ul><h4 id="_3-3-1-hierarchical-randomized-divide-and-conquer" tabindex="-1"><a class="header-anchor" href="#_3-3-1-hierarchical-randomized-divide-and-conquer" aria-hidden="true">#</a> <em>3.3.1 Hierarchical Randomized Divide-and-Conquer</em></h4><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">//输入：</span>
<span class="token comment">//数据集D;近似kNN图的k;树构建算法构建的随机截断KD树集S;处理的最小深度Dep(从叶子向上处理)</span>
Input<span class="token operator">:</span>
the data set D<span class="token punctuation">,</span> the k in approximate kNN graph <span class="token punctuation">,</span> the randomized truncated KD<span class="token operator">-</span>tree set S built with Algorithm <span class="token number">2</span><span class="token punctuation">,</span> the conquer<span class="token operator">-</span>to depth Dep<span class="token punctuation">.</span>
<span class="token comment">//输出：</span>
<span class="token comment">//近似kNN图G</span>
Output<span class="token operator">:</span>
approximate kNN graph G<span class="token punctuation">.</span>
<span class="token comment">//“分”阶段</span>
<span class="token comment">//使用树构建算法建立多个随机截断KD树，得到树集S</span>
<span class="token operator">%</span><span class="token operator">%</span>Division step
Using Algorithm <span class="token number">2</span> to build tree<span class="token punctuation">,</span> which leads to the input S
<span class="token comment">//“治”阶段</span>
<span class="token operator">%</span><span class="token operator">%</span> Conquer step
<span class="token comment">//近似kNN图G初始化为空</span>
G <span class="token operator">=</span> ∅
<span class="token comment">//对数据集中的每一个结点i</span>
<span class="token keyword">for</span> all point i in D <span class="token keyword">do</span>
	Candidate pool C <span class="token operator">=</span> ∅
	<span class="token comment">//对树集中的每一棵树t</span>
	<span class="token keyword">for</span> all binary tree t in S <span class="token keyword">do</span>
        <span class="token comment">//用数据点i在树t上进行搜索直到叶子结点</span>
		search in tree t with point i to the leaf node<span class="token punctuation">.</span>
        <span class="token comment">//添加叶子结点中的所有数据点到候选集C</span>
		add all the point in the leaf node to C<span class="token punctuation">.</span>
        <span class="token comment">//记下叶子结点的深度d</span>
		d <span class="token operator">=</span> depth of the leaf node
		<span class="token comment">//当d大于给定的处理到的最小深度Dep执行下面的循环</span>
		<span class="token keyword">while</span> d <span class="token operator">&gt;</span> Dep <span class="token keyword">do</span>
			d <span class="token operator">=</span> d − <span class="token number">1</span>
            <span class="token comment">//用数据点i在树t上执行深度优先搜索直到深度d，在深度为d的那层搜索到的非叶结点标记为N，它的还没被访问过的孩子结点标记为Sib</span>
			Depth<span class="token operator">-</span>first<span class="token operator">-</span>search in the tree t with point i to depth d<span class="token punctuation">.</span> Suppose N is the non<span class="token operator">-</span>leaf node on the search path with depth d<span class="token punctuation">.</span> Suppose Sib is the child node of N<span class="token punctuation">.</span> And Sib is <span class="token operator">not</span> on the search path of point i<span class="token punctuation">.</span>
            <span class="token comment">//在以Sib为根的子树上用数据点i执行深度优先搜索，直到叶子结点，添加叶子结点中所有的数据点到候选集C</span>
			Depth<span class="token operator">-</span>first<span class="token operator">-</span>search to the leaf node in the subtree of Sib with point i <span class="token punctuation">.</span> Add all the points in the leaf node to C<span class="token punctuation">.</span>
		end <span class="token keyword">while</span>
	end <span class="token keyword">for</span>
    <span class="token comment">//保留候选集C中离数据点i最近的K个数据点</span>
	Reserve K closest points to i in C<span class="token punctuation">.</span>
	<span class="token comment">//添加候选集C中的点到近邻图，作为数据点i的初始化邻居</span>
	Add C to G<span class="token punctuation">.</span>
end <span class="token keyword">for</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+m+'" alt="Algorithm3" tabindex="0" loading="lazy"><figcaption>Algorithm3</figcaption></figure><ul><li><p>[12]使用一个随机的 k 近邻图作为初始化，并使用 NN-descent 算法对其进行细化以获得高精度的 k 近邻图。我们的想法非常简单。我们试图为 NN-descent 提供一个更好的初始化。一个好的初始化应该在短时间内产生具有一定精度的初始 k 近邻图。</p></li><li><p>分治(Divide-and-conquer)是实现这一目标的一个好策略。</p></li><li><p>一个常规的分治过程首先递归地将问题分解为子问题，直到子问题足够小且易于解决。然后将子问题的解合并起来以得到原始问题的解。</p></li><li><p>对于近似图的构建，划分(division )部分很容易。我们可以按照构建树的方式（第 3.2 节）对数据集进行划分。We can divide the data set as the way the tree was constructed (Section3.2).</p></li><li><p>然后我们从叶子节点向上递归地合并兄弟节点。如果只有一棵树，我们需要向上征服到根节点以得到一个完全连通的图。但是如果我们直接从叶子节点向根节点进行征服，计算成本会呈指数增长，并且不少于暴力构建图的成本。Then we merge sibling nodes recursively upwards from leaf. With only one tree, we need to conquer to root to get a full connected graph. But if we directly conquer from leaf to root, the computational cost grows exponentially upwards and is no less than brute-force graph construction.</p></li><li><p>为了降低计算复杂度，我们需要找到一种更好的“conquer” strategy and avoid conquering to root.我们遵循先前工作[16]的灵感，即多次随机划分可以产生重叠的子集。因此，“征服”不需要进行到根节点。此外，我们对更好的“征服”策略的动机是减少在每个级别进行“征服”时涉及的点的数量，并保持点的“质量”（我们确保始终选择尽可能接近的点进行“征服”）。例如，在图 1 中，如果我们知道节点（或子集）8 中的点 q 更接近节点 10 的区域，那么当用 q 在级别 1 中“征服”4 和 5 时，我们只需要考虑节点 10 中的点。</p></li><li><p>We follow the inspiration of previous work [16], which said multiple randomized division can produce overlapping subsets. As a result, the conquer doesn’t need to be carried out to root. In addition, our motivation on better conquer strategy is to reduce the number of points involved in conquering at each level and keep points’ “quality” (we make sure that we always choose the closest possible points for conquer). For example, in Fig 1, if we know that point <em>q</em> in node (or subset) 8 is closer to the area of node 10, then we just need to consider the points in node 10 when conquering 4 and 5 at level 1 with <em>q</em>.</p></li><li><p>换句话说，当我们“征服”两个兄弟非叶节点时（兄弟叶节点可以直接被“征服”），对于一个子树中的某些点，我们可能只考虑兄弟子树中“最接近”的可能叶节点。因为其余的叶节点更远，它们中的点也可能更远，因此被排除在距离计算之外。我们可以将树视为一个多类分类器，每个叶节点可以被视为一个类别。这个分类器可以像图 1 中的矩形那样划分数据空间，不同的颜色代表不同的标签。有时最近邻（例如区域 8 和区域 10 中的白点）彼此接近，但根据这个具有区分平面（例如节点 2）将它们分开的分类器，它们适合不同的区域。在图 1 中，假设当点 q 被输入到树分类器中时，点 q 将被分配标签 8。当在级别 1 进行“征服”时，我们需要知道在子树 5 中节点 10 和 11 之间的哪个区域更接近 q。既然整棵树可以被视为一个多类分类器，那么它的任何子树也可以是一个多类分类器。为了知道哪个区域更近，我们只需让子树 5 的分类器做出选择。通过将 q 输入到分类器中，我们将为 q 获得 10 和 11 之间的一个标签。由于 q 是区域 8 中的白点，从图 1 中的矩形可以明显看出，q 将被标记为 10。因此，对于 q，在级别 1 进行“征服”时，只有节点 10 中的点会被涉及。</p></li><li><p>when we conquer two sibling nonleaf nodes (sibling leaf nodes can be conquered directly), for some points in one subtree, we may just consider the “closest” possible leaf node in the sibling subtree.</p></li><li><p>Because the rest leaf nodes are farther, the points in them are also likely to be farther, thus excluded from distance calculating. We can regard the tree as a <strong>multi-class</strong> <strong>classifier</strong>, each leaf node can be treated as a class. This classifier may divide the data space like the rectangle in Fig. 1 does and different colors represent different labels.</p></li><li><p>Sometimes nearest neighbors (<em>e.g</em>. white points in area 8 and area 10) are close to each other, but fit in different area according to this classifier with a discriminative plane (<em>e.g</em>. node 2) separating them. In Fig .1, suppose point <em>q</em> will be assigned label 8 when <em>q</em> is input into the tree classifier. When conquering at level 1, we need to know in subtree 5 which area between node 10 and 11 is closer to <em>q</em>.</p></li><li><p>Now that the whole tree can be treated as a <strong>multi-class classifier</strong>, any subtree of it can be a <strong>multi</strong><em>class classifier</em>*, too. To know which area is closer, we simply let the classifier of subtree 5 make the choice. By inputing <em>q</em> to the classifier, we will obtain a label between 10 and 11 for q. Since <em>q</em> is the white point in area 8, from the rectangle in Fig. 1, it’s obvious that <em>q</em> will be labeled as 10. Therefore for <em>q</em>, when conquering at level 1, only points in node 10 will be involved.</p></li></ul><figure><img src="'+g+'" alt="figure1" tabindex="0" loading="lazy"><figcaption>figure1</figcaption></figure>',42),j=e("ul",null,[e("li",null,[e("p",null,"通过这种方式，对于每个级别上的每个点，只会考虑一个最接近的叶节点中的点，这大大降低了计算复杂度并保留了准确性。我们多次执行随机分治过程并得到一个初始图。")]),e("li",null,[e("p",null,"In this way, for each point at each level, only the points in one closest leaf node will be considered, which reduces the computation complexity greatly and reserves(保留)accuracy. We perform our random divide-and-conquer process multiple times and get an initial graph.")]),e("li",null,[e("p",null,"同样，在初始图的准确性和参数调整的时间成本之间存在权衡。当“征服”深度 Dep 较小时（即“征服”到接近根节点的级别），或者当树的数量 Tc 较大时，准确性更高，但时间成本也更高。在我们的实验中，我们使用随机 KD 树作为分层分治结构。有关随机 KD 树分治算法的详细信息，请参见算法 3。")]),e("li",null,[e("p",null,[i("Again there is a trade-off between accuracy of initial graph and time cost in parameter tuning. When conquer to depth "),e("em",null,"Dep"),i(" is small ("),e("em",null,"i.e"),i(". conquering to a level close to root), or when tree number "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"T"),e("mi",null,"c")])]),e("annotation",{encoding:"application/x-tex"},"T_c")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight"},"c")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])])])])]),i(" is larger, the accuracy is higher but time cost is higher. In our experiments, we use the randomized KD-tree as the hierarchical divideand-conquer structure. See Algorithm 3 for details on randomized KD-tree divide-and-conquer algorithm).")])]),e("li",null,[e("p",null,[i("[12] uses a random "),e("em",null,"k"),i("NN graph as the initialization and refined it with the NN-descent algorithm to get a "),e("em",null,"k"),i("NN graph with high accuracy.")])]),e("li",null,[e("p",null,[i("Our idea is very simple. We try to provide a better initialization for NN-descent. A good initialization should produce an initial "),e("em",null,"k"),i("NN graph with certain accuracy within short time.短时间内获得不错的准确度；")])])],-1),V=s('<p>Divide-and-conquer：分而治之；</p><p>conquer：征服，克服，战胜</p><ul><li><p>Divide-and-conquer is a good strategy to achieve this goal. A normal divide-and-conquer process first breaks</p><p>the problem into subproblems recursively until the subproblem is small and easy enough to solve. Then the</p><p>solutions of subproblems are combined to get a solution to the original problem.</p></li><li><p>For approximate graph construction, the division part is easy.</p></li><li><p>We can divide the data set as the way the tree was constructed (Section 3.2). Then we merge sibling nodes recursively upwards from leaf. With only one tree, we need to conquer to root to get a full connected graph. But if we directly conquer from leaf to root, the computational cost grows exponentially upwards and is no less than brute-force graph construction.</p></li><li><p>To reduce the computational complexity, we need to find a better conquer strategy and avoid conquering to root. We follow the inspiration of previous work [16], which said multiple randomized division can produce overlapping subsets. As a result, the conquer doesn’t need to be carried out to root. In addition, our motivation on better conquer strategy is to reduce the number of points involved in conquering at each level and keep points’ “quality” (we make sure that we always choose</p><p>the closest possible points for conquer). For example, in Fig 1, if we know that point <em>q</em> in node (or subset) 8 is closer to the area of node 10, then we just need to consider the points in node 10 when conquering 4 and 5 at level 1 with <em>q</em>.</p></li></ul><h4 id="_3-3-2-graph-refinement" tabindex="-1"><a class="header-anchor" href="#_3-3-2-graph-refinement" aria-hidden="true">#</a> <em>3.3.2 Graph Refinement</em></h4><ul><li>We use the NN-descent proposed by [12] to refine the resulting graph we get from the divide-and-conquer step.</li><li>The main idea is also to find better neighbors iteratively, however, different from NN-expansion, they proposed several techniques to get much better performance.</li><li>We rewrite their algorithms to make it easy to understand. See Algorithm 4 for details. <ul><li>The pool size <em>P</em> and neighbor checking num <em>L</em> are essential parameters of this algorithm.</li><li>Usually, Larger <em>L</em> and <em>P</em> will result in better accuracy but higher computation cost.</li></ul></li></ul><h4 id="_3-3-3-nn-expansion-vs-nn-descent" tabindex="-1"><a class="header-anchor" href="#_3-3-3-nn-expansion-vs-nn-descent" aria-hidden="true">#</a> <em>3.3.3 NN-expansion VS. NN-descent</em></h4><ul><li>Some approximate <em>k</em>NN graph construction methods [37] [31] claim to outperform NN-descent [12] significantly. However, based on their reported results and our analysis, there seems a misunderstanding of NN-descent [12].</li><li>Actually, NN-descent is quite different than NN-expansion. For given point <em>p</em>, <ul><li>NN-expansion assume the neighbors of <em>p</em>’s neighbors are likely to be neighbors of <em>p</em>.</li><li>While NN-descent thinks that <em>p</em>’s neighbors are more likely to be neighbors of each other.</li></ul></li><li>Our experimental results have shown that NN-descent is much more efficient than NN-expansion in building approximate <em>k</em>NN graph. However, the NN-descent idea cannot be applied to ANN search.</li><li>所以就是说 NN-expansion可以应用到 ANN search，而NN-descent是不行的？</li></ul><h3 id="_3-4-online-index-updating" tabindex="-1"><a class="header-anchor" href="#_3-4-online-index-updating" aria-hidden="true">#</a> <strong>3.4 Online index updating</strong></h3><ul><li>EFANNA index building algorithm is easily to be extended to accept stream data.</li><li>Firstly, when a new point arrived, we can insert it into the tree easily. <ul><li>And when the number of points in the inserted node exceeds given threshold, we just need to split the node.</li><li>When the tree is unbalanced to some degree, we should adjust the tree structure, which is quite fast on large scale data.</li></ul></li><li>Secondly, the graph building algorithm can accept stream data as well, we can use the same algorithm we describe before. <ul><li>First we search in the tree for candidates,</li><li>and use NN-descent to update the graph with the involved points. And this step is quite fast, too.</li></ul></li></ul><p>因为整个index包括了 两部分 一个是树一个是图； 树的话就是插入，图的话是先通过树进行获得初始化，然后再调用NN-Descent</p><h2 id="no-4-experiments" tabindex="-1"><a class="header-anchor" href="#no-4-experiments" aria-hidden="true">#</a> No.4 EXPERIMENTS</h2><ul><li>为了证明(demonstrate)所提出的方法 EFANNA 的有效性，在本节中报告了在大规模数据集(large-scale data sets)上进行的大量(extensive)实验。</li></ul><h3 id="_4-1-data-set-and-experiment-setting" tabindex="-1"><a class="header-anchor" href="#_4-1-data-set-and-experiment-setting" aria-hidden="true">#</a> <strong>4.1 Data Set and Experiment Setting</strong></h3><ul><li>在两个流行的真实世界数据集 SIFT1M 和 GIST1M1 上进行了实验(conducted)。数据集的详细信息列在表 1 中。我们使用的所有代码都是用 C++编写的，并由 g++4.9 编译，我们允许的唯一优化选项(optimization option)是 g++的“O3”。并行性(Parallelism)和其他优化（如 SSE 指令）被禁用。在 SIFT1M 上的实验是在一台具有 i7-3770K CPU 和 16G 内存的机器上进行的，而在 GIST1M 上的实验是在一台具有 i7-4790K CPU 和 32G 内存的机器上进行的。 <ul><li>这个的意思 是 para的优化部分被去掉了 还是说 不用并行？</li></ul></li></ul><figure><img src="'+N+'" alt="table1" tabindex="0" loading="lazy"><figcaption>table1</figcaption></figure><h3 id="_4-2-experiments-on-ann-search" tabindex="-1"><a class="header-anchor" href="#_4-2-experiments-on-ann-search" aria-hidden="true">#</a> <strong>4.2 Experiments on ANN Search</strong></h3><h4 id="_4-2-1-evaluation-protocol" tabindex="-1"><a class="header-anchor" href="#_4-2-1-evaluation-protocol" aria-hidden="true">#</a> <em>4.2.1 Evaluation Protocol</em></h4>',17),J=e("ul",null,[e("li",null,[e("p",null,[i("为了衡量不同算法在近似最近邻搜索中的性能，我们使用了众所周知的平均召回率("),e("em",null,"average recall"),i(")作为准确性度量[24]。对于给定的查询点，所有算法都应返回 k 个点。然后我们需要检查这个返回的集合中有多少个点在查询点的真正的 k 个最近邻之中。假设对于一个查询点，返回的 k 个点的集合为 "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i("，查询点的真正的 k 个最近邻集合为 R，则召回率定义为：")]),e("ul",null,[e("li",null,"那个公式；")])]),e("li",null,[e("p",null,[i("然后平均召回率是对所有查询进行平均。由于 "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i("和 R 的大小相同，所以 "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i(" 的召回率与 "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i(" 的准确率相同。")])]),e("li",null,[e("p",null,[i("我们通过要求每个查询点 有不同数量的最近邻 来比较不同算法的性能，包括 1 近邻（1-NN）和 100 近邻（100-NN）。换句话说，R（"),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i("）的大小将分别为 1 和 100。有关10-NN和 50-NN的更多结果，请参阅我们的技术报告[14]。")])])],-1),Z=s('<h4 id="_4-2-2-comparison-algorithms" tabindex="-1"><a class="header-anchor" href="#_4-2-2-comparison-algorithms" aria-hidden="true">#</a> <em>4.2.2 Comparison Algorithms</em></h4><ul><li><p>为了展示(demonstrate)所提出的 EFANNA 方法的有效性，在实验中对以下四种最先进( state-of-the-art)的近似最近邻搜索方法和暴力搜索(brute-force)方法进行了比较。</p><ul><li><strong>brute-force</strong>.我们报告暴力搜索的性能以展示使用近似最近邻搜索方法的优势。为了获得不同的召回率，我们只需在不同比例的查询数量上执行暴力搜索。例如，原始查询集的 90%查询上的暴力搜索时间代表 90%平均召回率下的暴力搜索时间。</li><li>因为暴力搜索的 recall为 100%； 所以这样 如果仅仅暴力查询90%那么就是达到recall=0.9的 时间；</li><li><strong>flann</strong>.FLANN 是一个著名的用于近似最近邻搜索的开源库[26]。FLANN 中的随机 KD 树( Randomized KD-tree)算法提供了最先进(sota)的性能。在我们的实验中，对于两个数据集我们都使用 16 trees。并且我们调整“max-check”参数以获得时间 - 召回率曲线(time-recall curve)。</li><li><strong>GNNS</strong>.GNNS 是第一个使用 KNNG的近似最近邻搜索方法[18]。对于一个查询，GNNS 通过随机选择生成初始候选（邻居）generates the initial candidates (neighbors) 。然后 GNNS 使用近邻扩展思想(NN-expansion idea)（即，迭代地检查邻居的邻居以找到更近的邻居）来优化结果。GNNS 的主要参数是初始结果的大小和迭代次数。我们将迭代次数固定为 10，并调整初始候选数量以获得时间 - 召回率曲线。</li></ul></li><li><p>在五种被比较的近似最近邻方法中，Flann’ KD-tree是基于 hierarchical structure (tree)的方法。其他四种被比较的方法都是基于图的方法。我们没有与基于哈希(hashing)的方法进行比较，因为[22]表明 IEH 相对于相应的(corresponding)哈希方法有显著的改进(significant improvement)。</p></li><li><p>所有基于图的方法都需要一个 pre-built <em>k</em>NN graph，我们使用一个真实的 10 近邻图。</p></li></ul><h4 id="_4-2-3-results" tabindex="-1"><a class="header-anchor" href="#_4-2-3-results" aria-hidden="true">#</a> <em>4.2.3 Results</em></h4><ul><li>在两个数据集上所有算法的时间召回曲线(time-recall curves)可以在图 2 和图 3 中看到。各种算法的索引大小(The index size)如表 2 所示。可以得出一些有趣的结论如下：the index size is the mem usage <ul><li>1）我们的 Efanna 算法在两个数据集的所有情况下都显著优于其他所有方法。即使在相对较高的召回率（例如 95%）下，Efanna 在 SIFT1M 上比暴力搜索快约 100 倍，在 GIST1M 上比暴力搜索快约 10 倍。</li><li>2）对于近似最近邻搜索来说，GIST1M 是比 SIFT1M 更难的( harder dataset)数据集。在相对(relatively)较高的召回率（例如 95%）下，所有的近似最近邻搜索方法都明显比暴力搜索快。然而，在 GIST1M 上，一些方法（flann、GNNS、kGrpah）与暴力搜索相似（甚至更慢）。原因可能是 GIST1M 的高维度(the high dimensionality)。</li><li>3）当所需的最近邻数量较大时（例如 10、50 和 100），所有基于图的方法都明显优于 Flann 的 KD-tree。由于在实际搜索场景中 10、50 或 100 个结果更为常见，所以基于图的方法具有优势。</li><li><ol start="4"><li>GNNS 和 kGraph 本质上(essentially)是相同的算法。实验结果证实了这一点。微小的差异可能是由于随机初始化造成的。</li></ol></li><li><ol start="5"><li>我们使用完全相同的框架实现了四种基于图的方法（GNNS、IEH-LSH、IEH-ITQ 和 Efanna）。唯一的区别在于初始化：GNNS 使用随机选择，IEH 使用哈希，Efanna 使用截断 KD 树。这些方法之间的性能差距表明了不同初始化方法的有效性。截断 KD 树优于哈希，这两者又优于随机选择。并且 ITQ 优于 LSH。</li></ol></li><li>6）表 2 显示了不同算法的索引大小。Flann 消耗的内存最大。Efanna 的索引大小略大于 IEH。为了减小索引大小，可以在 Efanna 中使用较少的树，但仍保持高性能。我们将在 4.5 节中讨论这个问题。.</li><li>7）GNNS 和 KGraph 的索引大小最小，因为它们只需要存储一个 k 近邻图。IEH 和 Efanna 都为了更好的搜索性能而牺牲了索引大小（额外的数据结构以获得更好的初始化）。</li><li>8）考虑到搜索性能和索引大小，基于图的方法比 Flann 的 KD 树是更好的选择。</li></ul></li></ul><figure><img src="'+f+'" alt="figure2" tabindex="0" loading="lazy"><figcaption>figure2</figcaption></figure><figure><img src="'+b+'" alt="figure3" tabindex="0" loading="lazy"><figcaption>figure3</figcaption></figure><figure><img src="'+v+'" alt="table2" tabindex="0" loading="lazy"><figcaption>table2</figcaption></figure><h3 id="_4-3-experiment-on-approximate-knn-graph-construction" tabindex="-1"><a class="header-anchor" href="#_4-3-experiment-on-approximate-knn-graph-construction" aria-hidden="true">#</a> <strong>4.3 Experiment on Approximate kNN Graph Construction</strong></h3><ul><li>在上一节中我们展示了基于图的方法在近似最近邻搜索中可以实现非常好的性能。</li><li>然而，上述结果是基于一个真实(ground truth)的10-NN graph。表 3 显示了为两个数据集构建真实的10-NN graph的时间成本。在 SIFT1M 数据集上大约需要 17 个小时的 CPU 时间，在 GIST1M 数据集上大约需要一周。</li><li>显然，暴力搜索不是一个可接受的选择。[18]、[22]假设真实的 k 近邻图已经存在。然而，构建 k 近邻图是所有基于图的方法中索引部分的一个步骤。为了使基于图的近似最近邻搜索方法在实际中有用，我们需要讨论如何有效地构建 k 近邻图。 <ul><li>如何构建KNNG 快速的 用来当检验</li></ul></li></ul><figure><img src="'+k+'" alt="table3" tabindex="0" loading="lazy"><figcaption>table3</figcaption></figure><ul><li>在本节中，我们将比较几种近似kNN图构造方法的性能。</li></ul><h4 id="_4-3-1-evaluation-protocol" tabindex="-1"><a class="header-anchor" href="#_4-3-1-evaluation-protocol" aria-hidden="true">#</a> <em>4.3.1 Evaluation Protocol</em></h4><ul><li>recall的计算介绍</li></ul><h4 id="_4-3-2-comparison-algorithms" tabindex="-1"><a class="header-anchor" href="#_4-3-2-comparison-algorithms" aria-hidden="true">#</a> <em>4.3.2 Comparison Algorithms</em></h4><ul><li><p>\\1) <strong>brute-force</strong>:To get different graph accuracy, we simply perform brute-force graph construction on different percentage of the data points.</p></li><li><p>\\2) <strong>SGraph</strong>: We refer to the algorithm proposed in [16] as SGraph. SGraph build the graph with three steps. First they generates initial graph by randomly dividing the data set into small ones iteratively and the dividing is carried out many</p><p>times. Then they do brute-force graph construction within each subsets and combine all the subgraph into a whole. Finally they refine the graph using a technique similar to NN-expansion.</p></li><li><p>\\3) <strong>FastKNN</strong>: We refer to the algorithm proposed in [37] as FastKNN. The last two steps of their graph building process is similar to SGraph. While FastKNN uses hashing method (specifically, AGH[23]) to generate the initial graph.</p></li><li><p>\\4) <strong>NN-expansion</strong>: The main idea of building approximate <em>k</em>NN graph with NN-expansion is to cast the graph construction problem as <em>N</em> ANN search problems, where <em>N</em> is the data size. However, NNexpansion is proposed for ANN search while not</p><p>for AKNN graph construction. The reason we add it to the compared algorithms in this section is that some previous works [37] [31] claim to outperform NN-descent. While we find there may be misunderstanding that they may actually compared with NN-expansion rather than NN-descent.</p></li><li><p>\\5) <strong>NN-descent</strong> [12]: This algorithm first initializes the graph randomly. Then NN-descent refine it iteratively with techniques like local join and sampling[12]. Local join is to do brute-force searching within a point <em>q</em>’s neighbors which is irrelevant to <em>q</em>.</p><p>Sampling is to ensure number of points involved in the local join is small, but the algorithm is still efficient。</p></li><li><p>\\6) <strong>kGraph</strong>: kGraph [11] is an open source library for approximate <em>k</em>NN graph construction and ANN search. The author of kGraph is the author of NN-descent. The approximate <em>k</em>NN graph algorithm implemented in kGraph library is exactly NN-descent. kGraph implements with OpenMP and SSE instruction for speed-up. For faire comparison, we disable the parallelism and SSE instruction.</p></li><li><p>\\7) <strong>LargeVis</strong> [31]: This algorithm is proposed for high dimension data visualization. The first step of LargeVis is to build an approximate <em>k</em>NN graph. LargeVis uses random projection tree and NN-expansion to build this graph.</p></li><li><p>\\8) <strong>Efanna</strong>: The algorithm proposed in this paper. We use hierarchical divide-and-conquer to get an initial graph. And then use NN-descent to refine the graph. In this experiments, <mark>we use 8 randomized truncated KD-trees to initialize the graph.</mark></p></li><li></li></ul><h4 id="_4-3-3-results" tabindex="-1"><a class="header-anchor" href="#_4-3-3-results" aria-hidden="true">#</a> <em>4.3.3 Results</em></h4><ul><li><p>The time-accuracy curves of different algorithms on two data sets are shown in Fig. 4 and Fig. 5 receptively. A</p><p>number of interesting conclusions can be made.</p><ul><li>\\1) EFANNA outperforms all the other algorithms on approximate <em>k</em>NN graph building. It can achieve more than 300 times speed-up over brute-force construction to reach 95% accuracy. Without parallelism, it takes a week to build a 10-NN graph on GIST1M using brute-force search. Now the time can be reduced to less than an hour by using EFANNA.</li><li>\\2) We didn’t get the source code of SGraph and FastKNN. So we implement their algorithms on our own. However, the performances shown in the two figures are quite different from what the original papers [16] [37] claim. One of the reasons may be the implementation. In the original FastKNN paper [37], the authors fail to add the hashing time into the total graph building time but actually should do. Fortunately, [16] reported that SGraph achieved 100 times speed-up over brute-force on the SIFT1M at 95% accuracy. And SGraph got 50 times speed-up over brute-force on the gist1M (384 dimensions) at 90% accuracy. While EFANNA achieves over 300 times speed-up on both SIFT1M and GIST1M (960 dimensions).</li><li>\\3) LargeVis achieve significant better result than NN-expansion. However, NN-descent is better than LargeVis, especially when we want an accurate graph. This results confirm our assumption that many previous works had the misunderstanding of NN-descent. The result reported in their paper is actually NN-expansion [37], [31] rather than NN-descent.</li><li>\\4) kGraph and NN-descent are actually the same algorithm. The only difference is that we implement NN-descent by ourselves and kGraph is an open library. The performance difference of these two methods should due to the implementation.</li><li>\\5) The only difference between EFANNA and NN-descent (kGraph) is the initialization. EFANNA uses randomized truncated KD-tree to build the initial graph while NN-descent (kGraph) use random initialization.</li><li>\\6) The performance advantage of EFANNA over NN-descent is larger on the SIFT1M than on the GIST1M. The reason maybe the GIST1M (960 dimensions) has higher dimensionality than the SIFT1M (128 dimensions). The KD-tree initialization becomes less effective when dimensions becomes high. The similar phenomena happens when we compare EFANNA and LargeVis. Since LargeVis uses random projection trees for initialization, this suggests random projection trees meybe better than KD-tree when the dimensions is high. Using random projection trees as the hierarchical structures of EFANNA can be the future work. random projection trees ：随机投影树</li></ul></li></ul><figure><img src="'+w+'" alt="figure4" tabindex="0" loading="lazy"><figcaption>figure4</figcaption></figure><figure><img src="'+A+'" alt="figure5" tabindex="0" loading="lazy"><figcaption>figure5</figcaption></figure><h3 id="_4-4-efanna-with-approximate-knn-graphs" tabindex="-1"><a class="header-anchor" href="#_4-4-efanna-with-approximate-knn-graphs" aria-hidden="true">#</a> <strong>4.4 EFANNA with Approximate kNN Graphs</strong></h3><ul><li>The experimental results in the last section show that EFANNA can build an approximate <em>k</em>NN graph efficiently. However, there are no published results on the performance of graph based ANN search methods on an approximate <em>k</em>NN graph. <ul><li>？？？？</li></ul></li><li>In this section, we evaluate the performance of EFANNA on approximate <em>k</em>NN graphs with various accuracy. The results on two data sets are shown in Fig.6 and 7 respectively. <ul><li>意思是测试 使用一个其它的图， 仅仅测试 EFANNA的search算法？？</li></ul></li><li>From these two figures, we can see that the ANN search performance of EFANNA suffers from very little decrease in performance even when the graph is only “half right”. Specifically, the ANN search preformance of EFANNA with a 60% accurate 10-NN graph is still significant better than Flann-kd tree on SIFT1M. On GIST1M, EFANNA with a 57% accurate 10-NN graph is significant better than Flann-kdtree. <ul><li>这里说明了 在 KNNG的 recall不好的 时候， EFANNA 的 search仍然很好？</li></ul></li></ul><figure><img src="'+x+'" alt="figure6" tabindex="0" loading="lazy"><figcaption>figure6</figcaption></figure><figure><img src="'+y+'" alt="figure7" tabindex="0" loading="lazy"><figcaption>figure7</figcaption></figure>',23),X=e("ul",null,[e("li",null,[e("p",null,[i("These results are significant because building a less accurate "),e("em",null,"k"),i("NN graph using EFANNA is very efficient.Table 4 shows the indexing time of EFANNA and Flann-k-dtree. If a 60% accurate graph is used, the indexing time of EFANNA is similar to that of Flann-kdtree. Combing the results in Table 2, we can see that comparing with Flann-kdtree, EFANNA takes similar indexing time, smaller index size and significant better ANN search performance.")])]),e("li",null,[e("p",null,[i("Why EFANNA can get such a good ANN search performance even with a “half right” graph? Table 5 may explain the reason. The accuracy defined in Eqn.2 uses the size of "),e("em",null,"R"),i(" and "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msup",null,[e("mi",null,"R"),e("mtext",null,"‘")])]),e("annotation",{encoding:"application/x-tex"},"R^‘")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8491em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.8491em"}},[e("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mtight"},"‘")])])])])])])])])])]),i(" . The former is the true nearest neighbors set while the latter is the returned nearest neighbors set of an algorithm. In the previous experiments, we fix the sizes of both "),e("em",null,"R"),i(" and "),e("em",null,"R**0"),i(" as 10.")])])],-1),Y=s('<figure><img src="'+E+'" alt="table5" tabindex="0" loading="lazy"><figcaption>table5</figcaption></figure><ul><li>Table 5 reports the results by varying the size of <em>R</em> form 10 to 100. We cam see that a 60% accurate 10-NN graph constructed by EFANNA in SIFT1M means 60% of all the neighbors are true 10-nearest neighbors. And the remaining 40% neighbors are not randomly select from the whole dataset. Actually, 98.9% of the neighbors are true 100-nearest neighbors. These results show that the approximate <em>k</em>NN graphs constructed by EFANNA are very good approximation of the ground truth <em>k</em>NN graph.</li></ul><h3 id="_4-5-efanna-with-different-number-of-trees" tabindex="-1"><a class="header-anchor" href="#_4-5-efanna-with-different-number-of-trees" aria-hidden="true">#</a> <strong>4.5 EFANNA with Different Number of Trees</strong></h3><ul><li><p>In the previous experiments, EFANNA uses 16 truncated kd-trees for search initialization. Table 2 shows that these</p><p>trees consume a large number of memory space. In this experiment, we want to explore how the number of trees will influence the performance of EFANNA on ANN search. Throughout this experiment, we use the 10-NN ground truth graph.</p></li><li><p>The ANN search results on SIFT1M and GIST1M are shown in Fig. 8 and FIg. 9 respectively. We simply compare with IEH-ITQ and FLANN, because IEH-ITQ is the second best algorithm on ANN search in our previous experiment while FLANN also has the tree number parameter.</p></li></ul><figure><img src="'+F+'" alt="figure8" tabindex="0" loading="lazy"><figcaption>figure8</figcaption></figure><figure><img src="'+_+'" alt="figure9" tabindex="0" loading="lazy"><figcaption>figure9</figcaption></figure><ul><li><p>From Fig. 8 and 9, we can see that with less number of trees, the ANN search performances of both EFANNA and Flann decrease. However, with only 4 trees, EFANNA still significantly better than IEH-ITQ (especially on the GIST1M data set). While the index sizes can be significantly reduced as suggested by Table\\6. With 4 trees, the index size of EFANNA is smaller than</p><p>that of IEH-ITQ.</p></li></ul><figure><img src="'+T+'" alt="table6" tabindex="0" loading="lazy"><figcaption>table6</figcaption></figure><ul><li>The results in this section show the flexibility of EFANNA over other graph based ANN search methods.One can easily make trade-off between index size and search performance.</li></ul><h3 id="_4-6-efanna-with-different-number-of-k-in-knngraph" tabindex="-1"><a class="header-anchor" href="#_4-6-efanna-with-different-number-of-k-in-knngraph" aria-hidden="true">#</a> <strong>4.6 EFANNA with Different Number of</strong> <em>k</em> <strong>in</strong> <em>k</em>NN<strong>Graph</strong></h3><ul><li><p>The EFANNA index contains two parts: the truncated kd-trees and the <em>k</em>NN graph. If we regard the <em>k</em>NN graph as an <em>N</em> <em>×</em>k matrix, we can use the “width” of the graph to denote <em>k</em>. In the previous section, we have checked the performance of EFANNA with different number of trees. Now we will show how the “width” of <em>k</em>NN graph influences ANNS performance of EFANNA.</p></li><li><p>Fig.10 and 11 show the ANNS performance of EFANNA with graph 10NN, 20NN, 40NN on SIFT1M and GIST1M. The index size are showed in TABLE 7 respectively. From TABLE 7 we can see that, from 10NN to 40NN, the size of EFANNA index grows gradually. Besides, in Fig.10, 11, the performance of increase with the growing of graph ‘width’.</p></li></ul><figure><img src="'+S+'" alt="figure10" tabindex="0" loading="lazy"><figcaption>figure10</figcaption></figure><figure><img src="'+I+'" alt="figure11" tabindex="0" loading="lazy"><figcaption>figure11</figcaption></figure><figure><img src="'+z+'" alt="table7" tabindex="0" loading="lazy"><figcaption>table7</figcaption></figure><ul><li><p>Compared with Fig. 8, 9, we can get a conclusion that widening the graph provides more boost on ANNS performance of EFANNA than add more trees. And from the comparison between TABLE 6 and 7, we find that with equal extra memory cost, widening graph is a better choice then using more trees.</p></li><li><p>However, we should also notice that the performance boost does not increase linearly with the ‘width’ of the graph. In other words, there may exists an upper bound of performance boost by increasing EFANNA index size, either from the aspect of tree or graph.</p></li></ul><h3 id="_4-7-ann-search-comparison-with-same-index-size" tabindex="-1"><a class="header-anchor" href="#_4-7-ann-search-comparison-with-same-index-size" aria-hidden="true">#</a> <strong>4.7 ANN Search Comparison with Same Index Size</strong></h3><ul><li><p>The results in previous section suggest the comparisons in section 4.2 is not quite fair due to different index size of different algorithms. In this section, we try to compare different algorithms with (almost) equal index size.</p></li><li><p>We reported the performance of EFANNA, IEH-ITQ, GNNS and flann’s KD-tree. We do not compare with IEH-LSH simply because IEH-ITQ is better than IEHLSH. We do not compare with kGraph because GNNS is almost identical等同 with kGraph.</p></li><li><p>We restrict the index size of each algorithm to about 265 MB. Finally, we use 4 trees for flann’s KD-tree; 4 trees and 40NN graph for EFANNA; 1 table and 40NN graph for IEH-ITQ; 60NN graph for GNNS. See TABLE 8 for details on how we organize the index to get almost equal size. Fig. 12, 13 show the performance of these algorithms on SIFT1M and GIST1M.<br><img src="'+q+'" alt="table8" loading="lazy"></p></li></ul><figure><img src="'+G+'" alt="figure12" tabindex="0" loading="lazy"><figcaption>figure12</figcaption></figure><figure><img src="'+K+'" alt="figure13" tabindex="0" loading="lazy"><figcaption>figure13</figcaption></figure><ul><li><p>On both two datasets, graph based methods achieve over 20x speed up over flann’s KD-tree with the same</p><p>index size. Particularly, EFANNA is about 30x faster than flann’s KD-tree. This suggests the advantage of graph based methods over traditional tree structure based methods.</p></li><li><p>Compared with the results in Fig. 2 and 3, we can find that the performance gain achieved by EFANNA over IEH-ITQ and GNNS (kGraph) becomes smaller as the “width” of graph grows. This indicates the impact of good initialization for NN-expansion becomes small as the “width” of graph grows. 随着KNNG 的 K变大，那么 其 初始化的 效果变得不明显了；</p></li><li><p>With the same index size, EFANNA and IEH-ITQ still have small advantage than GNNS on SIFT1M when the recall is low. At a high recall level (<em>e.g</em>., 95%), the performances of three algorithms are almost the same. Particularly, when we search for 100NN, the performance of GNNS (random initialization) is better than EFANNA and IEH-ITQ at 95% recall level.</p><ul><li>K100的时候不好；</li></ul></li><li><p>This is actually expected because good initializations require additional time. If the information provided by the <em>k</em>NN graph is enough, random initialization is the best choice.</p></li><li><p>On GIST1M, EFANNA still have the advantage over IEH-ITQ and GNNS (kGraph), which again suggest that GIST1M is a “harder” dataset for ANNS problem. We surprisingly find that GNNS is better than IEH-ITQ which suggests truncated KD-tree (used in EFANNA) is a better choice than hashing (ITQ) used for initialization. It’s interesting to investigate better initialization algorithms.</p></li></ul><h2 id="no-5-the-efanna-library" tabindex="-1"><a class="header-anchor" href="#no-5-the-efanna-library" aria-hidden="true">#</a> <strong>No.5 T</strong>HE <strong>EFANNA L</strong>IBRARY</h2>',21),$={href:"https://github.com/ZJULearning/efanna",target:"_blank",rel:"noopener noreferrer"},ee=s('<h2 id="no-6-conclusion" tabindex="-1"><a class="header-anchor" href="#no-6-conclusion" aria-hidden="true">#</a> <strong>No.6 C</strong>ONCLUSION</h2><ul><li>这篇文章EFANNA主要是 为了 解决两个 问题：</li><li>ANN search problem：</li><li>On ANN search, we use hierarchical structures to provide better initialization for NN-expansion <ul><li>这个主要是提供 更好的 切入点 即 entry points</li></ul></li><li>approximate <em>k</em>NN graph construction problem：</li><li>we use a divide-and-conquer way to construct an initial graph and refine it with NN-descent。 <ul><li>还是 提供一个初始化图；即 K-trees and the nn-descent;</li></ul></li></ul>',2);function ie(ne,ae){const r=l("ExternalLinkIcon"),a=l("router-link");return p(),h("div",null,[R,M,c(" more "),e("div",H,[L,e("ul",null,[C,W,e("li",null,[e("p",null,[i("paper: "),e("a",O,[i("https://whenever5225.github.io/2019/09/21/efanna1/"),n(r)])])]),e("li",null,[e("p",null,[i("code： "),e("a",B,[i("https://github.com/ZJULearning/efanna"),n(r)])])]),P])]),e("nav",U,[e("ul",null,[e("li",null,[n(a,{to:"#no-0-摘要"},{default:t(()=>[i("No.0 摘要")]),_:1})]),e("li",null,[n(a,{to:"#no-1-introduction"},{default:t(()=>[i("No.1 INTRODUCTION")]),_:1})]),e("li",null,[n(a,{to:"#no-2-related-work"},{default:t(()=>[i("No.2 RELATED WORK")]),_:1})]),e("li",null,[n(a,{to:"#no-3-efanna-algorithms-for-ann-search"},{default:t(()=>[i("No.3 EFANNA ALGORITHMS FOR ANN SEARCH")]),_:1}),e("ul",null,[e("li",null,[n(a,{to:"#_3-1-ann-search-with-efanna-index"},{default:t(()=>[i("3.1 ANN search with EFANNA index")]),_:1})]),e("li",null,[n(a,{to:"#_3-2-efanna-index-building-algorithms-i-treebuidling"},{default:t(()=>[i("3.2 EFANNA Index Building Algorithms I : TreeBuidling")]),_:1})]),e("li",null,[n(a,{to:"#_3-3-efanna-index-building-algorithms-ii-approximate-knn-graph-construction"},{default:t(()=>[i("3.3 EFANNA Index Building Algorithms II : Approximate kNN Graph Construction")]),_:1})]),e("li",null,[n(a,{to:"#_3-4-online-index-updating"},{default:t(()=>[i("3.4 Online index updating")]),_:1})])])]),e("li",null,[n(a,{to:"#no-4-experiments"},{default:t(()=>[i("No.4 EXPERIMENTS")]),_:1}),e("ul",null,[e("li",null,[n(a,{to:"#_4-1-data-set-and-experiment-setting"},{default:t(()=>[i("4.1 Data Set and Experiment Setting")]),_:1})]),e("li",null,[n(a,{to:"#_4-2-experiments-on-ann-search"},{default:t(()=>[i("4.2 Experiments on ANN Search")]),_:1})]),e("li",null,[n(a,{to:"#_4-3-experiment-on-approximate-knn-graph-construction"},{default:t(()=>[i("4.3 Experiment on Approximate kNN Graph Construction")]),_:1})]),e("li",null,[n(a,{to:"#_4-4-efanna-with-approximate-knn-graphs"},{default:t(()=>[i("4.4 EFANNA with Approximate kNN Graphs")]),_:1})]),e("li",null,[n(a,{to:"#_4-5-efanna-with-different-number-of-trees"},{default:t(()=>[i("4.5 EFANNA with Different Number of Trees")]),_:1})]),e("li",null,[n(a,{to:"#_4-6-efanna-with-different-number-of-k-in-knngraph"},{default:t(()=>[i("4.6 EFANNA with Different Number of k in kNNGraph")]),_:1})]),e("li",null,[n(a,{to:"#_4-7-ann-search-comparison-with-same-index-size"},{default:t(()=>[i("4.7 ANN Search Comparison with Same Index Size")]),_:1})])])]),e("li",null,[n(a,{to:"#no-5-the-efanna-library"},{default:t(()=>[i("No.5 THE EFANNA LIBRARY")]),_:1})]),e("li",null,[n(a,{to:"#no-6-conclusion"},{default:t(()=>[i("No.6 CONCLUSION")]),_:1})])])]),Q,j,V,J,Z,X,Y,e("ul",null,[e("li",null,[e("a",$,[i("ZJULearning/efanna: fast library for ANN search and KNN graph construction (github.com)"),n(r)])])]),ee])}const re=o(D,[["render",ie],["__file","EFANNA.html.vue"]]);export{re as default};
