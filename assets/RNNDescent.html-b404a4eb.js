import{_ as m}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o,c,d as h,a as l,e as s,w as n,b as a,f as i}from"./app-2a2d189a.js";const N="/assets/image-20240411092250586-9f1158ca.png",u="/assets/image-20240411102354651-f91a4457.png",p="/assets/image-20240411102336800-007723b2.png",d="/assets/image-20240411152432367-4c316f3a.png",g={},_=l("h1",{id:"d-rnndescent",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#d-rnndescent","aria-hidden":"true"},"#"),a(" D-RNNDescent")],-1),b=l("p",null,"D-RNNDescent",-1),k=l("div",{class:"hint-container info"},[l("p",{class:"hint-container-title"},"相关信息"),l("ul",null,[l("li",null,"可能这里要有一些 关于 其它的对于这篇文章的解读"),l("li",null,"我们需要一些链接？"),l("li",null,[a("主要思想："),l("br"),a(" 提出的方法的技术核心是两种算法的结合，这两种算法在第三节中提到：NN-Descent和RNG策略。类似于NN-Descent，RNN-Descent通过逐步改善一个随机初始化的图来构建基于图的索引。但是，RNN-Descent的更新算法同时执行了来自NN-Descent的添加边操作和基于RNG策略的移除边操作。这种方法允许提出的方法直接构建图，而不需要用ANNS来找到邻居候选。此外，提出的更新算法自然保证了图的连通性，这对搜索性能至关重要。")])])],-1),y={class:"table-of-contents"},x=i('<h2 id="no-0-摘要" tabindex="-1"><a class="header-anchor" href="#no-0-摘要" aria-hidden="true">#</a> No.0 摘要</h2><ul><li><mark>总结点</mark></li><li>摘要中的表示 大部分 学者 是在 探索 search 中的 recall and speed；对于 加速索引构建 的 研究较少；</li><li>“无需ANNS” 是什么意思？</li></ul><br><ul><li><mark>原文翻译</mark></li><li>近似最近邻搜索（ANNS）是指寻找与给定查询向量最接近的数据库向量的任务。基于图的ANNS是在准确率和速度之间对于百万级数据集有最佳平衡的一系列方法。</li><li>然而，基于图的方法的缺点是索引构建时间长。最近，许多研究者 在搜索方面中 改进了准确率与速度之间的权衡。然而，关于加速索引构建的研究却很少。</li><li>我们提出了一种快速的图构建算法，相对最近邻下降（RNN-Descent）。RNN-Descent结合了构建近似K-最近邻图（K-NN图）的NN-Descent算法和用于选择对搜索有效的边的RNG策略。</li><li>该算法允许直接构建基于图的索引，无需ANNS。实验结果表明，所提出的方法具有最快的索引构建速度，同时其搜索性能与现有的最先进方法如NSG相媲美。例如，在GIST1M数据集的实验中，所提出方法的构建速度是NSG的2倍。另外，它的构建速度甚至比NN-Descent还要快。</li></ul><h2 id="no-1-introduction" tabindex="-1"><a class="header-anchor" href="#no-1-introduction" aria-hidden="true">#</a> No.1 <strong>INTRODUCTION</strong></h2><ul><li><mark>总结点</mark></li><li>注意一下 NSG 与 NN-Descent</li><li>好像大部分的 基于图的 search 基本 都是 在 贪婪的基础上进行的。</li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>近似最近邻搜索（ANNS）是寻找数据库中与给定查询向量最接近的向量的任务。它在许多应用中非常有用，从图像和文本检索到机器学习，例如k最近邻分类。ANNS在 速度、准确性和内存消耗之间存在权衡。解决ANNS问题的典型方法包括基于哈希的[1, 10]、基于量化的[9, 15]以及基于树的[21, 24]方法。</li><li>近年来，许多研究提出了基于图的近似最近邻搜索（ANNS）方法。基于图的方法通过将数据库向量作为顶点来构造图，图的边将彼此接近的向量连接起来。搜索算法通过沿着指向查询对象的图进行遍历来找到近似最近邻。基于图的ANNS是一系列方法的总称，这些方法为百万级数据集提供了最佳的准确性和速度之间的权衡[17, 20]。</li><li>基于图的方法的弱点是它们的索引构建时间较长。例如，考虑在包含100万个960维向量的GIST1M [15]数据集上构建索引，代表性的基于图的索引HNSW [19]的构建在AWS c6i.4xlarge实例（16 vCPUs，32GB内存）上大约需要20分钟。在许多情况下，加速 索引构建时间非常重要。其中一种情况是数据更新频繁。只有少数研究[22]解决了高效从基于图的索引中删除数据的问题。因此，当从索引中删除大量数据时，用户必须从头开始重建图。快速构建还可以提高找到各种数据集最佳构建参数的效率。尽管这很重要，但关于加速索引构建的研究却很少。</li><li>我们的目标是加快图索引的构建速度。我们将传统的基于图的索引分为两种类型，并展示了每种类型构建速度慢的<strong>原因</strong>。第一种类型是(1)<strong>基于细化(</strong> refinement-based)的方法[7, 8, 12, 13, 17]。这些方法首先构建一个初始的图（例如，通过NN-Descent [5]得到一个近似的K-NN图）。然后，它们使用RNG策略[8, 11, 19]等算法来 改善 K-NN图，以获得最终的图。refinement-based的方法可以通过使用K-NN图来缩小索引的候选边缘，从而快速构建高性能图索引。然而，构建初始的K-NN图需要时间。第二种类型是(2)直接方法[14, 18, 19]，它直接构建图索引。HNSW [19]是这种类型的典型方法。HNSW依次将新向量添加到索引中。在这里，HNSW通过对正在构建的索引执行ANNS来找到新数据的邻居。HNSW是最佳表现的基于图的方法之一。然而，构建高性能图需要精确的ANNS，这是耗时的，因为ANNS的速度和准确性之间存在权衡。</li><li>我们提出了一种新的图索引构建算法，称为Relative NN-Descent（RNN-Descent）。RNN-Descent同时解决了基于细化方法和直接方法的问题。首先，RNN-Descent<strong>直接构造基于图的索引</strong>，避免了构造K-NN图的步骤，这是基于细化方法的问题所在。其次，RNN-Descent使用从NN-Descent派生的算法构建图，这种算法不使用ANNS，这与直接方法不同。该提出方法的技术思想是同时执行基于细化方法中的K-NN图的构造和改进。具体来说，RNN-Descent结合了NN-Descent（一个用于构建K-NN图的算法）和RNG策略（一个用于选择对ANNS有效边的算法）。这一思想允许算法直接构建索引而不使用ANNS。邻域更新算法也自然保留了连通性，这对图索引的性能十分重要。</li><li>我们的实验表明，所提出的方法在搜索性能上与传统方法相当，而在构建速度上则比它们要快得多。例如，在GIST1M数据集上，提出的方法的构建速度大约是NSG [8]（一种最先进的方法之一）的两倍。值得注意的是，提出的方法的构建速度甚至比K-NN图的还要快。我们还在SIFT20M [16]数据集上构建了图，以分析它们在大数据集上的性能。</li><li>我们的贡献如下： <ul><li>我们致力于加速图索引的构建。尽管这一点非常重要，但以往的研究还没有解决加快图构建速度的问题。</li><li>我们提出了一种新的图索引构建算法RNN-Descent。RNN-Descent同时解决了传统基于图的方法的问题：(1)基于细化的方法和(2)直接构建方法。</li><li>我们的实验表明，RNN-Descent是最快的构建算法，而且构建的索引在搜索性能上与传统方法相当。</li></ul></li></ul><h2 id="no-2-related-work" tabindex="-1"><a class="header-anchor" href="#no-2-related-work" aria-hidden="true">#</a> No.2 RELATED WORK</h2><h3 id="_2-1-approximate-nearest-neighbor-search-anns" tabindex="-1"><a class="header-anchor" href="#_2-1-approximate-nearest-neighbor-search-anns" aria-hidden="true">#</a> 2.1 Approximate Nearest Neighbor Search (ANNS)</h3><ul><li><mark>总结点</mark></li><li>ANNS 即涉及到 search</li><li></li></ul><br>',12),S=l("ul",null,[l("li",null,[l("p",null,[l("mark",null,"原文翻译")])]),l("li",null,[l("p",null,"首先，我们确定最近邻搜索（NNS）问题的定义。令查询向量 𝒒 ∈ ℝ^𝑑，数据库向量的数量为 𝑛 ∈ ℤ，数据库向量集合为 X = {𝒙1, ..., 𝒙𝑛} ⊆ ℝ^𝑑，距离函数为 dist : ℝ^𝑑 × ℝ^𝑑 → ℝ。在这里，NNS是一个返回数据库中与查询向量 𝒒 最近的向量 𝒙𝑖* ∈ X 的ID的任务：")]),l("li",null,[l("p",{class:"katex-block"},[l("span",{class:"katex-display"},[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[l("semantics",null,[l("mrow",null,[l("mi",null,"𝑖"),l("mo",null,"∗"),l("mo",null,"="),l("mi",null,"a"),l("mi",null,"r"),l("mi",null,"g"),l("mi",null,"m"),l("mi",null,"i"),l("mi",null,"n"),l("mi",null,"d"),l("mi",null,"i"),l("mi",null,"s"),l("mi",null,"t"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝒒"),l("mo",{separator:"true"},","),l("mi",null,"𝒙"),l("mi",null,"𝑖"),l("mo",{stretchy:"false"},")")]),l("annotation",{encoding:"application/x-tex"}," 𝑖* = argmin dist(𝒒, 𝒙𝑖) ")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.6595em"}}),l("span",{class:"mord mathnormal"},"i"),l("span",{class:"mord"},"∗"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord mathnormal"},"a"),l("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),l("span",{class:"mord mathnormal"},"min"),l("span",{class:"mord mathnormal"},"d"),l("span",{class:"mord mathnormal"},"i"),l("span",{class:"mord mathnormal"},"s"),l("span",{class:"mord mathnormal"},"t"),l("span",{class:"mopen"},"("),l("span",{class:"mord boldsymbol",style:{"margin-right":"0.105em"}},"q"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord boldsymbol",style:{"margin-right":"0.12583em"}},"x"),l("span",{class:"mord mathnormal"},"i"),l("span",{class:"mclose"},")")])])])])])]),l("li",null,[l("p",{class:"katex-block"},[l("span",{class:"katex-display"},[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[l("semantics",null,[l("mrow",null,[l("mi",null,"𝑖"),l("mo",null,"∈"),l("mrow",null,[l("mn",null,"1"),l("mo",{separator:"true"},","),l("mi",{mathvariant:"normal"},"."),l("mi",{mathvariant:"normal"},"."),l("mi",{mathvariant:"normal"},"."),l("mo",{separator:"true"},","),l("mi",null,"𝑛")])]),l("annotation",{encoding:"application/x-tex"}," 𝑖∈ {1,...,𝑛} ")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.6986em","vertical-align":"-0.0391em"}}),l("span",{class:"mord mathnormal"},"i"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"∈"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8389em","vertical-align":"-0.1944em"}}),l("span",{class:"mord"},[l("span",{class:"mord"},"1"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord"},"..."),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal"},"n")])])])])])])]),l("li",null,[l("p",null,"一个简单的NNS解决方法是计算所有数据与查询之间的距离。然而，穷尽搜索的时间复杂度是 𝑂(𝑛𝑑)，对于大规模和高维数据来说这种方法很慢。")]),l("li",null,[l("p",null,[a("为此，许多研究提出了近似NNS（ANNS）作为处理大规模数据的方法。ANNS是一种在牺牲少量准确精度损失的情况下显著提高速度的方法。ANNS方法通常在"),l("strong",null,"精度、速度和内存消耗"),a("之间有一个折衷。因此，适当的方法取决于问题的规模。ANNS方法包括基于哈希的方法[1, 10]、基于量化的方法[9, 15]和基于树的方法[21, 24]。")])])],-1),f=i('<h3 id="_2-2-graph-based-anns" tabindex="-1"><a class="header-anchor" href="#_2-2-graph-based-anns" aria-hidden="true">#</a> 2.2 Graph-Based ANNS</h3><ul><li><mark>总结点</mark></li><li>主要是介绍了；三阶段的东西：</li><li>1-search</li><li>2-KNNG</li><li>3-散化(选边策略) 启发式选边；</li></ul><br><ul><li><mark>原文翻译</mark></li><li>本节描述了基于图的近似最近邻搜索（ANNS）。对于百万级数据集，基于图的方法在精确度和速度之间有最佳的平衡[2, 17, 25]。在搜索之前，基于图的方法会构建一个图，图的顶点代表数据库向量。图的边将彼此接近的向量连接起来。搜索算法通过沿着构建的图朝向查询方向遍历，找到近似最近邻。</li><li>算法1展示了典型的图搜索算法。这里，𝑉 = {1, ..., 𝑛} 是图的顶点集，𝐸 ⊂ 𝑉 × 𝑉 是边集，而 𝐿 ∈ ℤ 是一个超参数。每个𝑣 ∈ 𝑉 对应于一个数据库向量 𝒙𝑣 ∈ ℝ^𝑑。首先，算法初始化候选邻域点集 𝐶 ⊂ 𝑉 (L1)。在while循环的每一步中，算法会从 𝐶 中取出最靠近查询点 𝑢 的未访问点(L3)，然后将 𝑢 的未访问邻居顶点加入到 𝐶 中 (L4-7)。随后，算法从 𝐶 中选出最靠近查询点 𝒒 的 𝐿 个点，确保 𝐶 的大小不超过 𝐿 (L8-9)。当 𝐶 不再更新时，算法终止while循环并从 𝐶 中输出最近邻点(L10-11)。</li></ul><p>​ <img src="'+N+'" alt="image-20240411092250586" loading="lazy"></p><ul><li>K最近邻图（K-NN图）是最直接的图索引之一。K-NN图的每个顶点都有边连接到与其距离最近的前K个顶点。由于构建一个精确的K-NN图非常耗时，因此有几种方法用于构建近似K-NN图。NN-Descent [5] 是最快和最准确的方法之一。NN-Descent通过逐步处理邻居的邻居作为新邻居的候选者，以改进K-NN图。其他方法包括分治法 [4,23]、基于哈希的方法 [6]，以及通过在数据的一个子集上解决ANNS问题来顺序添加顶点 [26]。</li><li>很多方法开发了不同的图结构来提高搜索性能。第一种方法是改进近似K-NN图。因为简单的K-NN图作为ANNS索引的性能差，一些方法对K-NN图进行了精细化，将其优化为高性能的图索引。NSG [8] 从K-NN图中提取边候选，然后使用RNG策略选出必要的边。RNG策略是一个基于邻域间距离选择边的广泛使用的启发式算法。<strong>相关的方法包括</strong>那些关注边之间角度的方法 [7, 17] 或者通过额外的参数调整选择的程度 [14]。第二种方法是直接构建基于图的索引。HNSW [19] 逐步将新数据添加到索引中。构建算法通过解决当前图索引的ANNS问题来找到新顶点的邻居候选。HNSW依次添加数据。这些方法将新增加的数据视作查询向量，并解决当前图索引的ANNS问题，以找到新数据的候选邻居。尽管这些构建方法比通过K-NN图的方法简单，但对于高维数据集来说，在ANNS难以进行的情况下，构建速度却更慢。</li></ul><h2 id="no-3-preliminary" tabindex="-1"><a class="header-anchor" href="#no-3-preliminary" aria-hidden="true">#</a> No.3 PRELIMINARY</h2><h3 id="_3-1-nn-descent" tabindex="-1"><a class="header-anchor" href="#_3-1-nn-descent" aria-hidden="true">#</a> 3.1 NN-Descent</h3><ul><li><mark>总结点</mark></li><li></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>NN-Descent [5] 是一种快速构建近似K-NN图的算法。NN-Descent的基本思想是邻居的邻居很可能再次成为邻居。NN-Descent首先随机初始化图。然后重复执行“join step”来寻找新的邻居候选者，“update step”用于从候选者中选择邻居。</li><li>图1(a)显示了 join 操作的一个示例。为了加快速度，加入操作检查了围绕任意顶点的所有对，而不是检查邻居的邻居。在图1(a)中，顶点𝑣1、𝑣2和𝑣3通过𝑢成为邻居的邻居。因此，加入算法在每一对顶点(𝑣𝑖, 𝑣𝑗) (1 ≤ 𝑖 &lt; 𝑗 ≤ 3)之间添加了一个双向边。</li><li>算法2是NN-Descent加入操作的伪代码。该算法接收图𝐺 = (𝑉, 𝐸)，并返回新边的候选者。在第2行之后的每个循环中，为𝑢的每对邻居双向添加边。NN-Descent为每个邻居分配一个标记，以确定它是否在最近的步骤中新添加的。每个加入步骤仅在任一标记为“新”（L5）的情况下才在邻居对之间添加一条边。这种方法允许加入步骤只检查每对邻居一次，从而加速了加入步骤。由于提出的方法结合了标记，我们将在第4.2节再次详细描述算法。</li></ul><h3 id="_3-2-rng-strategy" tabindex="-1"><a class="header-anchor" href="#_3-2-rng-strategy" aria-hidden="true">#</a> 3.2 RNG Strategy</h3><ul><li><mark>总结点</mark></li><li>选边策略 主要是 这样： 选取 的 新加入的 节点 必须是 在 主点和其邻居 的dist中 与 主点最近才行。才可以加入。</li><li></li><li>好像看着伪代码 是 在 最后的NN-Descent 之后 再进行暴力 选择的；最原始的选边策略；即将 index + 选边 + search 分开；。</li><li>然后 本论文 是 在 index 的 时候 进行 选边；那为什么 不在 index的时候进行选边+搜索呢？仿照Hnsw；</li><li></li></ul><br>',14),D=l("ul",null,[l("li",null,[l("p",null,[l("mark",null,"原文翻译")])]),l("li",null,[l("p",null,[a("RNG 策略[8, 11, 19]是一种用于选择对近邻加速算法（ANNS）有效的图中边的方法。图1(b)显示了 RNG 策略的图像。RNG 策略减少了边的数量，使得每个顶点𝑢的"),l("strong",null,"任意两个邻居𝑣和𝑤满足"),a("以下不等式：")]),l("ul",null,[l("li",null,[l("p",{class:"katex-block"},[l("span",{class:"katex-display"},[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[l("semantics",null,[l("mrow",null,[l("mtext",null,"𝛿"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝑢"),l("mo",{separator:"true"},","),l("mi",null,"𝑣"),l("mo",{stretchy:"false"},")"),l("mo",null,"<"),l("mtext",null,"𝛿"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝑣"),l("mo",{separator:"true"},","),l("mi",null,"𝑤"),l("mo",{stretchy:"false"},")"),l("mo",null,"∧"),l("mtext",null,"𝛿"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝑢"),l("mo",{separator:"true"},","),l("mi",null,"𝑤"),l("mo",{stretchy:"false"},")"),l("mo",null,"<"),l("mtext",null,"𝛿"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝑣"),l("mo",{separator:"true"},","),l("mi",null,"𝑤"),l("mo",{stretchy:"false"},")")]),l("annotation",{encoding:"application/x-tex"}," 𝛿(𝑢, 𝑣) < 𝛿 (𝑣,𝑤) ∧ 𝛿 (𝑢,𝑤) < 𝛿 (𝑣,𝑤) ")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord"},"𝛿"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal"},"u"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),l("span",{class:"mclose"},")"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"<"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord"},"𝛿"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),l("span",{class:"mclose"},")"),l("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),l("span",{class:"mbin"},"∧"),l("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord"},"𝛿"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal"},"u"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),l("span",{class:"mclose"},")"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"<"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord"},"𝛿"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),l("span",{class:"mclose"},")")])])])])])])])]),l("li",null,[l("p",null,"这里，𝛿（𝑢，𝑣）是𝑢和𝑣之间的距离："),l("ul",null,[l("li",null,[l("p",{class:"katex-block"},[l("span",{class:"katex-display"},[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[l("semantics",null,[l("mrow",null,[l("mtext",null,"𝛿"),l("mo",{stretchy:"false"},"("),l("mi",null,"𝑢"),l("mo",{separator:"true"},","),l("mi",null,"𝑣"),l("mo",{stretchy:"false"},")"),l("mo",null,"="),l("mi",null,"d"),l("mi",null,"i"),l("mi",null,"s"),l("mi",null,"t"),l("mo",{stretchy:"false"},"("),l("msub",null,[l("mi",null,"x"),l("mi",null,"u")]),l("mo",{separator:"true"},","),l("msub",null,[l("mi",null,"x"),l("mi",null,"v")]),l("mo",{stretchy:"false"},")")]),l("annotation",{encoding:"application/x-tex"}," 𝛿(𝑢, 𝑣) = dist(x_u, x_v ) ")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord"},"𝛿"),l("span",{class:"mopen"},"("),l("span",{class:"mord mathnormal"},"u"),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),l("span",{class:"mclose"},")"),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord mathnormal"},"d"),l("span",{class:"mord mathnormal"},"i"),l("span",{class:"mord mathnormal"},"s"),l("span",{class:"mord mathnormal"},"t"),l("span",{class:"mopen"},"("),l("span",{class:"mord"},[l("span",{class:"mord mathnormal"},"x"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight"},"u")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mpunct"},","),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal"},"x"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mclose"},")")])])])])])])])]),l("li",null,[l("p",null,"例如，在图1(b)中，顶点𝑣1和𝑣2不满足 𝛿(𝑢, 𝑣2) < 𝛿(𝑣2, 𝑣1)，所以算法删除了从𝑢到𝑣2的边。直观上讲，当𝑣和𝑤彼此太接近时，很可能已经存在𝑣和𝑤之间的边。因此，如果从𝑢到𝑣或𝑤存在一条边，那么从𝑢也可达到另一个顶点。")])],-1),R=i('<p>​ <img src="'+u+'" alt="image-20240411102354651" loading="lazy"></p><ul><li>算法3是RNG策略的伪代码。输入是顶点<strong>𝑢的邻居</strong>候选集𝑈，输出是选定的邻居𝑈&#39;. 首先，算法按照到𝑢的距离升序排序𝑈（L1）。然后，对于𝑈中的每一个顶点𝑣，算法判断输出候选集𝑈&#39;是否包含𝑣（L3-8）。具体来说，对于已经添加到𝑈&#39;中的每一个顶点𝑤，它检查𝑣是否满足约束𝛿(𝑢, 𝑣) &lt; 𝛿(𝑣,𝑤)。如果𝑣通过检查，算法就将𝑣添加到𝑈&#39;（L9-10）。</li></ul><p>​ <img src="'+p+'" alt="image-20240411102336800" loading="lazy"></p><h2 id="no-4-method" tabindex="-1"><a class="header-anchor" href="#no-4-method" aria-hidden="true">#</a> No.4 METHOD</h2><h3 id="_4-1-motivation" tabindex="-1"><a class="header-anchor" href="#_4-1-motivation" aria-hidden="true">#</a> 4.1 Motivation</h3><ul><li><mark>总结点</mark></li><li></li><li>大概优势 就是说：index construction 相比于 NN-Descent 没有经历的KNNG的构造，相对于 hnsw 没有 ANNS 的进行。</li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>传统的基于图的近邻加速搜索（ANNS）方法主要分为两种方法：（1）基于精炼的方法；（2）直接方法。基于精炼的方法首先构造一个近似的K-NN图，然后对其进行精炼，以获得最终的图形索引。然而，构建K-NN图是耗时的，并增加了整体索引构建时间。另一方面，直接方法直接构建基于图的索引，通过在构建中的索引上解决ANNS问题，而不经过K-NN图。然而，这种方法也较慢，因为要构建性能良好的基于图的索引，ANNS的准确性必须很高。</li><li>我们提出了一种新的图构建算法——相对最近邻下降(RNN-Descent)，以解决上述问题。RNN-Descent比基于精炼的方法更快，因为它不需要经过K-NN图。此外，与直接方法相比，提出的方法可以在较短的时间内构造出高性能的基于图的索引，因为它在构造索引时不需要进行ANNS。</li><li>我们提出的方法的技术核心是两种算法的结合，这两种算法在第三节中提到：NN-Descent和RNG策略。类似于NN-Descent，RNN-Descent通过逐步改善一个随机初始化的图来构建基于图的索引。但是，RNN-Descent的更新算法同时执行了来自NN-Descent的添加边操作和基于RNG策略的移除边操作。这种方法允许提出的方法直接构建图，而不需要用ANNS来找到邻居候选。此外，提出的更新算法自然保证了图的连通性，这对搜索性能至关重要。</li><li>第4.2节详细描述了提出的邻居更新算法，而第4.3节讨论了添加反向边的算法以避免次优图。最后，第4.4节描述了整体的构建和搜索算法。</li></ul><h3 id="_4-2-updating-neighbors" tabindex="-1"><a class="header-anchor" href="#_4-2-updating-neighbors" aria-hidden="true">#</a> 4.2 Updating neighbors</h3><ul><li><mark>总结点</mark></li><li></li><li></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>这一节描述了更新邻居的算法。所提方法的思想是同时执行NN-Descent的邻域更新算法和RNG策略的边选择算法。</li><li>图1展示了所提出算法的工作原理。我们考虑一个顶点𝑢的邻居。常规的NN-Descent会在任意两个邻域间添加一条边。例如，在图1(a)中，NN-Descent在𝑣1、𝑣2和𝑣3之间添加双向边。然而，在每一对邻居之间添加边在构建基于图的索引时是没有用的没必要的，因为有些边会被像RNG策略这样的算法去除，正如传统的基于细化的方法所做的那样。因此，所提方法使用RNG策略概念，仅添加必要的边。根据RNG策略，在图1(b)中，边(𝑢, 𝑣2)是不必要的，因为有不等式 𝛿(𝑢, 𝑣2) &gt; 𝛿(𝑣2, 𝑣1)。所提方法同时去除多余的边并添加适当的边。在图1(c)中，算法去除了边(𝑢, 𝑣2)然后代之以插入边(𝑣2, 𝑣1)。也就是说，所提方法结合了NN-Descent和RNG策略。我们的邻居更新算法也保持了图的连通性，这对于提高ANNS的性能是重要的。例如，在图1(c)中，无论更新算法之前还是之后，𝑣2都可以从𝑢到达。</li><li>算法4是邻居更新算法的伪代码。该算法以图𝐺 = (𝑉 , 𝐸)作为输入并更新边集𝐸 ⊂ 𝑉 × 𝑉。该算法的大部分内容与RNG策略相同。其中一个不同之处在于第11行。如果𝑢的邻居𝑣对于某个选定的顶点𝑤不满足不等式，算法将会移除边(𝑢, 𝑣)，并插入边(𝑣,𝑤)作为替代。如果边(𝑤, 𝑣)已经存在，则算法不添加边。然而，在这种情况下，𝑤仍然可以从𝑢达到。</li><li>另一个区别是引入了一个标志，用来确定每个邻居是否在最后一次迭代中新加入的。这种技巧源于原始的NN-Descent算法。如果顶点𝑣和𝑤的标志都是“旧”的，算法会跳过计算它们之间的距离（L5-6）。这是因为如果𝑣和𝑤是旧邻居，算法已经检查过𝑣和𝑤是否满足RNG不等式。在迭代的最后，算法会将所有在 𝑈&#39; 中邻居的标志设置为“旧”（L15）。</li></ul><h3 id="_4-3-adding-reverse-edges" tabindex="-1"><a class="header-anchor" href="#_4-3-adding-reverse-edges" aria-hidden="true">#</a> 4.3 Adding reverse edges</h3><ul><li><mark>总结点</mark></li><li>就是说 虽然 满足了 RNG策略 但是 是一些长边；</li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>4.2节中更新算法的问题是，构建的图很可能会陷入一个性能低下的局部最优状态。这里所说的局部最优是指所有邻居都满足RNG策略的条件，更新算法将不再更新边。次优的图有较长的平均边距离，导致搜索性能不佳。</li><li>我们的解决方案是向次优图中添加反向边。增加一个不满足等式2的新边，使得更新算法能够重启。此外，由于图的反向边很可能比随机选择的边要短，它们适合于使图收敛到一个更好的解决方案。</li><li>算法5是一个添加反向边的伪代码。在这里，输入 𝑅 ∈ Z 是一个控制减少边数量的参数。首先，算法将反向边添加到当前边集 𝐸 (L1)。然后，它移除一些长边，以防止边的数量增加得太多。具体来说，算法减少边，使得入度（L3-5）和出度小于或等于 𝑅 (L6-8)。</li></ul><h3 id="_4-4-overall-algorithm" tabindex="-1"><a class="header-anchor" href="#_4-4-overall-algorithm" aria-hidden="true">#</a> 4.4 Overall algorithm</h3><ul><li><mark>总结点</mark></li><li></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>算法6是整个RNN-Descent算法的伪代码。首先，算法随机初始化图像（L1）。这里，𝑆 ∈ Z 是初始图的出度。随后循环中的每一步逐步改进初始化的图 𝐺。第4.2节中的算法更新边集 𝑇2次（L4-5）。然后，第4.3节中的算法添加反向边，以防止图形变得次优（L6-7）。重复这一系列步骤 𝑇1次后，算法输出最终图（L8）。</li><li>最后，我们描述了为构建索引而设计的搜索算法。RNN-Descent不限制构建图的出度。相反，搜索算法限制了度数的数量。我们按如下方式替换算法1中的L4：</li><li>$<em>𝑈</em> ← top <em>𝐾</em> nearest points to <em>𝑢</em> in {<em>𝑣</em>| (<em>𝑢, 𝑣</em>) ∈ <em>𝐸</em>} $</li><li>这里，𝐾 ∈ Z 是一个决定最大出度的参数。该算法允许用户改变最大出度而不需要重构图。最优的度数取决于数据集，但在构建之前很难知道。相比之下，提出的方法可以在搜索过程中动态地确定最优的出度。</li></ul><h2 id="no-5-experiments" tabindex="-1"><a class="header-anchor" href="#no-5-experiments" aria-hidden="true">#</a> No.5 EXPERIMENTS</h2><h3 id="_5-1-settings" tabindex="-1"><a class="header-anchor" href="#_5-1-settings" aria-hidden="true">#</a> 5.1 Settings</h3><ul><li><mark>总结</mark></li><li></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>本实验测量了每种基于图的方法的构建时间和搜索性能。搜索准确性度量采用的是召回率@1（R@1），其中R@1是找到正确最近邻的查询百分比。我们还使用每秒查询次数（QPS）来衡量搜索速度。一般而言，ANNS方法在搜索时有指定的参数，这些参数控制了速度和准确性之间的平衡。我们通过变化搜索参数，将R@1和QPS绘制在平面上来评估每种方法。</li><li>我们比较了以下算法： <ul><li>NN-Descent [5]: An approximate K-NN graph. We set <em>𝐾</em> =64*, 𝑆* = 10*, 𝐿* = 114*, 𝑅* = 100*,* iter = 10</li><li>NSG [8]: The SOTA of refinement-based approach. NSG uses NN-Descent to construct a K-NN graph. We set <em>𝑅</em> = 32*, 𝐿* =64*,𝐶* = 132. The parameters of NN-Descent are same as above.</li><li>HNSW [19]: The SOTA of the direct approach. We set <em>𝑀</em> = 32*,* efC = 500.</li><li>RNN-Descent: The proposed method. We set <em>𝑆</em> = 20, 𝑅 =96,𝑇1 = 4,𝑇2 = 15.</li></ul></li><li>我们使用了Faiss实现进行比较。我们在AWS c6i.4xlarge实例上进行了实验（16个vCPUs, 32 GB内存）。我们将线程数设置为16。表1总结了我们使用的数据集的属性。</li></ul><h3 id="_5-2-comparison-to-other-methods" tabindex="-1"><a class="header-anchor" href="#_5-2-comparison-to-other-methods" aria-hidden="true">#</a> 5.2 Comparison to other methods</h3><ul><li><mark>总结</mark></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>图2和图3展示了每种基于图的方法的搜索性能和构建时间。请注意，我们仅绘制了帕累托最优点。提出的方法的搜索性能与现有的SOTA方法相当。另一方面，构建时间是所有方法中最短的。我们强调，提出的方法的构建速度比NN-Descent的构建速度要快。这个结果意味着，基于细化的方法至少在使用NN-Descent的情况下，不能比提出的方法更快地构建索引。另一方面，HNSW属于直接方法，可能比K-NN图更快。然而，实验结果显示HNSW在所有方法中构建时间最慢。</li></ul><h3 id="_5-3-degree-distribution" tabindex="-1"><a class="header-anchor" href="#_5-3-degree-distribution" aria-hidden="true">#</a> 5.3 Degree distribution</h3><ul><li><mark>总结</mark></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li>图4和图5显示了每种方法构建的图的入度和出度分布。提出的方法限制图的入度小于或等于𝑅。然而，平均度数比𝑅小得多，大约是20，这个值与现有方法大致相同。提出的方法与现有方法具有可比的内存效率，因为索引的内存消耗与图的平均度数成正比。</li><li>SIFT1M和Deep1M的度分布与NSG的相似。尽管提出的方法没有限制出度，但最大出度约为150。这些结果表明，提出的方法可以自动调整相对简单数据集的出度。另一方面，对于GIST1M数据集，一些顶点具有非常大的出度。这些顶点由于搜索算法在访问它们时需要检查许多邻居，因此会拖慢搜索速度。因此，提出的方法在搜索过程中动态限制出度，以避免检查过多的邻居。此外，提出的方法的输入度相比其他方法具有更集中的峰值。</li><li></li></ul><h3 id="_5-4-ablation-study" tabindex="-1"><a class="header-anchor" href="#_5-4-ablation-study" aria-hidden="true">#</a> 5.4 Ablation study</h3><ul><li><mark>总结</mark></li><li></li><li></li></ul><br><ul><li><mark>原文翻译</mark></li><li></li></ul><h2 id="no-6-conclusion" tabindex="-1"><a class="header-anchor" href="#no-6-conclusion" aria-hidden="true">#</a> No.6 CONCLUSION</h2><ul><li><mark>总结</mark></li><li></li><li></li></ul><br>',40),A=l("li",null,[l("mark",null,"原文翻译")],-1),v={href:"https://github.com/mti-lab/rnn-descent",target:"_blank",rel:"noopener noreferrer"},G=l("li",null,"致谢",-1),w=i('<h2 id="no-7-algorithm" tabindex="-1"><a class="header-anchor" href="#no-7-algorithm" aria-hidden="true">#</a> No.7 Algorithm</h2><h3 id="_7-1-algorithm-1" tabindex="-1"><a class="header-anchor" href="#_7-1-algorithm-1" aria-hidden="true">#</a> 7.1 Algorithm 1</h3><h3 id="_7-4-algorithm-4-updateneighbors-g" tabindex="-1"><a class="header-anchor" href="#_7-4-algorithm-4-updateneighbors-g" aria-hidden="true">#</a> 7.4 Algorithm 4 UpdateNeighbors(𝐺)</h3>',3),K=l("ul",null,[l("li",null,[l("mark",null,"总结")]),l("li",null,"大概就是 说 邻居 要去掉一点 以及加入 几个"),l("li",null,[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("msup",null,[l("mi",null,"U"),l("mn",null,"1")])]),l("annotation",{encoding:"application/x-tex"},"U^1")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8141em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"U"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.8141em"}},[l("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mtight"},"1")])])])])])])])])])]),a(" 是表示 要留下了的邻居。")])],-1),L=l("figure",null,[l("img",{src:d,alt:"image-20240411152432367",tabindex:"0",loading:"lazy"}),l("figcaption",null,"image-20240411152432367")],-1);function M(T,I){const e=t("router-link"),r=t("ExternalLinkIcon");return o(),c("div",null,[_,b,h(" more "),k,l("nav",y,[l("ul",null,[l("li",null,[s(e,{to:"#no-0-摘要"},{default:n(()=>[a("No.0 摘要")]),_:1})]),l("li",null,[s(e,{to:"#no-1-introduction"},{default:n(()=>[a("No.1 INTRODUCTION")]),_:1})]),l("li",null,[s(e,{to:"#no-2-related-work"},{default:n(()=>[a("No.2 RELATED WORK")]),_:1}),l("ul",null,[l("li",null,[s(e,{to:"#_2-1-approximate-nearest-neighbor-search-anns"},{default:n(()=>[a("2.1 Approximate Nearest Neighbor Search (ANNS)")]),_:1})]),l("li",null,[s(e,{to:"#_2-2-graph-based-anns"},{default:n(()=>[a("2.2 Graph-Based ANNS")]),_:1})])])]),l("li",null,[s(e,{to:"#no-3-preliminary"},{default:n(()=>[a("No.3 PRELIMINARY")]),_:1}),l("ul",null,[l("li",null,[s(e,{to:"#_3-1-nn-descent"},{default:n(()=>[a("3.1 NN-Descent")]),_:1})]),l("li",null,[s(e,{to:"#_3-2-rng-strategy"},{default:n(()=>[a("3.2 RNG Strategy")]),_:1})])])]),l("li",null,[s(e,{to:"#no-4-method"},{default:n(()=>[a("No.4 METHOD")]),_:1}),l("ul",null,[l("li",null,[s(e,{to:"#_4-1-motivation"},{default:n(()=>[a("4.1 Motivation")]),_:1})]),l("li",null,[s(e,{to:"#_4-2-updating-neighbors"},{default:n(()=>[a("4.2 Updating neighbors")]),_:1})]),l("li",null,[s(e,{to:"#_4-3-adding-reverse-edges"},{default:n(()=>[a("4.3 Adding reverse edges")]),_:1})]),l("li",null,[s(e,{to:"#_4-4-overall-algorithm"},{default:n(()=>[a("4.4 Overall algorithm")]),_:1})])])]),l("li",null,[s(e,{to:"#no-5-experiments"},{default:n(()=>[a("No.5 EXPERIMENTS")]),_:1}),l("ul",null,[l("li",null,[s(e,{to:"#_5-1-settings"},{default:n(()=>[a("5.1 Settings")]),_:1})]),l("li",null,[s(e,{to:"#_5-2-comparison-to-other-methods"},{default:n(()=>[a("5.2 Comparison to other methods")]),_:1})]),l("li",null,[s(e,{to:"#_5-3-degree-distribution"},{default:n(()=>[a("5.3 Degree distribution")]),_:1})]),l("li",null,[s(e,{to:"#_5-4-ablation-study"},{default:n(()=>[a("5.4 Ablation study")]),_:1})])])]),l("li",null,[s(e,{to:"#no-6-conclusion"},{default:n(()=>[a("No.6 CONCLUSION")]),_:1})]),l("li",null,[s(e,{to:"#no-7-algorithm"},{default:n(()=>[a("No.7 Algorithm")]),_:1}),l("ul",null,[l("li",null,[s(e,{to:"#_7-1-algorithm-1"},{default:n(()=>[a("7.1 Algorithm 1")]),_:1})]),l("li",null,[s(e,{to:"#_7-4-algorithm-4-updateneighbors-g"},{default:n(()=>[a("7.4 Algorithm 4 UpdateNeighbors(𝐺)")]),_:1})])])])])]),x,S,f,D,R,l("ul",null,[A,l("li",null,[a("这篇论文提出了RNN-Descent，这是一种新型的基于图的近邻搜索 索引构建算法。RNN-Descent结合了NN-Descent算法和RNG策略。它同时根据RNN-Descent添加边并基于RNG策略移除边。实验结果表明，RNN-Descent显著加快了索引构建速度，同时保持了与现有最优方法相当的性能。例如，在GIST1M数据集上的实验显示，RNN-Descent构建索引的速度大约是NSG的两倍。我们的源代码已经在 "),l("a",v,[a("https://github.com/mti-lab/rnn-descent"),s(r)]),a(" 上公开。")]),G]),w,K,L])}const O=m(g,[["render",M],["__file","RNNDescent.html.vue"]]);export{O as default};
