import{_ as r}from"./plugin-vue_export-helper-c27b6911.js";import{r as u,o as k,c as d,d as b,a as n,e as a,w as t,b as s,f as l}from"./app-2a2d189a.js";const m="/assets/figure4-1-f7b5153f.png",v="/assets/figure4-2-2a04569d.png",f="/assets/table4-6ad67ee3.png",g="/assets/figure4-3-0a5af6bf.png",y="/assets/figure4-4-945528df.png",_="/assets/figure4-5-52f1f7ab.png",h="/assets/figure4-6-74e997da.png",x="/assets/figure4-7-111ed265.png",w="/assets/figure4-8-d18298aa.png",A="/assets/figure4-9-ffc0a963.png",D="/assets/figure4-10-8ed310cb.png",M="/assets/figure4-11-244c6987.png",S="/assets/figure4-12-b764eb8e.png",C="/assets/figure4-13-fd739cb9.png",U="/assets/figure4-14-e5ab5f8d.png",P="/assets/figure4-15-014dd815.png",B="/assets/figure4-16-9c03e39a.png",z="/assets/figure4-17-e6831c22.png",I="/assets/figure4-18-141a11e2.png",E="/assets/figure4-19-b3a2b3a0.png",R="/assets/figure4-20-5ce13dbc.png",G="/assets/figure4-21-e91b93c6.png",T="/assets/figure4-22-8d2111d8.png",H="/assets/figure4-23-25e44efd.png",$="/assets/figure4-24-05739d45.png",O="/assets/figure4-25-e911f783.png",F="/assets/figure4-26-be76c47f.png",N="/assets/table4-5-826732aa.png",K="/assets/table4-9-c33b4f7c.png",V="/assets/figure4-27-77204283.png",Z="/assets/figure4-28-92fbcc46.png",q="/assets/table4-10-39a99526.png",L="/assets/figure4-29-15e47fc2.png",W="/assets/figure4-30-af194e30.png",X="/assets/table4-11-bffaabb9.png",j="/assets/figure4-31-bd10a74f.png",J="/assets/table4-12-6c3483ac.png",Q="/assets/table4-13-aea3bc42.png",Y="/assets/table4-14-3479f46c.png",nn="/assets/figure4-32-c2318438.png",sn="/assets/figure4-33-0ce68dbd.png",an="/assets/figure4-34-26016deb.png",tn={},en=n("h1",{id:"d-第四章节",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#d-第四章节","aria-hidden":"true"},"#"),s(" D-第四章节")],-1),on=n("p",null,"D-第四章节",-1),pn={class:"table-of-contents"},cn=l('<h2 id="简单介绍主要是基础" tabindex="-1"><a class="header-anchor" href="#简单介绍主要是基础" aria-hidden="true">#</a> 简单介绍主要是基础</h2><div class="hint-container info"><p class="hint-container-title">说明</p><p>主要是各种搜索找的学习；</p><p>主题：CUDA核心GPU编程</p><p>前置条件：</p><ul><li>应具备C++编程知识</li><li>需理解内存管理，如malloc和free</li><li>理解STL及其模板机制</li><li>需配置NVIDIA显卡，型号需为900系列或更高</li><li>所用扩展要求版本为11或更新</li><li>编译器版本不低于11</li><li>CMake版本需在3.18以上</li></ul></div><div class="hint-container info"><p class="hint-container-title">问题</p><ul><li><mark>对于本章节的 寄存器部分需要好好看看</mark></li><li>它涉及到了 每个线程 自己的 寄存器变量，</li></ul></div><h2 id="第4章-全局内存" tabindex="-1"><a class="header-anchor" href="#第4章-全局内存" aria-hidden="true">#</a> 第4章 全局内存</h2><p>Global Memory</p><ul><li><p>WHAT’S IN THIS CHAPTER?</p></li><li><p>学习CUDA内存模型</p></li><li><p>CUDA内存管理</p></li><li><p>全局内存编程</p></li><li><p>探索全局内存访问模式</p></li><li><p>研究全局内存数据布局</p></li><li><p>统一内存编程</p></li><li><p>最大限度地提高全局内存吞吐量</p></li><li><p>在上一章中，你已经了解了线程是如何在GPU中执行的，以及如何通过操作线程束来<br> 优化核函数性能。但是，核函数性能并不是只和线程束的执行有关。回忆一下第3章的内<br> 容，在3.3.2节中，把一个线程块最里面一层的维度设为线程束大小的一半，这导致内存负<br> 载效率的大幅下降。这种性能损失不能用线程束调度或并行性来解释，造成这种性能损失<br> 的真正原因是较差的全局内存访问模式。</p></li><li><p>在本章，我们将剖析核函数与全局内存的联系及其对性能的影响。本章将介绍CUDA<br> 内存模型，并通过分析不同的全局内存访问模式来教你如何通过核函数高效地利用全局内存。</p></li></ul><h2 id="_4-1-cuda内存模型概述" tabindex="-1"><a class="header-anchor" href="#_4-1-cuda内存模型概述" aria-hidden="true">#</a> 4.1 CUDA内存模型概述</h2><ul><li><p>内存的访问和管理是所有编程语言的重要部分。在现代加速器中，内存管理对高性能<br> 计算有着很大的影响。</p></li><li><p>因为多数工作负载被<mark>加载和存储数据的速度所限制</mark>，所以有大量低延迟、高带宽的内<br> 存对性能是十分有利的。然而，大容量、高性能的内存造价高且不容易生产。因此，在现<br> 有的硬件存储子系统下，必须依靠内存模型获得最佳的延迟和带宽。CUDA内存模型结合<br> 了主机和设备的内存系统，展现了完整的内存层次结构，使你能显式地控制数据布局以优<br> 化性能。</p></li></ul><h3 id="_4-1-1-内存层次结构的优点" tabindex="-1"><a class="header-anchor" href="#_4-1-1-内存层次结构的优点" aria-hidden="true">#</a> 4.1.1 内存层次结构的优点</h3><ul><li><p>一般来说，应用程序不会在某一时间点访问任意数据或运行任意代码。应用程序往往<br> 遵循局部性原则，这表明它们可以在任意时间点访问相对较小的局部地址空间。有两种不<br> 同类型的局部性：<br> 时间局部性<br> 空间局部性</p></li><li><p>时间局部性认为如果一个数据位置被引用，那么该数据在较短的时间周期内很可能会<br> 再次被引用，随着时间流逝，该数据被引用的可能性逐渐降低。空间局部性认为如果一个<br> 内存位置被引用，则附近的位置也可能会被引用。</p></li><li><p>现代计算机使用不断改进的低延迟低容量的内存层次结构来优化性能。这种内存层次<br> 结构仅在支持局部性原则的情况下有效。一个内存层次结构由具有不同延迟、带宽和容量<br> 的多级内存组成。通常，随着从处理器到内存延迟的增加，内存的容量也在增加。一个典<br> 型的层次结构如图4-1所示。图4-1底部所示的存储类型通常有如下特点：</p></li></ul><figure><img src="'+m+'" alt="figure4-1" tabindex="0" loading="lazy"><figcaption>figure4-1</figcaption></figure><ul><li><p>更低的每比特位的平均成本</p></li><li><p>更高的容量</p></li><li><p>更高的延迟</p></li><li><p>更少的处理器访问频率</p></li><li><p>CPU和GPU的主存都采用的是DRAM（动态随机存取存储器），而低延迟内存（如<br> CPU一级缓存）使用的则是SRAM（静态随机存取存储器）。内存层次结构中最大且最慢<br> 的级别通常使用磁盘或闪存驱动来实现。在这种内存层次结构中，当数据被处理器频繁使<br> 用时，该数据保存在低延迟、低容量的存储器中；而当该数据被存储起来以备后用时，数<br> 据就存储在高延迟、大容量的存储器中。这种内存层次结构符合大内存低延迟的设想。</p></li><li><p>GPU与CPU在内存层次结构设计中都使用相似的准则和模型。GPU和CPU内存模型的<br> 主要区别是，CUDA编程模型能将内存层次结构更好地呈现给用户，能让我们显式地控制<br> 它的行为.（比如共享内存？）</p></li></ul><h3 id="_4-1-2-cuda内存模型" tabindex="-1"><a class="header-anchor" href="#_4-1-2-cuda内存模型" aria-hidden="true">#</a> 4.1.2 CUDA内存模型</h3><ul><li><p>对于程序员来说，一般有两种类型的存储器：<br> 可编程的：你需要显式地控制哪些数据存放在可编程内存中<br> 不可编程的：你不能决定数据的存放位置，程序将自动生成存放位置以获得良好的性能</p></li><li><p>在CPU内存层次结构中，一级缓存和二级缓存都是不可编程的存储器。另一方面，<br> CUDA内存模型提出了多种可编程内存的类型：<br> 寄存器<br> 共享内存<br> 本地内存<br> 常量内存<br> 纹理内存<br> 全局内存</p></li><li><p>图4-2所示为这些内存空间的层次结构，每种都有不同的作用域、生命周期和缓存行<br> 为。一个核函数中的线程都有自己私有的本地内存。一个线程块有自己的共享内存，对同一线程块中所有线程都可见，其内容持续线程块的整个生命周期。所有线程都可以访问全局内存。所有线程都能访问的只读内存空间有：常量内存空间和纹理内存空间。全局内存、常量内存和纹理内存空间有不同的用途。纹理内存为各种数据布局提供了不同的寻址模式和滤波模式。对于一个应用程序来说，全局内存、常量内存和纹理内存中的内容具有相同的生命周期。</p></li></ul><figure><img src="'+v+`" alt="figure4-2" tabindex="0" loading="lazy"><figcaption>figure4-2</figcaption></figure><h3 id="_4-1-2-1-寄存器-重点" tabindex="-1"><a class="header-anchor" href="#_4-1-2-1-寄存器-重点" aria-hidden="true">#</a> 4.1.2.1 寄存器[重点]</h3><ul><li><p>寄存器是GPU上运行速度最快的内存空间。核函数中声明的一个没有其他修饰符的自变量，通常存储在寄存器中。在核函数声明的数组中，如果用于引用该数组的索引是常量且能在编译时确定，那么该数组也存储在寄存器中。（就是说在核函数中，比如 int pretemp；这样一个变量通常是存在了该线程的寄存器中，）</p></li><li><p>寄存器变量对于每个线程来说都是私有的，一个核函数通常使用寄存器来保存需要频繁访问的线程私有变量。寄存器变量与核函数的生命周期相同。一旦核函数执行完毕，就不能对寄存器变量进行访问了。</p></li><li><p>寄存器是一个在SM中由活跃线程束划分出的较少资源。在Fermi GPU中，每个线程限<br> 制最多拥有63个寄存器。Kepler GPU将该限制扩展至每个线程可拥有255个寄存器。在核函数中使用较少的寄存器将使在SM上有更多的常驻线程块。每个SM上并发线程块越多，使用率和性能就越高。</p></li><li><p>你可以用如下的nvcc编译器选项检查核函数使用的硬件资源情况。下面的命令会输出寄存器的数量、共享内存的字节数以及每个线程所使用的常量内存的字节数。<br> -Xptxas -v,-abi=no</p></li><li><p>如果一个核函数使用了超过硬件限制数量的寄存器，则会用本地内存替代多占用的寄<br> 存器。这种寄存器溢出会给性能带来不利影响。nvcc编译器使用启发式策略来最小化寄存<br> 器的使用，以避免寄存器溢出。我们也可以在代码中为每个核函数显式地加上额外的信息<br> 来帮助编译器进行优化：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> 
<span class="token function">__launch_bounds__</span><span class="token punctuation">(</span>maxThreadsPerBlock<span class="token punctuation">,</span> minBlocksPerMultiprocessor<span class="token punctuation">)</span> 
<span class="token function">kernel</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token comment">// your kernel body</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>maxThreadsPerBlock指出了每个线程块可以包含的最大线程数，这个线程块由核函数<br> 来启动。minBlockPerMultiprocessor是可选参数，指明了在每个SM中预期的最小的常驻线<br> 程块数量。对于给定的核函数，最优的启动边界会因主要架构的版本不同而有所不同。</p></li><li><p>你还可以使用maxrregcount编译器选项，来控制一个编译单元里所有核函数使用的寄<br> 存器的最大数量。在这个例子中：<br> -maxrregcount=32</p></li><li><p>如果使用了指定的启动边界，则这里指定的值（32）将失效。</p></li></ul><h3 id="_4-1-2-2-本地内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-2-本地内存" aria-hidden="true">#</a> 4.1.2.2 本地内存</h3><ul><li>核函数中符合存储在寄存器中但不能进入被该核函数分配的寄存器空间中的变量将溢<br> 出到本地内存中。编译器可能存放到本地内存中的变量有：<br> 在编译时使用未知索引引用的本地数组<br> 可能会占用大量寄存器空间的较大本地结构体或数组<br> 任何不满足核函数寄存器限定条件的变量</li><li>“本地内存”这一名词是有歧义的：溢出到本地内存中的变量本质上与全局内存在同一<br> 块存储区域，因此本地内存访问的特点是高延迟和低带宽，并且如在本章后面的4.3节中<br> 所描述的那样，本地内存访问符合高效内存访问要求。对于计算能力2.0及以上的GPU来<br> 说，本地内存数据也是存储在每个SM的一级缓存和每个设备的二级缓存中。</li></ul><h3 id="_4-1-2-3-共享内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-3-共享内存" aria-hidden="true">#</a> 4.1.2.3 共享内存</h3><ul><li><p>在核函数中使用如下修饰符修饰的变量存放在共享内存中：<br><strong>shared</strong></p></li><li><p>因为共享内存是片上内存，所以与本地内存或全局内存相比，它具有更高的带宽和更<br> 低的延迟。它的使用类似于CPU一级缓存，但它是可编程的。</p></li><li><p>每一个SM都有一定数量的由线程块分配的共享内存。因此，必须非常小心不要过度<br> 使用共享内存，<mark>否则将在不经意间限制活跃线程束的数量</mark>。</p></li><li><p>共享内存在核函数的范围内声明，其生命周期伴随着整个线程块。当一个线程块执行<br> 结束后，其分配的共享内存将被释放并重新分配给其他线程块。</p></li><li><p>共享内存是线程之间相互通信的基本方式。一个块内的线程通过使用共享内存中的数<br> 据可以相互合作。访问共享内存必须同步使用如下调用，该命令是在之前章节中介绍过的<br> CUDA运行时调用：<br> void __syncthreads();</p></li><li><p>该函数设立了一个执行障碍点，即同一个线程块中的所有线程必须在其他线程被允许<br> 执行前达到该处。为线程块里所有线程设立障碍点，这样可以避免潜在的数据冲突。第3<br> 章对数据冲突有详细介绍。当一组未排序的多重访问通过不同的线程访问相同的内存地址<br> 时，这些访问中至少有一个是可写的，这时就会出现数据冲突。_syncthreads也会通过频<br> 繁强制SM到空闲状态来影响性能。</p></li><li><p>SM中的一级缓存和共享内存都使用64KB的片上内存，它通过静态划分，但在运行时<br> 可以通过如下指令进行动态配置：<br> cudaError_t cudaFuncSetCacheConfig(const void* func, enum cudaFuncCache<br> cacheConfig);</p></li><li><p>这个函数在每个核函数的基础上配置了片上内存划分，为func指定的核函数设置了配<br> 置。支持的缓存配置如下：<br> cudaFuncCachePreferNone: no preference (default)<br> cudaFuncCachePreferShared: prefer 48KB shared memory and 16KB L1 cache<br> cudaFuncCachePreferL1: prefer 48KB L1 cache and 16KB shared memory<br> cudaFuncCachePreferEqual: Prefer equal size of L1 cache and shared memory, both 32KB</p></li><li><p>Fermi设备支持前三种配置，Kepler设备支持以上所有配置。</p></li></ul><h3 id="_4-1-2-4-常量内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-4-常量内存" aria-hidden="true">#</a> 4.1.2.4 常量内存</h3><ul><li><p>常量内存驻留在设备内存中，并在每个SM专用的常量缓存中缓存。常量变量用如下<br> 修饰符来修饰：<br><strong>constant</strong></p></li><li><p>常量变量必须在全局空间内和所有核函数之外进行声明。对于所有计算能力的设备，<br> 都只可以声明64KB的常量内存。常量内存是静态声明的，并对同一编译单元中的所有核<br> 函数可见。</p></li><li><p>核函数只能从常量内存中读取数据。因此，常量内存必须在主机端使用下面的函数来<br> 初始化：</p></li><li><p>cudaError_t cudaMemcpyToSymbol(const void* symbol, const void* src,<br> size_t count);</p></li><li><p>这个函数将count个字节从src指向的内存复制到symbol指向的内存中，这个变量存放<br> 在设备的全局内存或常量内存中。在大多数情况下这个函数是同步的。</p></li><li><p>线程束中的所有线程从相同的内存地址中读取数据时，常量内存表现最好。举个例<br> 子，数学公式中的系数就是一个很好的使用常量内存的例子，因为一个线程束中所有的线<br> 程使用相同的系数来对不同数据进行相同的计算。如果线程束里每个线程都从不同的地址<br> 空间读取数据，并且只读一次，那么常量内存中就不是最佳选择，因为每从一个常量内存<br> 中读取一次数据，都会广播给线程束里的所有线程。</p></li></ul><h3 id="_4-1-2-5-纹理内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-5-纹理内存" aria-hidden="true">#</a> 4.1.2.5 纹理内存</h3><ul><li>纹理内存驻留在设备内存中，并在每个SM的只读缓存中缓存。纹理内存是一种通过<br> 指定的只读缓存访问的全局内存。只读缓存包括硬件滤波的支持，它可以将浮点插入作为<br> 读过程的一部分来执行。纹理内存是对二维空间局部性的优化，所以线程束里使用纹理内<br> 存访问二维数据的线程可以达到最优性能。对于一些应用程序来说，这是理想的内存，并<br> 由于缓存和滤波硬件的支持所以有较好的性能优势。然而对于另一些应用程序来说，与全局内存相比，使用纹理内存更慢。</li></ul><h3 id="_4-1-2-6-全局内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-6-全局内存" aria-hidden="true">#</a> 4.1.2.6 全局内存</h3><ul><li><p>全局内存是GPU中最大、延迟最高并且最常使用的内存。global指的是其作用域和生<br> 命周期。它的声明可以在任何SM设备上被访问到，并且贯穿应用程序的整个生命周期。</p></li><li><p>一个全局内存变量可以被静态声明或动态声明。你可以使用如下修饰符在设备代码中<br> 静态地声明一个变量：<br><strong>device</strong></p></li><li><p>在第2章的2.1节中，你已经学习了如何动态分配全局内存。在主机端使用cuda-Malloc<br> 函数分配全局内存，使用cudaFree函数释放全局内存。然后指向全局内存的指针就会作为<br> 参数传递给核函数。全局内存分配空间存在于应用程序的整个生命周期中，并且可以访问<br> 所有核函数中的所有线程。<mark>从多个线程访问全局内存时必须注意。因为线程的执行不能跨<br> 线程块同步，不同线程块内的多个线程并发地修改全局内存的同一位置可能会出现问题，<br> 这将导致一个未定义的程序行为</mark>。</p></li><li><p>全局内存常驻于设备内存中，可通过32字节、64字节或128字节的内存事务进行访<br> 问。这些内存事务必须自然对齐，也就是说，首地址必须是32字节、64字节或128字节的<br> 倍数。优化内存事务对于获得最优性能来说是至关重要的。当一个线程束执行内存加载/<br> 存储时，需要满足的传输数量通常取决于以下两个因素：<br> 跨线程的内存地址分布<br> 每个事务内存地址的对齐方式<br><mark>上面这段话的话意思是：在进行通过内存事务取数据的时候，取多少字节以及开始的首地址字节要求？</mark></p></li><li><p>在一般情况下，用来满足内存请求的事务越多，未使用的字节被传输回的可能性就越<br> 高，这就造成了数据吞吐率的降低。</p></li><li><p>对于一个给定的线程束内存请求，事务数量和数据吞吐率是由设备的计算能力来确定<br> 的。对于计算能力为1.0和1.1的设备，全局内存访问的要求是非常严格的。对于计算能力<br> 高于1.1的设备，由于内存事务被缓存，所以要求较为宽松。缓存的内存事务利用数据局<br> 部性来提高数据吞吐率。</p></li><li><p>接下来的部分将研究如何优化全局内存访问，以及如何最大程度地提高全局内存的数<br> 据吞吐率。</p></li></ul><h3 id="_4-1-2-7-gpu缓存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-7-gpu缓存" aria-hidden="true">#</a> 4.1.2.7 GPU缓存</h3><ul><li><p>跟CPU缓存一样，<mark>GPU缓存是不可编程的内存</mark>。在GPU上有4种缓存：<br> 一级缓存<br> 二级缓存<br> 只读常量缓存<br> 只读纹理缓存</p></li><li><p>每个SM都有一个一级缓存，所有的SM共享一个二级缓存。一级和二级缓存都被用来<br> 在存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。对Fermi GPU和Kepler<br> K40或其后发布的GPU来说，CUDA允许我们配置读操作的数据是使用一级和二级缓存，<br> 还是只使用二级缓存。</p></li><li><p>在CPU上，内存的加载和存储都可以被缓存。但是，在GPU上只有内存加载操作可以<br> 被缓存，内存存储操作不能被缓存。</p></li><li><p>每个SM也有一个只读常量缓存和只读纹理缓存，它们用于在设备内存中提高来自于<br> 各自内存空间内的读取性能。</p></li></ul><h3 id="_4-1-2-8-cuda变量声明总结" tabindex="-1"><a class="header-anchor" href="#_4-1-2-8-cuda变量声明总结" aria-hidden="true">#</a> 4.1.2.8 CUDA变量声明总结</h3><figure><img src="`+f+'" alt="table4" tabindex="0" loading="lazy"><figcaption>table4</figcaption></figure><h3 id="_4-1-2-9-静态全局内存" tabindex="-1"><a class="header-anchor" href="#_4-1-2-9-静态全局内存" aria-hidden="true">#</a> 4.1.2.9 静态全局内存</h3><ul><li>下面的代码说明了如何静态声明一个全局变量。如代码清单4-1所示，一个浮点类型<br> 的全局变量在文件作用域内被声明。在核函数checkGolbal-Variable中，全局变量的值在输<br> 出之后，就发生了改变。在主函数中，全局变量的值是通过函数cudaMemcpyToSymbol初始化的。在执行完checkGlobalVariable函数后，全局变量的值被替换掉了。新的值通过使<br> 用cudaMemcpyFromSymbol函数被复制回主机。</li></ul>',35),ln={class:"hint-container details"},un=n("summary",null,"Click me to view the code!",-1),rn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token macro property"},[n("span",{class:"token directive-hash"},"#"),n("span",{class:"token directive keyword"},"include"),s(),n("span",{class:"token string"},"<cuda_runtime.h>")]),s(`
`),n("span",{class:"token macro property"},[n("span",{class:"token directive-hash"},"#"),n("span",{class:"token directive keyword"},"include"),s(),n("span",{class:"token string"},"<stdio.h>")]),s(`
__device__ `),n("span",{class:"token keyword"},"float"),s(" devData"),n("span",{class:"token punctuation"},";"),s(`
__global__ `),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token function"},"checkGlobalVariable"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// display the original value"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Device: the value of the global variable is %f\\n"'),n("span",{class:"token punctuation"},","),s("devData"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// alter the value"),s(`
 devData `),n("span",{class:"token operator"},"+="),n("span",{class:"token number"},"2.0f"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`),n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// initialize the global variable"),s(`
 `),n("span",{class:"token keyword"},"float"),s(" value "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"3.14f"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMemcpyToSymbol"),n("span",{class:"token punctuation"},"("),s("devData"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("value"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Host: copied %f to the global variable\\n"'),n("span",{class:"token punctuation"},","),s(" value"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// invoke the kernel"),s(`
 checkGlobalVariable `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy the global variable back to the host"),s(`
 `),n("span",{class:"token function"},"cudaMemcpyFromSymbol"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("value"),n("span",{class:"token punctuation"},","),s(" devData"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Host: the value changed by the kernel to %f\\n"'),n("span",{class:"token punctuation"},","),s(" value"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),kn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`用以下指令编译并运行这个例子：
$ nvcc `),n("span",{class:"token operator"},"-"),s("arch"),n("span",{class:"token operator"},"="),s("sm_20 globalVariable"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o globalVariable
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`globalVariable



下面是一组示例输出：
Host`),n("span",{class:"token operator"},":"),s(" copied "),n("span",{class:"token number"},"3.140000"),s(` to the global variable
Device`),n("span",{class:"token operator"},":"),s(" the value of the global variable is "),n("span",{class:"token number"},"3.140000"),s(`
Host`),n("span",{class:"token operator"},":"),s(" the value changed by the kernel to "),n("span",{class:"token number"},"5.140000"),s(`



尽管主机和设备的代码存储在同一个文件中，它们的执行却是完全不同的。即使在同
一文件内可见，主机代码也不能直接访问设备变量。类似地，设备代码也不能直接访问主
机变量。
你可能会认为主机代码可以使用如下代码访问设备的全局变量：
`),n("span",{class:"token function"},"cudaMemcpyToSymbol"),n("span",{class:"token punctuation"},"("),s("devD6ata"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("value"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`


是的，但是注意：
cudaMemcpyToSymbol函数是存在在CUDA运行时API中的，可以偷偷地使用GPU硬件
来执行访问
在这里变量devData作为一个标识符，并不是设备全局内存的变量地址
在核函数中，devData被当作全局内存中的一个变量
cudaMemcpy函数不能使用如下的变量地址传递数据给devData：
`),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("devData"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("value"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},","),s("cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`


你不能在主机端的设备变量中使用运算符“`),n("span",{class:"token operator"},"&"),s(`”，因为它只是一个在GPU上表示物理位
置符号。但是，你可以显式地使用下面的CUDA API调用来获取一个全局变量的地址：
cudaError_t `),n("span",{class:"token function"},"cudaGetSymbolAddress"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s(" devPtr"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"const"),s(),n("span",{class:"token keyword"},"void"),n("span",{class:"token operator"},"*"),s(" symbol"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`

这个函数用来获取与提供设备符号相关的全局内存的物理地址。获得变量devData的
地址后，你可以按如下方式使用cudaMemcpy函数：
`),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("dptr "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token constant"},"NULL"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaGetSymbolAddress"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("dptr"),n("span",{class:"token punctuation"},","),s(" devData"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("dptr"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"&"),s("value"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`

有一个例外，可以直接从主机引用GPU内存：CUDA固定内存。主机代码和设备代码
都可以通过简单的指针引用直接访问固定内存。在下一节中，你将会学习固定内存。
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),dn=l('<ul><li><p><mark>文件作用域中的变量：可见性与可访问性</mark></p></li><li><p>在CUDA编程中，你需要控制主机和设备这两个地方的操作。一般情况下，设备核函<br> 数不能访问主机变量，并且主机函数也不能访问设备变量，即使这些变量在同一文件作用<br> 域内被声明。</p></li><li><p>CUDA运行时API能够访问主机和设备变量，但是这取决于你给正确的函数是否提供<br> 了正确的参数，这样的话才能对正确的变量进行恰当的操作。因为运行时API对某些参数<br> 的内存空间给出了假设，如果传递了一个主机变量，而实际需要的是一个设备变量，或反<br> 之，都将导致不可预知的后果（如你的应用程序崩溃）。</p></li></ul><h2 id="_4-2-内存管理" tabindex="-1"><a class="header-anchor" href="#_4-2-内存管理" aria-hidden="true">#</a> 4.2 内存管理</h2><ul><li>CUDA编程的内存管理与C语言的类似，需要程序员显式地管理主机和设备之间的数<br> 据移动。随着CUDA版本的升级，NVIDIA正系统地实现主机和设备内存空间的统一，但<br> 对于大多数应用程序来说，仍需要手动移动数据。这一领域中的最新进展将在本章的4.2.6<br> 节中进行介绍。现在，工作重点在于如何使用CUDA函数来显式地管理内存和数据移动。 <ul><li>分配和释放设备内存</li><li>在主机和设备之间传输数据</li></ul></li><li>为了达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设<br> 备传输数据和从设备中获取数据。</li></ul><h3 id="_4-2-1-内存分配和释放" tabindex="-1"><a class="header-anchor" href="#_4-2-1-内存分配和释放" aria-hidden="true">#</a> 4.2.1 内存分配和释放</h3><ul><li><p>CUDA编程模型假设了一个包含一个主机和一个设备的异构系统，每一个异构系统都<br> 有自己独立的内存空间。核函数在设备内存空间中运行，CUDA运行时提供函数以分配和<br> 释放设备内存。你可以在主机上使用下列函数分配全局内存：<br><code>cudaError_t cudaMalloc(void **devPtr, size_t count);</code></p></li><li><p>这个函数在设备上分配了count字节的全局内存，并用devptr指针返回该内存的地址。<br> 所分配的内存支持任何变量类型，包括整型、浮点类型变量、布尔类型等。如果cudaMalloc函数执行失败则返回cudaErrorMemoryAllocation。在已分配的全局内存中的值不会<br> 被清除。你需要用从主机上传输的数据来填充所分配的全局内存，或用下列函数将其初始化：<br><code>cudaError_t cudaMemset(void *devPtr, int value, size_t count);</code></p></li><li><p>这个函数用存储在变量value中的值来填充从设备内存地址devPtr处开始的count字节。</p></li><li><p>一旦一个应用程序不再使用已分配的全局内存，那么可以以下代码释放该内存空间：<br><code>cudaError_t cudaFree(void *devPtr);</code></p></li><li><p>这个函数释放了devPtr指向的全局内存，该内存必须在此前使用了一个设备分配函数<br> （如cudaMalloc）来进行分配。否则，它将返回一个错误cudaErrorInvalidDevicePointer。如果地址空间已经被释放，那么cudaFree也返回一个错误。</p></li><li><p>设备内存的分配和释放操作成本较高，所以应用程序应重利用设备内存，以减少对整<br> 体性能的影响。</p></li></ul><h3 id="_4-2-2-内存传输" tabindex="-1"><a class="header-anchor" href="#_4-2-2-内存传输" aria-hidden="true">#</a> 4.2.2 内存传输</h3><ul><li><p>一旦分配好了全局内存，你就可以使用下列函数从主机向设备传输数据：<br><code>cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind);</code></p></li><li><p>这个函数从内存位置src复制了count字节到内存位置dst。变量kind指定了复制的方<br> 向，可以有下列取值：<br> cudaMemcpyHostToHost<br> cudaMemcpyHostToDevice<br> cudaMemcpyDeviceToHost<br> cudaMemcpyDeviceToDevice</p></li><li><p>如果指针dst和src与kind指定的复制方向不一致，那么cudaMemcpy的行为就是未定义<br> 行为。这个函数在大多数情况下都是同步的。</p></li><li><p>代码清单4-2是一个使用cudaMemcpy的例子。这个例子展示了在主机和设备之间来回<br> 地传输数据。使用cudaMalloc分配全局内存，使用cudaMemcpy将数据传输给设备，传输方<br> 向由cudaMemcpyHostToDevice指定。然后使用cudaMemcpy将数据传回主机，方向由<br> cudaMemcpyDeviceToHost指定。</p></li></ul>',7),bn={class:"hint-container details"},mn=n("summary",null,"Click me to view the code!",-1),vn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token macro property"},[n("span",{class:"token directive-hash"},"#"),n("span",{class:"token directive keyword"},"include"),s(),n("span",{class:"token string"},"<cuda_runtime.h>")]),s(`
`),n("span",{class:"token macro property"},[n("span",{class:"token directive-hash"},"#"),n("span",{class:"token directive keyword"},"include"),s(),n("span",{class:"token string"},"<stdio.h>")]),s(`
`),n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" argc"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s("argv"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// set up device"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" dev "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaSetDevice"),n("span",{class:"token punctuation"},"("),s("dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// memory size"),s(`
 `),n("span",{class:"token keyword"},"unsigned"),s(),n("span",{class:"token keyword"},"int"),s(" isize "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},"<<"),n("span",{class:"token number"},"22"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"unsigned"),s(),n("span",{class:"token keyword"},"int"),s(" nbytes "),n("span",{class:"token operator"},"="),s(" isize "),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// get device information"),s(`
 cudaDeviceProp deviceProp`),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaGetDeviceProperties"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("deviceProp"),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"%s starting at "'),n("span",{class:"token punctuation"},","),s(" argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"device %d: %s memory size %d nbyte %5.2fMB\\n"'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},","),s(`
 deviceProp`),n("span",{class:"token punctuation"},"."),s("name"),n("span",{class:"token punctuation"},","),s("isize"),n("span",{class:"token punctuation"},","),s("nbytes"),n("span",{class:"token operator"},"/"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"1024.0f"),n("span",{class:"token operator"},"*"),n("span",{class:"token number"},"1024.0f"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate the host memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("h_a "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nbytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate the device memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("d_a"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_a"),n("span",{class:"token punctuation"},","),s(" nbytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize the host memory"),s(`
 `),n("span",{class:"token keyword"},"for"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"unsigned"),s(),n("span",{class:"token keyword"},"int"),s(" i"),n("span",{class:"token operator"},"="),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s("i"),n("span",{class:"token operator"},"<"),s("isize"),n("span",{class:"token punctuation"},";"),s("i"),n("span",{class:"token operator"},"++"),n("span",{class:"token punctuation"},")"),s(" h_a"),n("span",{class:"token punctuation"},"["),s("i"),n("span",{class:"token punctuation"},"]"),s(),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0.5f"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// transfer data from the host to the device"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_a"),n("span",{class:"token punctuation"},","),s(" h_a"),n("span",{class:"token punctuation"},","),s(" nbytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// transfer data from the device to the host"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("h_a"),n("span",{class:"token punctuation"},","),s(" d_a"),n("span",{class:"token punctuation"},","),s(" nbytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// free memory"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_a"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_a"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// reset device "),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),fn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`使用nvcc编译代码并通过nvprof运行：
$ nvcc `),n("span",{class:"token operator"},"-"),s("O3 memTransfer"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o memTransfer
$ nvprof `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`memTransfer

在一个Fermi M2090上的运行输出结果如下所示：
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"3369"),n("span",{class:"token operator"},"=="),s(" NVPROF is profiling process "),n("span",{class:"token number"},"3369"),n("span",{class:"token punctuation"},","),s(" command"),n("span",{class:"token operator"},":"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`memTransfer
`),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(" memTransfer starting at device "),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(" Tesla M2090 memory size "),n("span",{class:"token number"},"4194304"),s(" nbyte "),n("span",{class:"token number"},"16.00"),s(`MB
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"3369"),n("span",{class:"token operator"},"=="),s(" Profiling application"),n("span",{class:"token operator"},":"),s(),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`memTransfer
`),n("span",{class:"token operator"},"=="),n("span",{class:"token number"},"3369"),n("span",{class:"token operator"},"=="),s(" Profiling result"),n("span",{class:"token operator"},":"),s(`
`),n("span",{class:"token function"},"Time"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"%"),n("span",{class:"token punctuation"},")"),s(` Time Calls Avg Min Max Name
 `),n("span",{class:"token number"},"53.50"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"3.7102"),s("ms "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"3.7102"),s("ms "),n("span",{class:"token number"},"3.7102"),s("ms "),n("span",{class:"token number"},"3.7102"),s("ms "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy DtoH"),n("span",{class:"token punctuation"},"]"),s(`
 `),n("span",{class:"token number"},"46.50"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"3.2249"),s("ms "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"3.2249"),s("ms "),n("span",{class:"token number"},"3.2249"),s("ms "),n("span",{class:"token number"},"3.2249"),s("ms "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy HtoD"),n("span",{class:"token punctuation"},"]"),s(`



从主机到设备的数据传输用HtoD来标记，从设备到主机用DtoH来标记。
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),gn=l('<ul><li>图4-3所示为CPU内存和GPU内存间的连接性能。从图中可以看到GPU芯片和板载<br> GDDR5 GPU内存之间的理论峰值带宽非常高，对于Fermi C2050 GPU来说为144GB/s。<br> CPU和GPU之间通过PCIe Gen2总线相连，这种连接的理论带宽要低得多，为8GB/s（PCIe<br> Gen3总线最大理论限制值是16GB/s）。这种差距意味着如果管理不当的话，主机和设备<br> 间的数据传输会降低应用程序的整体性能。因此，CUDA编程的一个基本原则应是尽可能<br> 地减少主机与设备之间的传输。</li></ul><figure><img src="'+g+'" alt="figure4-3" tabindex="0" loading="lazy"><figcaption>figure4-3</figcaption></figure><h3 id="_4-2-3-固定内存" tabindex="-1"><a class="header-anchor" href="#_4-2-3-固定内存" aria-hidden="true">#</a> 4.2.3 固定内存</h3><ul><li><p>分配的主机内存默认是pageable（可分页），它的意思也就是因页面错误导致的操<br> 作，该操作按照操作系统的要求将主机虚拟内存上的数据移动到不同的物理位置。虚拟内<br> 存给人一种比实际可用内存大得多的假象，就如同一级缓存好像比实际可用的片上内存大<br> 得多一样。</p></li><li><p>GPU不能在可分页主机内存上安全地访问数据，因为当主机操作系统在物理位置上移<br> 动该数据时，它无法控制。当从可分页主机内存传输数据到设备内存时，CUDA驱动程序<br> 首先分配临时页面锁定的或固定的主机内存，将主机源数据复制到固定内存中，然后从固<br> 定内存传输数据给设备内存，如图4-4左边部分所示：</p></li></ul><figure><img src="'+y+`" alt="figure4-4" tabindex="0" loading="lazy"><figcaption>figure4-4</figcaption></figure><ul><li><p>CUDA运行时允许你使用如下指令直接分配固定主机内存：<br><code> cudaError_t cudaMallocHost(void **devPtr, size_t count);</code></p></li><li><p>这个函数分配了count字节的主机内存，这些内存是页面锁定的并且对设备来说是可<br> 访问的。由于固定内存能被设备直接访问，所以它能用比可分页内存高得多的带宽进行读<br> 写。然而，分配过多的固定内存可能会降低主机系统的性能，因为它减少了用于存储虚拟<br> 内存数据的可分页内存的数量，其中分页内存对主机系统是可用的。</p></li><li><p>下面的代码段用来分配固定主机内存，其中含错误检查和基本错误处理：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>cudaError_t status <span class="token operator">=</span> <span class="token function">cudaMallocHost</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>h_aPinned<span class="token punctuation">,</span> bytes<span class="token punctuation">)</span><span class="token punctuation">;</span> 
<span class="token keyword">if</span> <span class="token punctuation">(</span>status <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token function">fprintf</span><span class="token punctuation">(</span><span class="token constant">stderr</span><span class="token punctuation">,</span> <span class="token string">&quot;Error returned from pinned host memory allocation\\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token function">exit</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,7),yn=n("li",null,[s("固定主机内存必须通过下述指令来释放："),n("br"),n("code",null,"cudaError_t cudaFreeHost(void *ptr);")],-1),_n=n("br",null,null,-1),hn={href:"http://Wrox.xn--compinMemTransfer-gy50apsv9jg80nd66i.cu",target:"_blank",rel:"noopener noreferrer"},xn=n("br",null,null,-1),wn={href:"http://pinMemTransfer.cu",target:"_blank",rel:"noopener noreferrer"},An=n("br",null,null,-1),Dn=n("li",null,[s("下面的输出清楚地说明了使用固定内存与使用memTransfer生成的输出相比性能提升"),n("br"),s(" 了很多。在这个平台上，使用可分页主机内存时传输最初耗时总计为6.94ms，但是现在通"),n("br"),s(" 过固定主机内存只用了5.3485ms。")],-1),Mn=l(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ nvprof <span class="token punctuation">.</span><span class="token operator">/</span>pinMemTransfer
<span class="token operator">==</span><span class="token number">3425</span><span class="token operator">==</span> NVPROF is profiling process <span class="token number">3425</span><span class="token punctuation">,</span> command<span class="token operator">:</span> <span class="token punctuation">.</span><span class="token operator">/</span> pinMemTransfer
<span class="token punctuation">.</span><span class="token operator">/</span> pinMemTransfer starting at device <span class="token number">0</span><span class="token operator">:</span> Tesla M2090 memory size <span class="token number">4194304</span> 
 nbyte <span class="token number">16.00</span>MB
<span class="token operator">==</span><span class="token number">3425</span><span class="token operator">==</span> Profiling application<span class="token operator">:</span> <span class="token punctuation">.</span><span class="token operator">/</span> pinMemTransfer
<span class="token operator">==</span><span class="token number">3425</span><span class="token operator">==</span> Profiling result<span class="token operator">:</span>
<span class="token function">Time</span> <span class="token punctuation">(</span><span class="token operator">%</span><span class="token punctuation">)</span> Time Calls Avg Min Max Name
 <span class="token number">52.34</span><span class="token operator">%</span> <span class="token number">2.7996</span>ms <span class="token number">1</span> <span class="token number">2.7996</span>ms <span class="token number">2.7996</span>ms <span class="token number">2.7996</span>ms <span class="token punctuation">[</span>CUDA memcpy HtoD<span class="token punctuation">]</span>
 <span class="token number">47.66</span><span class="token operator">%</span> <span class="token number">2.5489</span>ms <span class="token number">1</span> <span class="token number">2.5489</span>ms <span class="token number">2.5489</span>ms <span class="token number">2.5489</span>ms <span class="token punctuation">[</span>CUDA memcpy DtoH<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><mark>主机与设备间的内存传输</mark></li><li>与可分页内存相比，固定内存的分配和释放成本更高，但是它为大规模数据传输提供<br> 了更高的传输吞吐量。</li><li>相对于可分页内存，使用固定内存获得的加速取决于设备计算能力。例如，当传输超<br> 过10MB的数据时，在Fermi设备上使用固定内存通常是更好的选择。</li><li>将许多小的传输批处理为一个更大的传输能提高性能，因为它减少了单位传输消耗。</li><li>主机和设备之间的数据传输有时可以与内核执行重叠。关于这个话题将在第6章中了<br> 解更多。你应该尽可能地减少或重叠主机和设备间的数据传输。</li></ul><h3 id="_4-2-4-零拷贝内存" tabindex="-1"><a class="header-anchor" href="#_4-2-4-零拷贝内存" aria-hidden="true">#</a> 4.2.4 零拷贝内存</h3>`,3),Sn=l("<li><p>通常来说，主机不能直接访问设备变量，同时设备也不能直接访问主机变量。但有一<br> 个例外：零拷贝内存。主机和设备都可以访问零拷贝内存。</p></li><li><p>GPU线程可以直接访问零拷贝内存。在CUDA核函数中使用零拷贝内存有以下几个优<br> 势。<br> ·当设备内存不足时可利用主机内存<br> 避免主机和设备间的显式数据传输<br> ·提高PCIe传输率</p></li><li><p>当使用零拷贝内存来共享主机和设备间的数据时，你必须同步主机和设备间的内存访<br> 问，同时更改主机和设备的零拷贝内存中的数据将导致不可预知的后果.</p></li><li><p>零拷贝内存是固定（不可分页）内存，该内存映射到设备地址空间中。你可以通过下<br> 列函数创建一个到固定内存的映射：<br> cudaError_t cudaHostAlloc(void **pHost, size_t count, unsigned int flags);</p></li><li><p>这个函数分配了count字节的主机内存，该内存是页面锁定的且设备可访问的。用这<br> 个函数分配的内存必须用cudaFreeHost函数释放。flags参数可以对已分配内存的特殊属性<br> 进一步进行配置：<br> ·cudaHostAllocDefalt<br> cudaHostAllocPortable<br> cudaHostAllocWriteCombined<br> cudaHostAllocMapped</p></li><li><p>cudaHostAllocDefault函数使cudaHostAlloc函数的行为与cudaMallocHost函数一致。设<br> 置cudaHostAllocPortable函数可以返回能被所有CUDA上下文使用的固定内存，而不仅是执<br> 行内存分配的那一个。标志cudaHostAllocWriteCombined返回写结合内存，该内存可以在<br> 某些系统配置上通过PCIe总线上更快地传输，但是它在大多数主机上不能被有效地读取。<br> 因此，写结合内存对缓冲区来说是一个很好的选择，该内存通过设备使用映射的固定内存<br> 或主机到设备的传输。零拷贝内存的最明显的标志是cudaHostAllocMapped，该标志返<br> 回，可以实现主机写入和设备读取被映射到设备地址空间中的主机内存。</p></li><li><p>你可以使用下列函数获取映射到固定内存的设备指针：<br> cudaError_t cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags);</p></li><li><p>该函数返回了一个在pDevice中的设备指针，该指针可以在设备上被引用以访问映射<br> 得到的固定主机内存。如果设备不支持映射得到的固定内存，该函数将失效。flag将留作<br> 以后使用。现在，它必须被置为0。</p></li><li><p>在进行频繁的读写操作时，使用零拷贝内存作为设备内存的补充将显著降低性能。因<br> 为每一次映射到内存的传输必须经过PCIe总线。与全局内存相比，延迟也显著增加。你可<br> 以使用在第2章中使用过的sumArrays核函数，来确认零拷贝内存的性能：<br><strong>global</strong> void sumArraysZeroCopy(float *A, float *B, float *C, const int N) {<br> int i = blockIdx.x * blockDim.x + threadIdx.x;<br> if (i &lt; N) C[i] = A[i] + B[i];<br> }</p></li><li><p>为了测试零拷贝内存读操作的性能，你可以给数组A和B分配零拷贝内存，并在设备<br> 内存上为数组C分配空间。代码清单4-3中有该操作的主函数。你可以从Wrox.com中下载<br> sumArrayZerocopy.cu的源代码。</p></li><li><p>主函数包含了两部分：第一部分为从设备内存加载数据及存储数据到设备内存；第二<br> 部分为从零拷贝内存加载数据，并将数据存储到设备内存中。开始时，你需要检查一下设<br> 备是否支持固定内存映射。</p></li><li><p>为了允许核函数从零拷贝内存中读取数据，你需要将数组A和B分配作为映射的固定<br> 内存。然后可以直接在主机上初始化数组A和B，不需要将它们传输给设备内存。接下<br> 来，你获得了供核函数使用映射的固定内存的设备指针。一旦内存被分配并初始化，你就<br> 可以调用核函数了。</p></li>",12),Cn={href:"http://sumArrayZerocopy.cu",target:"_blank",rel:"noopener noreferrer"},Un={class:"hint-container details"},Pn=n("summary",null,"Click me to view the code!",-1),Bn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" argc"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s("argv"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// part 0: set up device and array"),s(`
 `),n("span",{class:"token comment"},"// set up device"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" dev "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaSetDevice"),n("span",{class:"token punctuation"},"("),s("dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// get device properties"),s(`
 cudaDeviceProp deviceProp`),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaGetDeviceProperties"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("deviceProp"),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// check if support mapped memory"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"!"),s("deviceProp"),n("span",{class:"token punctuation"},"."),s("canMapHostMemory"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Device %d does not support mapping CPU host memory!\\n"'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"exit"),n("span",{class:"token punctuation"},"("),s("EXIT_SUCCESS"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Using Device %d: %s "'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},","),s(" deviceProp"),n("span",{class:"token punctuation"},"."),s("name"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up date size of vectors"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" ipower "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"10"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(" ipower "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" nElem "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},"<<"),s("ipower"),n("span",{class:"token punctuation"},";"),s(`
 size_t nBytes `),n("span",{class:"token operator"},"="),s(" nElem "),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("ipower "),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"18"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Vector size %d power %d nbytes %3.0f KB\\n"'),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},","),s(`\\
 ipower`),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),s("nBytes"),n("span",{class:"token operator"},"/"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"1024.0f"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(),n("span",{class:"token keyword"},"else"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"Vector size %d power %d nbytes %3.0f MB\\n"'),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},","),s(`\\
 ipower`),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),s("nBytes"),n("span",{class:"token operator"},"/"),n("span",{class:"token punctuation"},"("),n("span",{class:"token number"},"1024.0f"),n("span",{class:"token operator"},"*"),n("span",{class:"token number"},"1024.0f"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(`
 `),n("span",{class:"token comment"},"// part 1: using device memory"),s(`
 `),n("span",{class:"token comment"},"// malloc host memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("h_A"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),s("h_B"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),s("hostRef"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),s("gpuRef"),n("span",{class:"token punctuation"},";"),s(`
 h_A `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 h_B `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 hostRef `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 gpuRef `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize data at host side"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"memset"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"memset"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// add vector at host side for result checks"),s(`
 `),n("span",{class:"token function"},"sumArraysOnHost"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" h_B"),n("span",{class:"token punctuation"},","),s(" hostRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// malloc device global memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("d_A"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),s("d_B"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token operator"},"*"),s("d_C"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_B"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// transfer data from host to device"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_B"),n("span",{class:"token punctuation"},","),s(" h_B"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up execution configuration"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" iLen "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"512"),n("span",{class:"token punctuation"},";"),s(`
 dim3 `),n("span",{class:"token function"},"block"),s(),n("span",{class:"token punctuation"},"("),s("iLen"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 dim3 `),n("span",{class:"token function"},"grid"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("nElem"),n("span",{class:"token operator"},"+"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// invoke kernel at host side"),s(`
 sumArrays `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_B"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy kernel result back to host side"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// check device results"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// free device global memory"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_B"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// part 2: using zerocopy memory for array A and B"),s(`
 `),n("span",{class:"token comment"},"// allocate zerocpy memory"),s(`
 `),n("span",{class:"token keyword"},"unsigned"),s(),n("span",{class:"token keyword"},"int"),s(" flags "),n("span",{class:"token operator"},"="),s(" cudaHostAllocMapped"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaHostAlloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" flags"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaHostAlloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("h_B"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" flags"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize data at host side"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"memset"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"memset"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// pass the pointer to device"),s(`
 `),n("span",{class:"token function"},"cudaHostGetDevicePointer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),s("h_A"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaHostGetDevicePointer"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_B"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),s("h_B"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// add at host side for result checks"),s(`
 `),n("span",{class:"token function"},"sumArraysOnHost"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" h_B"),n("span",{class:"token punctuation"},","),s(" hostRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token comment"},"// execute kernel with zero copy memory"),s(`
 sumArraysZeroCopy `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s("grid"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_B"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy kernel result back to host side"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// check device results"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// free memory"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFreeHost"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFreeHost"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// reset device"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),zn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`用下列指令进行编译：
$ nvcc `),n("span",{class:"token operator"},"-"),s("O3 "),n("span",{class:"token operator"},"-"),s("arch"),n("span",{class:"token operator"},"="),s("sm_20 sumArrayZerocpy"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o sumZerocpy

使用nvprof收集配置文件信息。来自Fermi M2090的示例输出总结如下：
$ nvprof `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`sumZerocpy 
Using Device `),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(" Tesla M2090 Vector size "),n("span",{class:"token number"},"1024"),s(" power "),n("span",{class:"token number"},"10"),s(" nbytes "),n("span",{class:"token number"},"4"),s(` KB
`),n("span",{class:"token function"},"Time"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"%"),n("span",{class:"token punctuation"},")"),s(` Time Calls Avg Min Max Name
 `),n("span",{class:"token number"},"27.18"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"3.7760u"),s("s "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"3.7760u"),s("s "),n("span",{class:"token number"},"3.7760u"),s("s "),n("span",{class:"token number"},"3.7760u"),s(`s sumArraysZeroCopy
 `),n("span",{class:"token number"},"11.80"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"1.6390u"),s("s "),n("span",{class:"token number"},"1"),s(),n("span",{class:"token number"},"1.6390u"),s("s "),n("span",{class:"token number"},"1.6390u"),s("s "),n("span",{class:"token number"},"1.6390u"),s(`s sumArrays
 `),n("span",{class:"token number"},"25.56"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"3.5520u"),s("s "),n("span",{class:"token number"},"3"),s(),n("span",{class:"token number"},"1.1840u"),s("s "),n("span",{class:"token number"},"1.0240u"),s("s "),n("span",{class:"token number"},"1.5040u"),s("s "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy HtoD"),n("span",{class:"token punctuation"},"]"),s(`
 `),n("span",{class:"token number"},"35.47"),n("span",{class:"token operator"},"%"),s(),n("span",{class:"token number"},"4.9280u"),s("s "),n("span",{class:"token number"},"2"),s(),n("span",{class:"token number"},"2.4640u"),s("s "),n("span",{class:"token number"},"2.4640u"),s("s "),n("span",{class:"token number"},"2.4640u"),s("s "),n("span",{class:"token punctuation"},"["),s("CUDA memcpy DtoH"),n("span",{class:"token punctuation"},"]"),s(`

比较sumArraysZeroCopy核函数和sumArrays核函数的运行时间。当处理`),n("span",{class:"token number"},"1024"),s(`个元素
时，从零拷贝内存读取的核函数运行时间比只使用设备内存的核函数慢了`),n("span",{class:"token number"},"2.31"),s(`倍。还要注
意，从设备到主机传输数据的时间（DtoH），也需要计算在两个核函数的运行时间中。
因为它们都使用cudaMemcpy来更新主机端数据，计算结果在设备上被执行。

接下来，你可以通过运行下列命令检验使用不同大小的数组的性能，示例结果总结在
表`),n("span",{class:"token number"},"4"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"3"),s(`中。
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("sumZerocopy "),n("span",{class:"token operator"},"<"),s("size"),n("span",{class:"token operator"},"-"),s("log"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"2"),n("span",{class:"token operator"},">"),s(`


注：`),n("span",{class:"token number"},"1."),s(`使用Tesla M2090运行并使能一级缓存。
`),n("span",{class:"token number"},"2."),s("减度比＝读取零拷贝内存的运行时间"),n("span",{class:"token operator"},"/"),s(`读取设备内存的运行时间。
从结果中可以看出，如果你想共享主机和设备端的少量数据，零拷贝内存可能会是一
个不错的选择，因为它简化了编程并且有较好的性能。对于由PCIe总线连接的离散GPU上
的更大数据集来说，零拷贝内存不是一个好的选择，它会导致性能的显著下降。

`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),In=l('<ul><li><mark>零拷贝内存</mark></li><li>有两种常见的异构计算系统架构：集成架构和离散架构。</li><li>在集成架构中，CPU和GPU集成在一个芯片上，并且在物理地址上共享主存。在这种<br> 架构中，由于无须在PCIe总线上备份，所以零拷贝内存在性能和可编程性方面可能更佳。</li><li>对于通过PCIe总线将设备连接到主机的离散系统而言，零拷贝内存只在特殊情况下有<br> 优势。</li><li>因为映射的固定内存在主机和设备之间是共享的，你必须同步内存访问来避免任何潜<br> 在的数据冲突，这种数据冲突一般是由多线程异步访问相同的内存而引起的。</li><li>注意不要过度使用零拷贝内存。由于其延迟较高，从零拷贝内存中读取设备核函数可<br> 能很慢</li></ul><h3 id="_4-2-5-统一虚拟寻址" tabindex="-1"><a class="header-anchor" href="#_4-2-5-统一虚拟寻址" aria-hidden="true">#</a> 4.2.5 统一虚拟寻址</h3><ul><li>计算能力为2.0及以上版本的设备支持一种特殊的寻址方式，称为统一虚拟寻址<br> （UVA）。UVA，在CUDA 4.0中被引入，支持64位Linux系统。有了UVA，主机内存和设备<br> 内存可以共享同一个虚拟地址空间，如图4-5所示。</li></ul><figure><img src="'+_+`" alt="figure4-5" tabindex="0" loading="lazy"><figcaption>figure4-5</figcaption></figure><ul><li><p>在UVA之前，你需要管理哪些指针指向主机内存和哪些指针指向设备内存。有了<br> UVA，由指针指向的内存空间对应用程序代码来说是透明的。</p></li><li><p>通过UVA，由cudaHostAlloc分配的固定主机内存具有相同的主机和设备指针。因此，<br> 可以将返回的指针直接传递给核函数。回忆前一节中的零拷贝例子，可以知道以下几个方<br> 面。<br> 分配映射的固定主机内存<br> 使用CUDA运行时函数获取映射到固定内存的设备指针<br> 将设备指针传递给核函数</p></li><li><p>有了UVA，无须获取设备指针或管理物理上数据完全相同的两个指针。UVA会进一步<br> 简化前一节中的sumArrayZerocpy.cu示例：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token comment">// allocate zero-copy memory at the host side</span>
<span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>h_A<span class="token punctuation">,</span> nBytes<span class="token punctuation">,</span> cudaHostAllocMapped<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaHostAlloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>h_B<span class="token punctuation">,</span> nBytes<span class="token punctuation">,</span> cudaHostAllocMapped<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// initialize data at the host side</span>
<span class="token function">initialData</span><span class="token punctuation">(</span>h_A<span class="token punctuation">,</span> nElem<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">initialData</span><span class="token punctuation">(</span>h_B<span class="token punctuation">,</span> nElem<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// invoke the kernel with zero-copy memory</span>
sumArraysZeroCopy<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>h_A<span class="token punctuation">,</span> h_B<span class="token punctuation">,</span> d_C<span class="token punctuation">,</span> nElem<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,6),En=n("br",null,null,-1),Rn=n("br",null,null,-1),Gn={href:"http://sumArrayZerocpyUVA.cu",target:"_blank",rel:"noopener noreferrer"},Tn=n("li",null,[n("p",null,[s("执行与前一节相同的测试将产生相同的性能结果。使用更少的代码取得相同的结果，"),n("br"),s(" 这提高了应用程序的可读性和可维护性。")])],-1),Hn=l('<h3 id="_4-2-6-统一内存寻址" tabindex="-1"><a class="header-anchor" href="#_4-2-6-统一内存寻址" aria-hidden="true">#</a> 4.2.6 统一内存寻址</h3><ul><li><p>在CUDA 6.0中，引入了“统一内存寻址”这一新特性，它用于简化CUDA编程模型中的<br> 内存管理。统一内存中创建了一个托管内存池，内存池中已分配的空间可以用相同的内存<br> 地址（即指针）在CPU和GPU上进行访问。底层系统在统一内存空间中自动在主机和设备<br> 之间进行数据传输。这种数据传输对应用程序是透明的，这大大简化了程序代码。</p></li><li><p>统一内存寻址依赖于UVA的支持，但它们是完全不同的技术。UVA为系统中的所有处<br> 理器提供了一个单一的虚拟内存地址空间。但是，UVA不会自动将数据从一个物理位置转<br> 移到另一个位置，这是统一内存寻址的一个特有功能。</p></li><li><p>统一内存寻址提供了一个“单指针到数据”模型，在概念上它类似于零拷贝内存。但是<br> 零拷贝内存在主机内存中进行分配，因此，由于受到在PCIe总线上访问零拷贝内存的影<br> 响，核函数的性能将具有较高的延迟。另一方面，统一内存寻址将内存和执行空间分离，<br> 因此可以根据需要将数据透明地传输到主机或设备上，以提升局部性和性能。</p></li><li><p>托管内存指的是由底层系统自动分配的统一内存，与特定于设备的分配内存可以互操<br> 作，如它们的创建都使用cudaMalloc程序。因此，你可以在核函数中使用两种类型的内<br> 存：由系统控制的托管内存，以及由应用程序明确分配和调用的未托管内存。所有在设备<br> 内存上有效的CUDA操作也同样适用于托管内存。其主要区别是主机也能够引用和访问托<br> 管内存。</p></li><li><p>托管内存可以被静态分配也可以被动态分配。可以通过添加__managed__注释，静态<br> 声明一个设备变量作为托管变量。但这个操作只能在文件范围和全局范围内进行。该变量<br> 可以从主机或设备代码中直接被引用：<br><strong>device</strong> <strong>managed</strong> int y;</p></li><li><p>还可以使用下述的CUDA运行时函数动态分配托管内存：<br> cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flags=0);</p></li><li><p>这个函数分配size字节的托管内存，并用devPtr返回一个指针。该指针在所有设备和<br> 主机上都是有效的。使用托管内存的程序行为与使用未托管内存的程序副本行为在功能上<br> 是一致的。但是，使用托管内存的程序可以利用自动数据传输和重复指针消除功能。</p></li><li><p>在CUDA 6.0中，设备代码不能调用cudaMallocManaged函数。所有的托管内存必须在<br> 主机端动态声明或者在全局范围内静态声明。</p></li><li><p>在4.5节中，你将有机会亲自体验CUDA的统一内存寻址。</p></li></ul><h2 id="_4-3-内存访问模式" tabindex="-1"><a class="header-anchor" href="#_4-3-内存访问模式" aria-hidden="true">#</a> 4.3 内存访问模式</h2><ul><li><p>大多数设备端数据访问都是从全局内存开始的，并且多数GPU应用程序容易受内存带<br> 宽的限制。因此，最大限度地利用全局内存带宽是调控核函数性能的基本。如果不能正确<br> 地调控全局内存的使用，其他优化方案很可能也收效甚微。</p></li><li><p>为了在读写数据时达到最佳的性能，内存访问操作必须满足一定的条件。CUDA执行<br> 模型的显著特征之一就是指令必须以线程束为单位进行发布和执行。存储操作也是同样。<br> 在执行内存指令时，线程束中的每个线程都提供了一个正在加载或存储的内存地址。在线<br> 程束的32个线程中，每个线程都提出了一个包含请求地址的单一内存访问请求，它并由一<br> 个或多个设备内存传输提供服务。根据线程束中内存地址的分布，内存访问可以被分成不<br> 同的模式。在本节中，你将学习到不同的内存访问模式，并学习如何实现最佳的全局内存<br> 访问。</p></li></ul><h3 id="_4-3-1-对齐与合并访问" tabindex="-1"><a class="header-anchor" href="#_4-3-1-对齐与合并访问" aria-hidden="true">#</a> 4.3.1 对齐与合并访问</h3><ul><li><p>如图4-6所示，全局内存通过缓存来实现加载/存储。全局内存是一个逻辑内存空间，<br> 你可以通过核函数访问它。所有的应用程序数据最初存在于DRAM上，即物理设备内存<br> 中。核函数的内存请求通常是在DRAM设备和片上内存间以128字节或32字节内存事务来<br> 实现的。</p></li><li><p>所有对全局内存的访问都会通过二级缓存，也有许多访问会通过一级缓存，这取决于<br> 访问类型和GPU架构。如果这两级缓存都被用到，那么内存访问是由一个128字节的内存<br> 事务实现的。如果只使用了二级缓存，那么这个内存访问是由一个32字节的内存事务实现<br> 的。对全局内存缓存其架构，如果允许使用一级缓存，那么可以在编译时选择启用或禁用<br> 一级缓存。</p></li><li><p>一行一级缓存是128个字节，它映射到设备内存中一个128字节的对齐段。如果线程束<br> 中的每个线程请求一个4字节的值，那么每次请求就会获取128字节的数据，这恰好与缓存<br> 行和设备内存段的大小相契合。</p></li><li><p>因此在优化应用程序时，你需要注意设备内存访问的两个特性：<br> 对齐内存访问<br> 合并内存访问</p></li></ul><figure><img src="'+h+'" alt="figure4-6" tabindex="0" loading="lazy"><figcaption>figure4-6</figcaption></figure><ul><li>当设备内存事务的第一个地址是用于事务服务的缓存粒度的偶数倍时（32字节的二级<br> 缓存或128字节的一级缓存），就会出现对齐内存访问。运行非对齐的加载会造成带宽浪<br> 费。</li><li>当一个线程束中全部的32个线程访问一个连续的内存块时，就会出现合并内存访问。</li><li>对齐合并内存访问的理想状态是线程束从对齐内存地址开始访问一个连续的内存块。<br> 为了最大化全局内存吞吐量，为了组织内存操作进行对齐合并是很重要的。图4-7描述了<br> 对齐与合并内存的加载操作。在这种情况下，只需要一个128字节的内存事务从设备内存<br> 中读取数据。图4-8展示了非对齐和未合并的内存访问。在这种情况下，可能需要3个128<br> 字节的内存事务来从设备内存中读取数据：一个在偏移量为0的地方开始，读取连续地址<br> 之后的数据；一个在偏移量为256的地方开始，读取连续地址之前的数据；另一个在偏移<br> 量为128的地方开始读取大量的数据。注意在内存事务之前和之后获取的大部分字节将不<br> 能被使用，这样会造成带宽浪费。</li></ul><figure><img src="'+x+'" alt="figure4-7" tabindex="0" loading="lazy"><figcaption>figure4-7</figcaption></figure><figure><img src="'+w+'" alt="figure4-8" tabindex="0" loading="lazy"><figcaption>figure4-8</figcaption></figure><h3 id="_4-3-2-全局内存读取" tabindex="-1"><a class="header-anchor" href="#_4-3-2-全局内存读取" aria-hidden="true">#</a> 4.3.2 全局内存读取</h3><ul><li><p>在SM中，数据通过以下3种缓存/缓冲路径进行传输，具体使用何种方式取决于引用<br> 了哪种类型的设备内存：<br> 一级和二级缓存<br> 常量缓存<br> 只读缓存</p></li><li><p>一/二级缓存是默认路径。想要通过其他两种路径传递数据需要应用程序显式地说<br> 明，但要想提升性能还要取决于使用的访问模式。全局内存加载操作是否会通过一级缓存<br> 取决于两个因素：<br> 设备的计算能力<br> 编译器选项</p></li><li><p>在Fermi GPU（计算能力为2.x）和Kepler K40及以后的GPU（计算能力为3.5及以上）<br> 中，可以通过编译器标志启用或禁用全局内存负载的一级缓存。默认情况下，在Fermi设<br> 备上对于全局内存加载可以用一级缓存，在K40及以上GPU中禁用。以下标志通知编译器<br> 禁用一级缓存：<br> -Xptxas -dlcm=cg</p></li><li><p>如果一级缓存被禁用，所有对全局内存的加载请求将直接进入到二级缓存；如果二级<br> 缓存缺失，则由DRAM完成请求。每一次内存事务可由一个、两个或四个部分执行，每个<br> 部分有32个字节。一级缓存也可以使用下列标识符直接启用：<br> -Xptxas -dlcm=ca</p></li><li><p>设置这个标志后，全局内存加载请求首先尝试通过一级缓存。如果一级缓存缺失，该<br> 请求转向二级缓存。如果二级缓存缺失，则请求由DRAM完成。在这种模式下，一个内存<br> 加载请求由一个128字节的设备内存事务实现。</p></li><li><p>在Kepler K10、K20和K20X GPU中，一级缓存不用来缓存全局内存加载。一级缓存专<br> 门用于缓存寄存器溢出到本地内存中的数据。</p></li><li><p><mark>内存加载访问模式</mark></p></li><li><p>内存加载可以分为两类：</p></li><li><p>缓存加载（启用一级缓存）</p></li><li><p>没有缓存的加载（禁用一级缓存）</p></li><li><p>内存加载的访问模式有如下特点：</p></li><li><p>·有缓存与没有缓存：如果启用一级缓存，则内存加载被缓存</p></li><li><p>对齐与非对齐：如果内存访问的第一个地址是32字节的倍数，则对齐加载</p></li><li><p>合并与未合并：如果线程束访问一个连续的数据块，则加载合并</p></li><li><p>你将在下一节中学习这些内存访问模式对核函数性能的影响。</p></li></ul><h3 id="_4-3-2-1-缓存加载" tabindex="-1"><a class="header-anchor" href="#_4-3-2-1-缓存加载" aria-hidden="true">#</a> 4.3.2.1 缓存加载</h3><ul><li>缓存加载操作经过一级缓存，在粒度为128字节的一级缓存行上由设备内存事务进行<br> 传输。缓存加载可以分为对齐/非对齐及合并/非合并。</li><li>图4-9所示为理想情况：对齐与合并内存访问。线程束中所有线程请求的地址都在128<br> 字节的缓存行范围内。完成内存加载操作只需要一个128字节的事务。总线的使用率为100%，在这个事务中没有未使用的数据。</li></ul><figure><img src="'+A+'" alt="figure4-9" tabindex="0" loading="lazy"><figcaption>figure4-9</figcaption></figure><ul><li>图4-10所示为另一种情况：访问是对齐的，引用的地址不是连续的线程ID，而是128<br> 字节范围内的随机值。由于线程束中线程请求的地址仍然在一个缓存行范围内，所以只需<br> 要一个128字节的事务来完成这一内存加载操作。总线利用率仍然是100%，并且只有当每<br> 个线程请求在128字节范围内有4个不同的字节时，这个事务中才没有未使用的数据。</li></ul><figure><img src="'+D+'" alt="figure4-10" tabindex="0" loading="lazy"><figcaption>figure4-10</figcaption></figure><ul><li>图4-11也说明了一种情况：线程束请求32个连续4个字节的非对齐数据元素。在全局<br> 内存中线程束的线程请求的地址落在两个128字节段范围内。因为当启用一级缓存时，由<br> SM执行的物理加载操作必须在128个字节的界线上对齐，所以要求有两个128字节的事务<br> 来执行这段内存加载操作。总线利用率为50%，并且在这两个事务中加载的字节有一半是<br> 未使用的。</li></ul><figure><img src="'+M+'" alt="figure4-11" tabindex="0" loading="lazy"><figcaption>figure4-11</figcaption></figure><ul><li><p>图4-12说明了一种情况：线程束中所有线程请求相同的地址。因为被引用的字节落在<br> 一个缓存行范围内，所以只需请求一个内存事务，但总线利用率非常低。如果加载的值是4字节的，则总线利用率是4字节请求/128字节加载＝3.125%。<br><img src="'+S+'" alt="figure4-12" loading="lazy"></p></li><li><p>图4-13所示为最坏的情况：线程束中线程请求分散于全局内存中的32个4字节地址。<br> 尽管线程束请求的字节总数仅为128个字节，但地址要占用N个缓存行（0＜N≤32）。完成<br> 一次内存加载操作需要申请N次内存事务。<br><img src="'+C+'" alt="figure4-13" loading="lazy"></p></li><li><p><mark>CPU一级缓存和GPU一级缓存之间的差异</mark></p></li><li><p>CPU一级缓存优化了时间和空间局部性。GPU一级缓存是专为空间局部性而不是为时<br> 间局部性设计的。频繁访问一个一级缓存的内存位置不会增加数据留在缓存中的概率。</p></li></ul><h3 id="_4-3-2-2-没有缓存的加载" tabindex="-1"><a class="header-anchor" href="#_4-3-2-2-没有缓存的加载" aria-hidden="true">#</a> 4.3.2.2 没有缓存的加载</h3><ul><li><p>没有缓存的加载不经过一级缓存，它在内存段的粒度上（32个字节）而非缓存池的粒<br> 度（128个字节）执行。这是更细粒度的加载，可以为非对齐或非合并的内存访问带来更<br> 好的总线利用率。</p></li><li><p>图4-14所示为理想情况：对齐与合并内存访问。128个字节请求的地址占用了4个内存<br> 段，总线利用率为100%。</p></li></ul><figure><img src="'+U+'" alt="figure4-14" tabindex="0" loading="lazy"><figcaption>figure4-14</figcaption></figure><ul><li><p>图4-15说明了一种情况：内存访问是对齐的且线程访问是不连续的，而是在128个字<br> 节的范围内随机进行。只要每个线程请求唯一的地址，那么地址将占用4个内存段，并且<br> 不会有加载浪费。这样的随机访问不会抑制内核性能。<br><img src="'+P+'" alt="figure4-15" loading="lazy"></p></li><li><p>图4-16说明了一种情况：线程束请求32个连续的4字节元素但加载没有对齐到128个字<br> 节的边界。请求的地址最多落在5个内存段内，总线利用率至少为80%。与这些类型的请<br> 求缓存加载相比，使用非缓存加载会提升性能，这是因为加载了更少的未请求字节。<br><img src="'+B+'" alt="figure4-16" loading="lazy"></p></li><li><p>图4-17说明了一种情况：线程束中所有线程请求相同的数据。地址落在一个内存段<br> 内，总线的利用率是请求的4字节/加载的32字节＝12.5%，在这种情况下，非缓存加载性<br> 能也是优于缓存加载的性能。<br><img src="'+z+'" alt="figure4-17" loading="lazy"></p></li><li><p>图4-18说明了最坏的一种情况：线程束请求32个分散在全局内存中的4字节字。由于<br> 请求的128个字节最多落在N个32字节的内存分段内而不是N个128个字节的缓存行内，所<br> 以相比于缓存加载，即便是最坏的情况也有所改善。<br><img src="'+I+`" alt="figure4-18" loading="lazy"></p></li></ul><h3 id="_4-3-2-3-非对齐读取的示例" tabindex="-1"><a class="header-anchor" href="#_4-3-2-3-非对齐读取的示例" aria-hidden="true">#</a> 4.3.2.3 非对齐读取的示例</h3><ul><li><p>因为访问模式往往是由应用程序实现的一个算法来决定的，所以对于某些应用程序来<br> 说合并内存加载是一个挑战。然而，在大多数情况下，使用某些方法可以帮助对齐应用程<br> 序内存访问。</p></li><li><p>为了说明核函数中非对齐访问对性能的影响，我们对第3章中使用的向量加法代码进<br> 行修改，去掉所有的内存加载操作，来指定一个偏移量。注意在下面的核函数中使用了两<br> 种索引。新的索引k由给定的偏移量上移，由于偏移量的值可能会导致加载出现非对齐加<br> 载。只有加载数组A和数组B的操作会用到索引k。对数组C的写操作仍使用原来的索引i，<br> 以确保写入访问保持对齐。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">readOffset</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> 
 <span class="token keyword">int</span> offset<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> k <span class="token operator">=</span> i <span class="token operator">+</span> offset<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> C<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>为保证修改后核函数的正确性，主机代码也应该做出相应的修改：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">void</span> <span class="token function">sumArraysOnHost</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> 
 <span class="token keyword">int</span> offset<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> idx <span class="token operator">=</span> offset<span class="token punctuation">,</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> idx <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> idx<span class="token operator">++</span><span class="token punctuation">,</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 C<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,29),$n=n("li",null,[n("p",null,[s("代码清单4-4所示为主函数。这段代码中的绝大部分对你来说应该很熟悉了。你可以"),n("br"),s(" 从Wrox.com的readSegment.cu上下载完整代码。偏移量的默认值是0，但是它可以通过一个"),n("br"),s(" 命令行重写参数。")])],-1),On={href:"http://readSegment.cu",target:"_blank",rel:"noopener noreferrer"},Fn={class:"hint-container details"},Nn=n("summary",null,"Click me to view the code!",-1),Kn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" argc"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s("argv"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// set up device"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" dev "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 cudaDeviceProp deviceProp`),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaGetDeviceProperties"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("deviceProp"),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"%s starting reduction at "'),n("span",{class:"token punctuation"},","),s(" argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"device %d: %s "'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},","),s(" deviceProp"),n("span",{class:"token punctuation"},"."),s("name"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaSetDevice"),n("span",{class:"token punctuation"},"("),s("dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up array size"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" nElem "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},"<<"),n("span",{class:"token number"},"20"),n("span",{class:"token punctuation"},";"),s(),n("span",{class:"token comment"},"// total number of elements to reduce"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'" with array size %d\\n"'),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 size_t nBytes `),n("span",{class:"token operator"},"="),s(" nElem "),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up offset for summary"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" blocksize "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"512"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" offset "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(" offset "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),s(" blocksize "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// execution configuration"),s(`
 dim3 `),n("span",{class:"token function"},"block"),s(),n("span",{class:"token punctuation"},"("),s("blocksize"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 dim3 `),n("span",{class:"token function"},"grid"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("nElem"),n("span",{class:"token operator"},"+"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate host memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("h_A "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("h_B "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("hostRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("gpuRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize host array"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"memcpy"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},","),s("h_A"),n("span",{class:"token punctuation"},","),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// summary at host side"),s(`
 `),n("span",{class:"token function"},"sumArraysOnHost"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" h_B"),n("span",{class:"token punctuation"},","),s(" hostRef"),n("span",{class:"token punctuation"},","),s("nElem"),n("span",{class:"token punctuation"},","),s("offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate device memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("d_A"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"*"),s("d_B"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"*"),s("d_C"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_B"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy data from host to device"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_B"),n("span",{class:"token punctuation"},","),s(" h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// kernel 1:"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iStart "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 warmup `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_B"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},","),s(" offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iElaps "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"warmup <<< %4d, %4d >>> offset %4d elapsed %f sec\\n"'),n("span",{class:"token punctuation"},","),s(`
 grid`),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(`
 offset`),n("span",{class:"token punctuation"},","),s(" iElaps"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 iStart `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_B"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},","),s(" offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 iElaps `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"readOffset <<< %4d, %4d >>> offset %4d elapsed %f sec\\n"'),n("span",{class:"token punctuation"},","),s(` 
 grid`),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(`
 offset`),n("span",{class:"token punctuation"},","),s(" iElaps"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy kernel result back to host side and check device results"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token operator"},"-"),s("offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy kernel result back to host side and check device results"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token operator"},"-"),s("offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy kernel result back to host side and check device results"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token operator"},"-"),s("offset"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// free host and device memory"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_B"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_B"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// reset device"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Vn=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`使用下列指令编译代码：
$ nvcc `),n("span",{class:"token operator"},"-"),s("O3 "),n("span",{class:"token operator"},"-"),s("arch"),n("span",{class:"token operator"},"="),s("sm_20 readSegment"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o readSegment


你可以使用不同的偏移量进行实验。一个来自Fermi M2050的示例输出如下：
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"0"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"0"),s(" elapsed "),n("span",{class:"token number"},"0.001820"),s(` sec
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"11"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"11"),s(" elapsed "),n("span",{class:"token number"},"0.001949"),s(` sec
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"128"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"128"),s(" elapsed "),n("span",{class:"token number"},"0.001821"),s(` sec


使用值为`),n("span",{class:"token number"},"11"),s(`的偏移量会导致数组A和数组B的内存加载是非对齐的。在这种情况下，
运行时间也是最慢的。通过观察以全局加载效率为指标的结果，可以验证这些非对齐访问
就是性能损失的原因：
    全局加载率`),n("span",{class:"token operator"},"="),s("请求的全局内存加载吞吐量 "),n("span",{class:"token operator"},"/"),s(` 所需的全局内存加载吞吐量

请求的全局内存加载吞吐量包括重新执行的内存加载指令，这个指令不只需要一个内
存事务，而请求的全局内存加载吞吐量并不需要如此。

可以使用nvprof获取gld_efficiency指标，其中nvprof带有readSegment测试用例和不同偏
移量值：
$ nvprof `),n("span",{class:"token operator"},"--"),s("devices "),n("span",{class:"token number"},"0"),s(),n("span",{class:"token operator"},"--"),s("metrics gld_transactions "),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"0"),s(`
$ nvprof `),n("span",{class:"token operator"},"--"),s("devices "),n("span",{class:"token number"},"0"),s(),n("span",{class:"token operator"},"--"),s("metrics gld_transactions "),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"11"),s(`
$ nvprof `),n("span",{class:"token operator"},"--"),s("devices "),n("span",{class:"token number"},"0"),s(),n("span",{class:"token operator"},"--"),s("metrics gld_transactions "),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"128"),s(`

示例结果如下：
Offset `),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"100.00"),n("span",{class:"token operator"},"%"),s(` 
Offset `),n("span",{class:"token number"},"11"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"49.81"),n("span",{class:"token operator"},"%"),s(` 
Offset `),n("span",{class:"token number"},"128"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"100.00"),n("span",{class:"token operator"},"%"),s(`



对于非对齐读取的情况（偏移量为`),n("span",{class:"token number"},"11"),s(`），全局加载效率减半，这意味着请求的全局内
存加载吞吐量加倍。你可以借助全局加载事务指标来直接验证：
$ nvprof `),n("span",{class:"token operator"},"--"),s("devices "),n("span",{class:"token number"},"0"),s(),n("span",{class:"token operator"},"--"),s("metrics gld_transactions "),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`readSegment $OFFSET

示例结果如下：
Offset `),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(" gld_transactions "),n("span",{class:"token number"},"65184"),s(`
Offset `),n("span",{class:"token number"},"11"),n("span",{class:"token operator"},":"),s(" gld_transactions "),n("span",{class:"token number"},"131039"),s(`
Offset `),n("span",{class:"token number"},"128"),n("span",{class:"token operator"},":"),s(" gld_transactions "),n("span",{class:"token number"},"65744"),s(`


根据预测，对于偏移量为`),n("span",{class:"token number"},"11"),s(`的情况，全局加载事务数量加倍。你也可以看到禁用一级
缓存对全局内存加载性能有何影响。为了强制执行没有缓存的加载，重新编译代码并增加
了以下nvcc选项：
`),n("span",{class:"token operator"},"-"),s("Xptxas "),n("span",{class:"token operator"},"-"),s("dlcm"),n("span",{class:"token operator"},"="),s(`cg

注意，这个编译器标志只能改变Fermi和Kepler K40或最新GPU的执行过程。示例输出
显示如下：
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"0"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"0"),s(" elapsed "),n("span",{class:"token number"},"0.001825"),s(` sec
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("readSegment "),n("span",{class:"token number"},"11"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"11"),s(" elapsed "),n("span",{class:"token number"},"0.002309"),s(` sec
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(),n("span",{class:"token number"},"128"),s(`
readOffset `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"32768"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"512"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" offset "),n("span",{class:"token number"},"128"),s(" elapsed "),n("span",{class:"token number"},"0.001823"),s(` sec



结果显示，没有缓存的加载的整体性能略低于缓存访问的性能。缓存缺失对非对齐访
问的性能影响更大。如果启用缓存，一个非对齐访问可能将数据存到一级缓存，这个一级
缓存用于后续的非对齐内存访问。但是，如果没有一级缓存，那么每一次非对齐请求需要
多个内存事务，并且对将来的请求没有作用。


你也可以借助没有缓存的加载来检验全局加载效率。示例结果显示如下：
Offset `),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"100.00"),n("span",{class:"token operator"},"%"),s(` 
Offset `),n("span",{class:"token number"},"11"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"80.00"),n("span",{class:"token operator"},"%"),s(` 
Offset `),n("span",{class:"token number"},"128"),n("span",{class:"token operator"},":"),s(" gld_efficiency "),n("span",{class:"token number"},"100.00"),n("span",{class:"token operator"},"%"),s(`



对于非对齐情况，禁用一级缓存使加载效率得到了提高，从`),n("span",{class:"token number"},"49.8"),n("span",{class:"token operator"},"%"),s("提高到了"),n("span",{class:"token number"},"80"),n("span",{class:"token operator"},"%"),s(`。由
于禁用了一级缓存，每个加载请求是在`),n("span",{class:"token number"},"32"),s("个字节的粒度上而不是"),n("span",{class:"token number"},"128"),s(`个字节的粒度上进行
处理，因此加载的字节（但未使用的）数量减少了。


你可能注意到使用没有缓存的整体加载时间并没有减少，但是全局加载效率提高了。
确实是这样，但这种结果只针对这种测试示例。随着设备占用率的提高，没有缓存的加载
可帮助提高总线的整体利用率。对于没有缓存的非对齐加载模式来说，未使用的数据传输
量可能会显著减少。

`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),Zn=l(`<h3 id="_4-3-2-4-只读缓存" tabindex="-1"><a class="header-anchor" href="#_4-3-2-4-只读缓存" aria-hidden="true">#</a> 4.3.2.4 只读缓存</h3><ul><li><p>只读缓存最初是预留给纹理内存加载使用的。对计算能力为3.5及以上的GPU来说，<br> 只读缓存也支持使用全局内存加载代替一级缓存。</p></li><li><p>只读缓存的加载粒度是32个字节。通常，对分散读取来说，这些更细粒度的加载要优<br> 于一级缓存。</p></li><li><p>有两种方式可以指导内存通过只读缓存进行读取：<br> 使用函数__ldg<br> 在间接引用的指针上使用修饰符</p></li><li><p>例如，考虑下面的拷贝核函数：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">copyKernel</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>in<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>你可以使用内部函数__ldg来通过只读缓存直接对数组进行读取访问：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">copyKernel</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span>in<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">__ldg</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>in<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>你也可以将常量__restrict__修饰符应用到指针上。这些修饰符帮助nvcc编译器识别无<br> 别名指针（即专门用来访问特定数组的指针）。nvcc将自动通过只读缓存指导无别名指针<br> 的加载。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">copyKernel</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">*</span> __restrict__ out<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> <span class="token operator">*</span> __restrict__ in<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">int</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-3-3-全局内存写入" tabindex="-1"><a class="header-anchor" href="#_4-3-3-全局内存写入" aria-hidden="true">#</a> 4.3.3 全局内存写入</h3><ul><li><p>内存的存储操作相对简单。一级缓存不能用在Fermi或Kepler GPU上进行存储操作，<br> 在发送到设备内存之前存储操作只通过二级缓存。存储操作在32个字节段的粒度上被执<br> 行。内存事务可以同时被分为一段、两段或四段。例如，如果两个地址同属于一个128个<br> 字节区域，但是不属于一个对齐的64个字节区域，则会执行一个四段事务（也就是说，执<br> 行一个四段事务比执行两个一段事务效果更好）。</p></li><li><p>图4-19所示为理想情况：内存访问是对齐的，并且线程束里所有的线程访问一个连续<br> 的128字节范围。存储请求由一个四段事务实现。</p></li></ul><figure><img src="`+E+'" alt="figure4-19" tabindex="0" loading="lazy"><figcaption>figure4-19</figcaption></figure><ul><li>图4-20所示为内存访问是对齐的，但地址分散在一个192个字节范围内的情况。存储<br> 请求由3个一段事务来实现。</li></ul><figure><img src="'+R+'" alt="figure4-20" tabindex="0" loading="lazy"><figcaption>figure4-20</figcaption></figure><ul><li>图4-21所示为内存访问是对齐的，并且地址访问在一个连续的64个字节范围内的情<br> 况。这种存储请求由一个两段事务来完成。</li></ul><figure><img src="'+G+`" alt="figure4-21" tabindex="0" loading="lazy"><figcaption>figure4-21</figcaption></figure><ul><li><mark>非对齐写入的示例</mark></li><li>为了验证非对齐对内存存储效率的影响，按照下面的方式修改向量加法核函数。仍然<br> 使用两个不同的索引：索引k根据给定的偏移量进行变化，而索引i不变（并因此产生对齐<br> 访问）。使用对齐索引i从数组A和数组B中进行加载，以产生良好的内存加载效率。使用<br> 偏移量索引x写入数组C，可能会造成非对齐写入，这取决于偏移量的值。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">writeOffset</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> offset<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> k <span class="token operator">=</span> i <span class="token operator">+</span> offset<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> C<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>按以上要求修改主机端向量加法代码：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">void</span> <span class="token function">sumArraysOnHost</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> 
 <span class="token keyword">int</span> offset<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> idx <span class="token operator">=</span> offset<span class="token punctuation">,</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> idx <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> idx<span class="token operator">++</span><span class="token punctuation">,</span> k<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
 C<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,18),qn=n("li",null,"主函数与非对齐加载示例中的主函数几乎完全相同。你可以从Wrox.com中的write-Segment.cu下载完整的示例代码。",-1),Ln=n("br",null,null,-1),Wn={href:"http://writeSegment.cu",target:"_blank",rel:"noopener noreferrer"},Xn=n("li",null,"偏移量分别为0、11和128的输出如下所示：",-1),jn=l(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>writeSegment <span class="token number">0</span>
writeOffset <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">0</span> elapsed <span class="token number">0.000134</span> sec
$ <span class="token punctuation">.</span><span class="token operator">/</span>writeSegment <span class="token number">11</span>
writeOffset <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">11</span> elapsed <span class="token number">0.000184</span> sec
$ <span class="token punctuation">.</span><span class="token operator">/</span>writeSegment <span class="token number">128</span>
writeOffset <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">128</span> elapsed <span class="token number">0.000134</span> sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>显然，非对齐写入的情况（偏移量为11）性能最差。通过使用nvprof获取全局加载和<br> 存储效率指标，你可以查明这种非对齐情况的产生原因：<br> $ nvprof --devices 0 --metrics gld_efficiency --metrics gst_efficiency <br> ./writeSegment $OFFSET</p></li><li><p>nvprof的示例输出如下：<br> writeOffset Offset 0: gld_efficiency 100.00%<br> writeOffset Offset 0: gst_efficiency 100.00%<br> writeOffset Offset 11: gld_efficiency 100.00%<br> writeOffset Offset 11: gst_efficiency 80.00%<br> writeOffset Offset 128: gld_efficiency 100.00%<br> writeOffset Offset 128: gst_efficiency 100.00%</p></li><li><p>除了非对齐情况（偏移量为11）的存储，所有加载和存储的效率都为100%。非对齐<br> 写入的存储效率为80%。当偏移量为11且从一个线程束产生一个128个字节的写入请求<br> 时，该请求将由一个四段事务和一个一段事务来实现。因此，128个字节用来请求，160个<br> 字节用来加载，存储效率为80%。</p></li></ul><h3 id="_4-3-4-结构体数组与数组结构体" tabindex="-1"><a class="header-anchor" href="#_4-3-4-结构体数组与数组结构体" aria-hidden="true">#</a> 4.3.4 结构体数组与数组结构体</h3><ul><li><p>作为一个C程序员，你应该对两种数据组织方式非常熟悉：数组结构体（AoS）和结<br> 构体数组（SoA）。这是一个有趣的话题，因为当存储结构化数据集时，它们代表了可以<br> 采用的两种强大的数据组织方式（结构体和数组）。</p></li><li><p>下面是存储成对的浮点数据元素集的例子。首先，考虑这些成对数据元素集如何使用<br> AoS方法进行存储。如下定义一个结构体，命名为innerStruct：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">struct</span> <span class="token class-name">innerStruct</span> <span class="token punctuation">{</span>
 <span class="token keyword">float</span> x<span class="token punctuation">;</span>
 <span class="token keyword">float</span> y<span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>然后，按照下面的方法定义这些结构体数组。这是利用AoS方式来组织数据的。它存<br> 储的是空间上相邻的数据（例如，x和y），这在CPU上会有良好的缓存局部性。<br> struct innerStruct myAoS[N];</li><li>接下来，考虑使用SoA方法来存储数据：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">struct</span> <span class="token class-name">innerArray</span> <span class="token punctuation">{</span>
 <span class="token keyword">float</span> x<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token keyword">float</span> y<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>这里，在原结构体中每个字段的所有值都被分到各自的数组中。这不仅能将相邻数据<br> 点紧密存储起来，也能将跨数组的独立数据点存储起来。你可以使用如下结构体定义一个<br> 变量：struct innerArray moa;</p></li><li><p>图4-22说明了AoS和SoA方法的内存布局。用AoS模式在GPU上存储示例数据并执行一个只有x字段的应用程序，将导致50%的带宽损失，因为y值在每32个字节段或128个字节缓存行上隐式地被加载。AoS格式也在不需要的y值上浪费了二级缓存空间。</p></li><li><p>用SoA模式存储数据充分利用了GPU的内存带宽。由于没有相同字段元素的交叉存<br> 取，GPU上的SoA布局提供了合并内存访问，并且可以对全局内存实现更高效的利用。</p></li></ul><figure><img src="`+T+`" alt="figure4-22" tabindex="0" loading="lazy"><figcaption>figure4-22</figcaption></figure><ul><li><p><mark>AOS与SOA</mark></p></li><li><p>许多并行编程范式，尤其是SIMD型范式，更倾向于使用SoA。在CUDA C编程中也普<br> 遍倾向于使用SoA，因为数据元素是为全局内存的有效合并访问而预先准备好的，而被相<br> 同内存操作引用的同字段数据元素在存储时是彼此相邻的。</p></li><li><p>在每种数据布局中为了帮助理解访问数据对性能的影响，我们将通过执行相同的数学<br> 运算来比较两个核函数：一个使用AoS数据布局，另一个则使用SoA数据布局。</p></li></ul><h3 id="_4-3-4-1-示例-使用aos数据布局的简单数学运算" tabindex="-1"><a class="header-anchor" href="#_4-3-4-1-示例-使用aos数据布局的简单数学运算" aria-hidden="true">#</a> 4.3.4.1 示例：使用AoS数据布局的简单数学运算</h3><ul><li>下述核函数采用AoS布局。全局内存结构体数组是借助变量x和y进行线性存储的。每<br> 个线程的输入和输出是相同的：一个独立的innerStruct结构。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">testInnerStruct</span><span class="token punctuation">(</span>innerStruct <span class="token operator">*</span>data<span class="token punctuation">,</span> 
 innerStruct <span class="token operator">*</span>result<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 innerStruct tmp <span class="token operator">=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 tmp<span class="token punctuation">.</span>x <span class="token operator">+=</span> <span class="token number">10.f</span><span class="token punctuation">;</span>
 tmp<span class="token punctuation">.</span>y <span class="token operator">+=</span> <span class="token number">20.f</span><span class="token punctuation">;</span>
 result<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> tmp<span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,13),Jn=l("<li><p>输入长度定义为1M：<br> #define LEN 1&lt;&lt;20</p></li><li><p>使用以下语句分配全局内存：<br> int nElem = LEN;<br> size_t nBytes = nElem * sizeof(innerStruct);<br> innerStruct <em>d_A,<em>d_C;<br> cudaMalloc((innerStruct</em></em>)&amp;d_A, nBytes);<br> cudaMalloc((innerStruct**)&amp;d_C, nBytes);</p></li><li><p>使用以下主机函数初始化输入数组：<br> void initialInnerStruct(innerStruct *ip, int size) {<br> for (int i = 0; i &lt; size; i++) {<br> ip[i].x = (float)(rand() &amp; 0xFF) / 100.0f;<br> ip[i].y = (float)(rand() &amp; 0xFF) / 100.0f;<br> }<br> return;<br> }</p></li><li><p>主函数如代码清单4-5所示。你可以从Wrox.com中的simpleMathAoS.cu下载完整的示例<br> 代码。在下列代码中，warm-up核函数用于限制CUDA启动开销的影响，并且可以获得<br> textInnerStruct核函数更精确的时间测量。</p></li>",4),Qn={href:"http://simpleMethAoS.cu",target:"_blank",rel:"noopener noreferrer"},Yn=n("br",null,null,-1),ns={class:"hint-container details"},ss=n("summary",null,"Click me to view the code!",-1),as=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" argc"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s("argv"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// set up device"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" dev "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 cudaDeviceProp deviceProp`),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaGetDeviceProperties"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("deviceProp"),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"%s test struct of array at "'),n("span",{class:"token punctuation"},","),s(" argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"device %d: %s \\n"'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},","),s(" deviceProp"),n("span",{class:"token punctuation"},"."),s("name"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaSetDevice"),n("span",{class:"token punctuation"},"("),s("dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate host memory"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" nElem "),n("span",{class:"token operator"},"="),s(" LEN"),n("span",{class:"token punctuation"},";"),s(`
 size_t nBytes `),n("span",{class:"token operator"},"="),s(" nElem "),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),s("innerStruct"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 innerStruct `),n("span",{class:"token operator"},"*"),s("h_A "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),s("innerStruct "),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 innerStruct `),n("span",{class:"token operator"},"*"),s("hostRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),s("innerStruct "),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 innerStruct `),n("span",{class:"token operator"},"*"),s("gpuRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),s("innerStruct "),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize host array"),s(`
 `),n("span",{class:"token function"},"initialInnerStruct"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"testInnerStructHost"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" hostRef"),n("span",{class:"token punctuation"},","),s("nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate device memory"),s(`
 innerStruct `),n("span",{class:"token operator"},"*"),s("d_A"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"*"),s("d_C"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("innerStruct"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("innerStruct"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy data from host to device"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up offset for summary"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" blocksize "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"128"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(" blocksize "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// execution configuration"),s(`
 dim3 `),n("span",{class:"token function"},"block"),s(),n("span",{class:"token punctuation"},"("),s("blocksize"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 dim3 `),n("span",{class:"token function"},"grid"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("nElem"),n("span",{class:"token operator"},"+"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// kernel 1: warmup"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iStart "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 warmup `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iElaps "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"warmup <<< %3d, %3d >>> elapsed %f sec\\n"'),n("span",{class:"token punctuation"},","),s("grid"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(`
 block`),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s("iElaps"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkInnerStruct"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// kernel 2: testInnerStruct"),s(`
 iStart `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 testInnerStruct `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 iElaps `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"innerstruct <<< %3d, %3d >>> elapsed %f sec\\n"'),n("span",{class:"token punctuation"},","),s("grid"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(`
 block`),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s("iElaps"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkInnerStruct"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nElem"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// free memories both host and device"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// reset device"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),ts=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`用以下指令编译示例并运行。
$ nvcc `),n("span",{class:"token operator"},"-"),s("O3 "),n("span",{class:"token operator"},"-"),s("arch"),n("span",{class:"token operator"},"="),s("sm_20 simpleMathAoS"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o simpleMathAoS
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`simpleMathAoS

来自Fermi M2070的输出结果如下。
innerStruct `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(),n("span",{class:"token number"},"8192"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"128"),s(),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(" elapsed "),n("span",{class:"token number"},"0.000286"),s(` sec

运行以下nvprof命令来获取全局加载效率和全局存储效率指标：
$ nvprof `),n("span",{class:"token operator"},"--"),s("devices "),n("span",{class:"token number"},"0"),s(),n("span",{class:"token operator"},"--"),s("metrics gld_efficiency"),n("span",{class:"token punctuation"},","),s("gst_efficiency "),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s(`simpleMathAoS

下面展示的`),n("span",{class:"token number"},"50"),n("span",{class:"token operator"},"%"),s(`的效率结果表明，对于AOS数据布局，加载请求和内存存储请求是重
复的。因为字段x和y在内存中是被相邻存储的，并且有相同的大小，每当执行内存事务时
都要加载特定字段的值，被加载的字节数的一半也必须属于其他字段。因此，请求加载和
存储的`),n("span",{class:"token number"},"50"),n("span",{class:"token operator"},"%"),s(`带宽是未使用的。
gld_efficiency `),n("span",{class:"token number"},"50.00"),n("span",{class:"token operator"},"%"),s(`
gst_efficiency `),n("span",{class:"token number"},"50.00"),n("span",{class:"token operator"},"%"),s(`

`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),es=l(`<h3 id="_4-3-4-2-示例-使用soa数据布局的简单数学运算" tabindex="-1"><a class="header-anchor" href="#_4-3-4-2-示例-使用soa数据布局的简单数学运算" aria-hidden="true">#</a> 4.3.4.2 示例：使用SoA数据布局的简单数学运算</h3><ul><li>下述核函数采用SoA布局。分配两个一维全局内存基元数组来存储两个字段x和y的所<br> 有值。以下核函数通过索引为每个基元数组获得取适当的值。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">testInnerArray</span><span class="token punctuation">(</span>InnerArray <span class="token operator">*</span>data<span class="token punctuation">,</span> 
 InnerArray <span class="token operator">*</span>result<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>i<span class="token operator">&lt;</span>n<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">float</span> tmpx <span class="token operator">=</span> data<span class="token operator">-&gt;</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token keyword">float</span> tmpy <span class="token operator">=</span> data<span class="token operator">-&gt;</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
 tmpx <span class="token operator">+=</span> <span class="token number">10.f</span><span class="token punctuation">;</span>
 tmpy <span class="token operator">+=</span> <span class="token number">20.f</span><span class="token punctuation">;</span>
 result<span class="token operator">-&gt;</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> tmpx<span class="token punctuation">;</span>
 result<span class="token operator">-&gt;</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> tmpy<span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>使用的输入长度与AoS测试示例中的一样，都是1M：<br> #define LEN 1&lt;&lt;20</p></li><li><p>用下列语句分配全局内存。注意sizeof（Innerarray）包括其静态声明字段x和y的大小。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">int</span> nElem <span class="token operator">=</span> LEN<span class="token punctuation">;</span>
size_t nBytes <span class="token operator">=</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>InnerArray<span class="token punctuation">)</span><span class="token punctuation">;</span>
InnerArray <span class="token operator">*</span>d_A<span class="token punctuation">,</span><span class="token operator">*</span>d_C<span class="token punctuation">;</span>
<span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span>InnerArray <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_A<span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token punctuation">(</span>InnerArray <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>d_C<span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),os=n("br",null,null,-1),ps=n("br",null,null,-1),cs=n("br",null,null,-1),ls={href:"http://simpleMathSoA.cu",target:"_blank",rel:"noopener noreferrer"},is=n("br",null,null,-1),us=l("<li><p>在Fermi M2070上运行simpleMathSoA的示例输出如下。值得注意的是，与simple-AoS<br> 相比，其性能略有提升。使用更大的输入长度，性能提升将更为明显。<br> innerArray &lt;&lt;&lt; 8192, 128 &gt;&gt;&gt; elapsed 0.000200 sec</p></li><li><p>运行以下nvprof命令以获取全局加载效率和全局存储效率指标：<br><code>$ nvprof --devices 0 --metrics gld_efficiency,gst_efficiency ./simpleMathSoA</code></p></li><li><p>其结果展示如下，100%的效率说明当处理SoA数据布局时，加载或存储内存请求不<br> 会重复。每次访问都由一个独立的内存事务来处理。<br> gld_efficiency 100.00%<br> gst_efficiency 100.00%</p></li>",3),rs=l(`<h3 id="_4-3-5-性能调整" tabindex="-1"><a class="header-anchor" href="#_4-3-5-性能调整" aria-hidden="true">#</a> 4.3.5 性能调整</h3><ul><li><p>优化设备内存带宽利用率有两个目标：<br> 对齐及合并内存访问，以减少带宽的浪费<br> 足够的并发内存操作，以隐藏内存延迟</p></li><li><p>在上一节中，已经学习了如何组织内存访问模式以实现对齐合并的内存访问。这样做<br> 在设备DRAM和SM片上内存或寄存器之间能确保有效利用字节移动。</p></li><li><p>第3章讨论了优化指令吞吐量的核函数。回忆一下，实现并发内存访问最大化是通过<br> 以下方式获得的：<br> 增加每个线程中执行独立内存操作的数量<br> 对核函数启动的执行配置进行实验，以充分体现每个SM的并行性</p></li></ul><h3 id="_4-3-5-1-展开技术" tabindex="-1"><a class="header-anchor" href="#_4-3-5-1-展开技术" aria-hidden="true">#</a> 4.3.5.1 展开技术</h3><ul><li><p>包含了内存操作的展开循环增加了更独立的内存操作。对第3章中归约示例的学习你<br> 可能已经熟悉了展开操作。</p></li><li><p>考虑之前的readSegment示例。按如下方式修改readOffset核函数，使每个线程都执行4<br> 个独立的内存操作。因为每个加载过程都是独立的，所以你可以调用更多的并发内存访<br> 问。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">readOffsetUnroll4</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>C<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> offset<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token number">4</span> <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> k <span class="token operator">=</span> i <span class="token operator">+</span> offset<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>k <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 C<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k<span class="token punctuation">]</span> 
 C<span class="token punctuation">[</span>i <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>k <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 C<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>k <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 C<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> A<span class="token punctuation">[</span>k <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">+</span> B<span class="token punctuation">[</span>k <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>你可以从Wrox.com中的readSegmentUnroll.cu下载完整的示例代码。启用一级缓存进行<br> 编译（-Xptxas-dlcm=ca）并运行以下测试：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>readSegmentUnroll <span class="token number">0</span>
warmup <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">32768</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">0</span> elapsed <span class="token number">0.001990</span> sec
unroll4 <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">8192</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">0</span> elapsed <span class="token number">0.000599</span> sec
$ <span class="token punctuation">.</span><span class="token operator">/</span>readSegmentUnroll <span class="token number">11</span>
warmup <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">32768</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">11</span> elapsed <span class="token number">0.002114</span> sec
unroll4 <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">8192</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">11</span> elapsed <span class="token number">0.000615</span> sec
$ <span class="token punctuation">.</span><span class="token operator">/</span>readSegmentUnroll <span class="token number">128</span>
warmup <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">32768</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">128</span> elapsed <span class="token number">0.001989</span> sec
unroll4 <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> <span class="token number">8192</span><span class="token punctuation">,</span> <span class="token number">512</span> <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> offset <span class="token number">128</span> elapsed <span class="token number">0.000598</span> sec
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>结果是不是让你很惊讶？这一展开技术对性能有非常好的影响，甚至比地址对齐还要<br> 好。相对于原来无循环展开的readSegment示例，这些测试说明使用循环展开技术实现了<br> 3.04～3.17倍的加速。对于这样一个I/O密集型的核函数，充分说明内存访问并行有很高的<br> 优先级。正如你所预期的那样，两个对齐的测试示例在性能上仍然优于非对齐访问的情<br> 况。</p></li><li><p>但是，展开并不影响执行内存操作的数量（只影响并发执行的数量）。你可以通过使<br> 用以下指令来测试非对齐情况（偏移量为11），通过测量原始核函数和展开核函数的负载<br> 和存储效率指标可以证实这一点。<br><code>$ nvprof --devices 0 --metrics gld_efficiency,gst_efficiency ./readSegmentUnroll 11</code></p></li><li><p>示例结果总结如下。两个核函数的加载和存储效率相同。<br> readOffset gld_efficiency 49.69%<br> readOffset gst_efficiency 100.00%<br> readOffsetUnroll4 gld_efficiency 50.79%<br> readOffsetUnroll4 gst_efficiency 100.00%</p></li><li><p>现在，在非对齐情况下，尝试获得加载和存储事务的数量（偏移量为11）。<br><code>$ nvprof --devices 0 --metrics gld_transactions,gst_transactions ./readSegmentUnroll 11</code></p></li><li><p>示例结果总结如下。展开的核函数中执行读/写事务的数量明显减少了。<br> readOffset gld_transactions 132384<br> readOffset gst_transactions 32928<br> readOffsetUnroll4 gld_transactions 33152<br> readOffsetUnroll4 gst_transactions 8064</p></li></ul><h3 id="_4-3-5-2-增大并行性" tabindex="-1"><a class="header-anchor" href="#_4-3-5-2-增大并行性" aria-hidden="true">#</a> 4.3.5.2 增大并行性</h3><ul><li><p>为了充分体现并行性，你应该用一个核函数启动的网格和线程块大小进行试验，以找<br> 到该核函数最佳的执行配置。运行以下测试代码，使用对齐内存访问（偏移量＝0）的块<br> 大小进行实验。注意，你可能需要使用1/3的命令行参数来指定数据大小，否则在一个网<br> 格中线程块将超出限制。<br> $ ./readSegmentUnroll 0 1024 22<br> unroll4 &lt;&lt;&lt; 1024, 1024 &gt;&gt;&gt; offset 0 elapsed 0.000169 sec<br> $ ./readSegmentUnroll 0 512 22<br> unroll4 &lt;&lt;&lt; 2048, 512 &gt;&gt;&gt; offset 0 elapsed 0.000159 sec<br> $ ./readSegmentUnroll 0 256 22<br> unroll4 &lt;&lt;&lt; 4096, 256 &gt;&gt;&gt; offset 0 elapsed 0.000157 sec<br> $ ./readSegmentUnroll 0 128 22<br> unroll4 &lt;&lt;&lt; 8192, 128 &gt;&gt;&gt; offset 0 elapsed 0.000158 sec</p></li><li><p>对于展开核函数而言，最佳的线程块大小为每块有256个线程，与之前测试代码中使<br> 用的默认的每块有512个线程相比，线程块的数量加倍了。</p></li><li><p>尽管每块有128个线程对GPU来说有了更多的并行性，但其性能比每块有256个线程稍<br> 差。要想知道其中的原因，请参考第3章中的表3-2，需要特别注意的是此处有两个硬件限<br> 制。因为在这种情况下，测试系统使用Fermi GPU，每个SM最多有8个并发线程块，并且<br> 每个SM最多有48个并发线程束。当采用每个线程块有128个线程的方案时，则每个线程块<br> 有4个线程束。因为一个Fermi SM只可以同时放置8个线程块，所以该核函数被限制每个<br> SM上最多有32个线程束。这可能会导致不能充分利用SM的计算资源，因为没有达到48个<br> 线程束的上限。你也可以使用第3章介绍的CUDA占用率来达到相同的效果。</p></li><li><p>现在，当非对齐访问被执行时，可以验证线程块大小对性能的影响。以下结果与对齐<br> 访问示例产生的结果类似，与每个线程块有128、256和512个线程的完成情况非常类似。<br> 这表明，无论访问是否是对齐的，每个SM中相同的硬件资源限制仍会影响核函数的性<br> 能。<br> $ ./readSegmentUnroll 11 1024 22<br> unroll4 &lt;&lt;&lt; 1024, 1024 &gt;&gt;&gt; offset 11 elapsed 0.000184 sec<br> $ ./readSegmentUnroll 11 512 22<br> unroll4 &lt;&lt;&lt; 2048, 512 &gt;&gt;&gt; offset 11 elapsed 0.000162 sec<br> $ ./readSegmentUnroll 11 256 22<br> unroll4 &lt;&lt;&lt; 4096, 256 &gt;&gt;&gt; offset 11 elapsed 0.000162 sec<br> $ ./readSegmentUnroll 11 128 22<br> unroll4 &lt;&lt;&lt; 8192, 128 &gt;&gt;&gt; offset 11 elapsed 0.000162 sec</p></li><li><p><mark>最大化带宽利用率</mark></p></li><li><p>影响设备内存操作性能的因素主要有两个：</p></li><li><p>有效利用设备DRAM和SM片上内存之间的字节移动：为了避免设备内存带宽的浪<br> 费，内存访问模式应是对齐和合并的</p></li><li><p>当前的并发内存操作数：可通过以下两点实现最大化当前存储器操作数。1）展开，<br> 每个线程产生更多的独立内存访问，2）修改核函数启动的执行配置来使每个SM有更多的<br> 并行性</p></li></ul><h2 id="_4-4-核函数可达到的带宽" tabindex="-1"><a class="header-anchor" href="#_4-4-核函数可达到的带宽" aria-hidden="true">#</a> 4.4 核函数可达到的带宽</h2><ul><li><p>在分析核函数性能时，需要注意内存延迟，即完成一次独立内存请求的时间；内存带<br> 宽，即SM访问设备内存的速度，它以每单位时间内的字节数进行测量。</p></li><li><p>在上一节中，你已经尝试使用两种方法来改进核函数的性能：</p></li><li><p>通过最大化并行执行线程束的数量来隐藏内存延迟，通过维持更多正在执行的内存<br> 访问来达到更好的总线利用率</p></li><li><p>通过适当的对齐和合并内存访问来最大化内存带宽效率</p></li><li><p>然而，往往当前问题的本质就是有一个不好的访问模式。对于这样一个核函数来说，<br> 什么样的性能才是足够好的呢？在次理想的情况下可达到的最理想性能又是什么呢？在本<br> 节中，我们将利用一个矩阵转置的例子学习如何通过使用各种优化手段来调整核函数的带<br> 宽。你会看到，即使一个原本不好的访问模式，仍然可以通过重新设计核函数中的几个部<br> 分以实现良好的性能。</p></li></ul><h3 id="_4-4-1-内存带宽" tabindex="-1"><a class="header-anchor" href="#_4-4-1-内存带宽" aria-hidden="true">#</a> 4.4.1 内存带宽</h3><ul><li>大多数核函数对内存带宽非常敏感，也就是说它们有内存带宽的限制。因此，在调整<br> 核函数时需要注意内存带宽的指标。全局内存中数据的安排方式，以及线程束访问该数据<br> 的方式对带宽有显著影响。一般有如下两种类型的带宽：<br> 理论带宽<br> 有效带宽</li><li>理论带宽是当前硬件可以实现的绝对最大带宽。对禁用ECC的Fermi M2090来说，理<br> 论上设备内存带宽的峰值为177.6 GB/s。有效带宽是核函数实际达到的带宽，它是测量带<br> 宽，可以用下列公式计算：<br> :有效带宽(GB/s) = 读字节数+写字节数 * / 运行时间<br> 例如，对于从设备上传入或传出数据的拷贝来说（包含4个字节整数的2048×2048矩<br> 阵），有效带宽可用以下公式计算：</li></ul><h3 id="_4-4-2-矩阵转置问题" tabindex="-1"><a class="header-anchor" href="#_4-4-2-矩阵转置问题" aria-hidden="true">#</a> 4.4.2 矩阵转置问题</h3><ul><li><p>矩阵转置是线性代数中一个基本问题。虽然是基本问题，但却在许多应用程序中被使<br> 用。矩阵的转置意味着每一列与相应的一行进行互换。图4-23所示为一个简单的矩阵和它<br> 的转置。</p></li><li><p>以下是基于主机实现的使用单精度浮点值的错位转置算法。假设矩阵存储在一个一维<br> 数组中。通过改变数组索引值来交换行和列的坐标，可以很容易得到转置矩阵。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">void</span> <span class="token function">transposeHost</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> iy <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">;</span> <span class="token operator">++</span>iy<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> ix <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> ix <span class="token operator">&lt;</span> nx<span class="token punctuation">;</span> <span class="token operator">++</span>ix<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny<span class="token operator">+</span>iy<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx<span class="token operator">+</span>ix<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+H+'" alt="figure4-23" tabindex="0" loading="lazy"><figcaption>figure4-23</figcaption></figure><ul><li>在这个函数中有两个用一维数组存储的矩阵：输入矩阵in和转置矩阵out。矩阵维度被<br> 定义为nx行ny列。可以用一个一维数组执行转置操作，结果如图4-24所示。</li></ul><figure><img src="'+$+'" alt="figure4-24" tabindex="0" loading="lazy"><figcaption>figure4-24</figcaption></figure><ul><li><p>观察输入和输出布局，你会注意到：<br> 读：通过原矩阵的行进行访问，结果为合并访问<br> 写：通过转置矩阵的列进行访问，结果为交叉访问</p></li><li><p>交叉访问是使GPU性能变得最差的内存访问模式。但是，在矩阵转置操作中这是不可<br> 避免的。本节的剩余部分将侧重于使用两种转置核函数来提高带宽的利用率：一种是按行<br> 读取按列存储，另一种则是按列读取按行存储。</p></li><li><p>图4-25所示为第一种方法，图4-26所示为第二种方法。你能预测一下这两种实现的相<br> 对性能吗？如果禁用一级缓存加载，那么这两种实现的性能在理论上是相同的。但是，如<br> 果启用一级缓存，那么第二种实现的性能表现会更好。按列读取操作是不合并的（因此带<br> 宽将会浪费在未被请求的字节上），将这些额外的字节存入一级缓存意味着下一个读操作<br> 可能会在缓存上执行而不在全局内存上执行。因为写操作不在一级缓存中缓存，所以对按<br> 列执行写操作的例子而言，任何缓存都没有意义。在Kepler K10、K20和K20x设备中，这<br> 两种方法在性能上没有差别，因为一级缓存不用于全局内存访问。</p></li></ul><figure><img src="'+O+`" alt="figure4-25" tabindex="0" loading="lazy"><figcaption>figure4-25</figcaption></figure><h3 id="_4-4-2-1-为转置核函数设置性能的上限和下限" tabindex="-1"><a class="header-anchor" href="#_4-4-2-1-为转置核函数设置性能的上限和下限" aria-hidden="true">#</a> 4.4.2.1 为转置核函数设置性能的上限和下限</h3><ul><li>在执行矩阵转置核函数之前，可以先创建两个拷贝核函数来粗略计算所有转置核函数<br> 性能的上限和下限：</li><li>通过加载和存储行来拷贝矩阵（上限）。这样将模拟执行相同数量的内存操作作为<br> 转置，但是只能使用合并访问</li><li>通过加载和存储列来拷贝矩阵（下限）。这样将模拟执行相同数量的内存操作作为<br> 转置，但是只能使用交叉访问</li><li>核函数的实现如下:</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">copyRow</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
__global__ <span class="token keyword">void</span> <span class="token function">copyCol</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+F+'" alt="figure4-26" tabindex="0" loading="lazy"><figcaption>figure4-26</figcaption></figure>',26),ks=n("li",null,[n("p",null,[s("代码清单4-6提供了调用这些上限和下限核函数的主程序。你也可以从Wrox.com中的"),n("br"),s(" transpose.cu下载完整的代码。值得注意的是，通过调用主函数中的switch语句，可以使用"),n("br"),s(" 核函数标识符iKernel来选择需要调用的那个核函数。")])],-1),ds={href:"http://transpose.cu",target:"_blank",rel:"noopener noreferrer"},bs={class:"hint-container details"},ms=n("summary",null,"Click me to view the code!",-1),vs=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[n("span",{class:"token keyword"},"int"),s(),n("span",{class:"token function"},"main"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"int"),s(" argc"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),s("argv"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token comment"},"// set up device"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" dev "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 cudaDeviceProp deviceProp`),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaGetDeviceProperties"),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"&"),s("deviceProp"),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"%s starting transpose at "'),n("span",{class:"token punctuation"},","),s(" argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"device %d: %s "'),n("span",{class:"token punctuation"},","),s(" dev"),n("span",{class:"token punctuation"},","),s(" deviceProp"),n("span",{class:"token punctuation"},"."),s("name"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaSetDevice"),n("span",{class:"token punctuation"},"("),s("dev"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up array size 2048"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" nx "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},"<<"),n("span",{class:"token number"},"11"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" ny "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},"<<"),n("span",{class:"token number"},"11"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// select a kernel and block size"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" iKernel "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"0"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" blockx "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"16"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"int"),s(" blocky "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"16"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(" iKernel "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},")"),s(" blockx "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"2"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},")"),s(" blocky "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"3"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},")"),s(" nx "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"4"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("argc"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},")"),s(" ny "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"atoi"),n("span",{class:"token punctuation"},"("),s("argv"),n("span",{class:"token punctuation"},"["),n("span",{class:"token number"},"5"),n("span",{class:"token punctuation"},"]"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'" with matrix nx %d ny %d with kernel %d\\n"'),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token punctuation"},","),s("ny"),n("span",{class:"token punctuation"},","),s("iKernel"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 size_t nBytes `),n("span",{class:"token operator"},"="),s(" nx"),n("span",{class:"token operator"},"*"),s("ny "),n("span",{class:"token operator"},"*"),s(),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// execution configuration"),s(`
 dim3 `),n("span",{class:"token function"},"block"),s(),n("span",{class:"token punctuation"},"("),s("blockx"),n("span",{class:"token punctuation"},","),s("blocky"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 dim3 `),n("span",{class:"token function"},"grid"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),s("nx"),n("span",{class:"token operator"},"+"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("block"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),n("span",{class:"token punctuation"},"("),s("ny"),n("span",{class:"token operator"},"+"),s("block"),n("span",{class:"token punctuation"},"."),s("y"),n("span",{class:"token operator"},"-"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),s("block"),n("span",{class:"token punctuation"},"."),s("y"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate host memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("h_A "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("hostRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("gpuRef "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token function"},"malloc"),n("span",{class:"token punctuation"},"("),s("nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// initialize host array"),s(`
 `),n("span",{class:"token function"},"initialData"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token operator"},"*"),s("ny"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// transpose at host side"),s(`
 `),n("span",{class:"token function"},"transposeHost"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s("h_A"),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token punctuation"},","),s("ny"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// allocate device memory"),s(`
 `),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),s("d_A"),n("span",{class:"token punctuation"},","),n("span",{class:"token operator"},"*"),s("d_C"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaMalloc"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token operator"},"*"),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"&"),s("d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// copy data from host to device"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},","),s(" h_A"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyHostToDevice"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// warmup to avoide startup overhead"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iStart "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 warmup `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},","),s(" d_A"),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token punctuation"},","),s(" ny"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"double"),s(" iElaps "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"warmup elapsed %f sec\\n"'),n("span",{class:"token punctuation"},","),s("iElaps"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// kernel pointer and descriptor"),s(`
 `),n("span",{class:"token keyword"},"void"),s(),n("span",{class:"token punctuation"},"("),n("span",{class:"token operator"},"*"),s("kernel"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"float"),s(),n("span",{class:"token operator"},"*"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"int"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token keyword"},"int"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"char"),s(),n("span",{class:"token operator"},"*"),s("kernelName"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// set up kernel"),s(`
 `),n("span",{class:"token keyword"},"switch"),s(),n("span",{class:"token punctuation"},"("),s("iKernel"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token keyword"},"case"),s(),n("span",{class:"token number"},"0"),n("span",{class:"token operator"},":"),s(`
 kernel `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token operator"},"&"),s("copyRow"),n("span",{class:"token punctuation"},";"),s(`
 kernelName `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token string"},'"CopyRow "'),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"break"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"case"),s(),n("span",{class:"token number"},"1"),n("span",{class:"token operator"},":"),s(`
 kernel `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token operator"},"&"),s("copyCol"),n("span",{class:"token punctuation"},";"),s(`
 kernelName `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token string"},'"CopyCol "'),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"break"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(`
 `),n("span",{class:"token comment"},"// run kernel"),s(`
 iStart `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 kernel `),n("span",{class:"token operator"},"<<"),n("span",{class:"token operator"},"<"),s(" grid"),n("span",{class:"token punctuation"},","),s(" block "),n("span",{class:"token operator"},">>"),n("span",{class:"token operator"},">"),s(),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},","),s(" d_A"),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token punctuation"},","),s(" ny"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaDeviceSynchronize"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 iElaps `),n("span",{class:"token operator"},"="),s(),n("span",{class:"token function"},"seconds"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token operator"},"-"),s(" iStart"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// calculate effective_bandwidth"),s(`
 `),n("span",{class:"token keyword"},"float"),s(" ibnd "),n("span",{class:"token operator"},"="),s(),n("span",{class:"token number"},"2"),n("span",{class:"token operator"},"*"),s("nx"),n("span",{class:"token operator"},"*"),s("ny"),n("span",{class:"token operator"},"*"),n("span",{class:"token keyword"},"sizeof"),n("span",{class:"token punctuation"},"("),n("span",{class:"token keyword"},"float"),n("span",{class:"token punctuation"},")"),n("span",{class:"token operator"},"/"),n("span",{class:"token number"},"1e9"),n("span",{class:"token operator"},"/"),s("iElaps"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"printf"),n("span",{class:"token punctuation"},"("),n("span",{class:"token string"},'"%s elapsed %f sec <<< grid (%d,%d) block (%d,%d)>>> "'),s(`
 `),n("span",{class:"token string"},'"effective bandwidth %f GB\\n"'),n("span",{class:"token punctuation"},","),s(" kernelName"),n("span",{class:"token punctuation"},","),s(" iElaps"),n("span",{class:"token punctuation"},","),s(" grid"),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(" grid"),n("span",{class:"token punctuation"},"."),s("y"),n("span",{class:"token punctuation"},","),s(`
 block`),n("span",{class:"token punctuation"},"."),s("x"),n("span",{class:"token punctuation"},","),s(" block"),n("span",{class:"token punctuation"},"."),s("y"),n("span",{class:"token punctuation"},","),s(" ibnd"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// check kernel results"),s(`
 `),n("span",{class:"token keyword"},"if"),s(),n("span",{class:"token punctuation"},"("),s("iKernel"),n("span",{class:"token operator"},">"),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),s(),n("span",{class:"token punctuation"},"{"),s(`
 `),n("span",{class:"token function"},"cudaMemcpy"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},","),s(" d_C"),n("span",{class:"token punctuation"},","),s(" nBytes"),n("span",{class:"token punctuation"},","),s(" cudaMemcpyDeviceToHost"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"checkResult"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},","),s(" gpuRef"),n("span",{class:"token punctuation"},","),s(" nx"),n("span",{class:"token operator"},"*"),s("ny"),n("span",{class:"token punctuation"},","),s(),n("span",{class:"token number"},"1"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token punctuation"},"}"),s(`
 `),n("span",{class:"token comment"},"// free host and device memory"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"cudaFree"),n("span",{class:"token punctuation"},"("),s("d_C"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("h_A"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("hostRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token function"},"free"),n("span",{class:"token punctuation"},"("),s("gpuRef"),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token comment"},"// reset device"),s(`
 `),n("span",{class:"token function"},"cudaDeviceReset"),n("span",{class:"token punctuation"},"("),n("span",{class:"token punctuation"},")"),n("span",{class:"token punctuation"},";"),s(`
 `),n("span",{class:"token keyword"},"return"),s(" EXIT_SUCCESS"),n("span",{class:"token punctuation"},";"),s(`
`),n("span",{class:"token punctuation"},"}"),s(`
`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),fs=n("div",{class:"language-cpp line-numbers-mode","data-ext":"cpp"},[n("pre",{class:"language-cpp"},[n("code",null,[s(`启用一级负载缓存编译代码：
$ nvcc `),n("span",{class:"token operator"},"-"),s("arch"),n("span",{class:"token operator"},"="),s("sm_20 "),n("span",{class:"token operator"},"-"),s("Xptxas "),n("span",{class:"token operator"},"-"),s("dlcm"),n("span",{class:"token operator"},"="),s("ca transpose"),n("span",{class:"token punctuation"},"."),s("cu "),n("span",{class:"token operator"},"-"),s(`o transpose

运行以下指令检验两个拷贝核函数的性能：
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("transpose "),n("span",{class:"token number"},"0"),s(`
$ `),n("span",{class:"token punctuation"},"."),n("span",{class:"token operator"},"/"),s("transpose "),n("span",{class:"token number"},"1"),s(`


`)])]),n("div",{class:"line-numbers","aria-hidden":"true"},[n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"}),n("div",{class:"line-number"})])],-1),gs=l(`<ul><li>表4-4给出了在禁用ECC的Fermi M2090上的两个拷贝核函数的性能。这些测量值给出<br> 了上限和下限，它分别是理论峰值带宽的70%和33%。</li></ul><h3 id="_4-4-2-2-朴素转置-读取行与读取列" tabindex="-1"><a class="header-anchor" href="#_4-4-2-2-朴素转置-读取行与读取列" aria-hidden="true">#</a> 4.4.2.2 朴素转置：读取行与读取列</h3><ul><li>基于行的朴素转置核函数是基于主机实现的。这种转置按行加载按列存储：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">transposeNaiveRow</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>ix <span class="token operator">*</span> ny <span class="token operator">+</span> iy<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>iy <span class="token operator">*</span> nx <span class="token operator">+</span> ix<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>通过互换读索引和写索引，就生成了基于列的朴素转置核函数。这种转置按列加载按<br> 行存储：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">transposeNaiveCol</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>内核switch语句中添加了以下几种情况：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">case</span> <span class="token number">2</span><span class="token operator">:</span>
 kernel <span class="token operator">=</span> <span class="token operator">&amp;</span>transposeNaiveRow<span class="token punctuation">;</span> 
 kernelName <span class="token operator">=</span> <span class="token string">&quot;NaiveRow &quot;</span><span class="token punctuation">;</span>
 <span class="token keyword">break</span><span class="token punctuation">;</span>
<span class="token keyword">case</span> <span class="token number">3</span><span class="token operator">:</span>
 kernel <span class="token operator">=</span> <span class="token operator">&amp;</span>transposeNaiveCol<span class="token punctuation">;</span>
 kernelName <span class="token operator">=</span> <span class="token string">&quot;NaiveCol &quot;</span><span class="token punctuation">;</span>
 <span class="token keyword">break</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>启用一级缓存来重新编译代码，并运行以下指令测试这两个转置核函数的性能。结果<br> 总结如表4-5所示。<br> $ ./transpose 2<br> $ ./transpose 3</li></ul><figure><img src="`+N+`" alt="table4-5" tabindex="0" loading="lazy"><figcaption>table4-5</figcaption></figure><ul><li><p>结果表明，使用NaiveCol方法比NaiveRow方法性能表现得更好。如前面所述，导致<br> 这种性能提升的一个可能原因是在缓存中执行了交叉读取。即使通过某一方式读入一级缓<br> 存中的数据没有都被这次访问使用到，这些数据仍留在缓存中，在以后的访问过程中可能<br> 发生缓存命中。为了验证这种情况，试着通过禁用一级缓存（使用-Xptxas-dlcm=cg）来重<br> 新编译。</p></li><li><p>运行以下指令来获取禁用一级缓存加载的所有核函数的性能。表4-6中的结果清楚地<br> 说明了禁用缓存加载对交叉读取访问模式有显著影响。<br> $ ./transpose 0<br> $ ./transpose 1<br> $ ./transpose 2<br> $ ./transpose 3</p></li><li><p>结果表明，通过缓存交叉读取能够获得最高的加载吞吐量。在缓存读取的情况下，每<br> 个内存请求由一个128字节的缓存行来完成。按列读取数据，使得线程束里的每个内存请<br> 求都会重复执行32次（因为交叉读取2048个数据元素），一旦数据预先存储到了一级缓存<br> 中，那么许多当前全局内存读取就会有良好的隐藏延迟并取得较高的一级缓存命中率。</p></li><li><p>接下来，可以使用以下指标来衡量加载/存储效率</p></li><li><p>结果表明，对于NaiveCol实现而言，由于合并写入，存储请求从未被重复执行，但是<br> 由于交叉读取，多次重复执行了加载请求。这证明了即使是较低的加载效率，一级缓存中<br> 的缓存加载也可以限制交叉加载对性能的负面影响。</p></li></ul><h3 id="_4-4-2-3-展开转置-读取行与读取列" tabindex="-1"><a class="header-anchor" href="#_4-4-2-3-展开转置-读取行与读取列" aria-hidden="true">#</a> 4.4.2.3 展开转置：读取行与读取列</h3><ul><li><p>接下来，我们将利用展开技术来提高转置内存带宽的利用率。在这个例子中，展开的<br> 目的是为每个线程分配更独立的任务，从而最大化当前内存请求。</p></li><li><p>以下是一个展开因子为4的基于行的实现。这里引入了两个新的数组索引：一个用于<br> 行访问，另一个用于列访问。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">transposeUnroll4Row</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span><span class="token number">4</span> <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ti <span class="token operator">=</span> iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">;</span> <span class="token comment">// access in rows</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> to <span class="token operator">=</span> ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">;</span> <span class="token comment">// access in columns</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix<span class="token operator">+</span><span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>to<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ti<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>to <span class="token operator">+</span> ny<span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ti<span class="token operator">+</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>to <span class="token operator">+</span> ny<span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ti<span class="token operator">+</span><span class="token number">2</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>to <span class="token operator">+</span> ny<span class="token operator">*</span><span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ti<span class="token operator">+</span><span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>使用相似的展开交换读索引和写索引产生一个基于列的实现：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">transposeUnroll4Col</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> nx<span class="token punctuation">,</span> 
 <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>x<span class="token operator">*</span><span class="token number">4</span> <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ti <span class="token operator">=</span> iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">;</span> <span class="token comment">// access in rows</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> to <span class="token operator">=</span> ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">;</span> <span class="token comment">// access in columns</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix<span class="token operator">+</span><span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>ti<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>to<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>ti <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>to<span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token operator">*</span>ny<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>ti <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>to<span class="token operator">+</span> <span class="token number">2</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token operator">*</span>ny<span class="token punctuation">]</span><span class="token punctuation">;</span>
 out<span class="token punctuation">[</span>ti <span class="token operator">+</span> <span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>to<span class="token operator">+</span> <span class="token number">3</span><span class="token operator">*</span>blockDim<span class="token punctuation">.</span>x<span class="token operator">*</span>ny<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>向核函数switch语句中添加以下代码段。注意，由于添加了展开操作，这些核函数的<br> 网格大小需要做出相应调整。<br> case 4:<br> kernel = &amp; transposeUnroll4Row;<br> kernelName = &quot; Unroll4Row &quot;;<br> grid.x = (nx+block.x<em>4-1)/(block.x</em>4);<br> break;<br> case 5:<br> kernel = &amp; transposeUnroll4Col;<br> kernelName = &quot; Unroll4Col &quot;;<br> grid.x = (nx+block.x<em>4-1)/(block.x</em>4);<br> break;</p></li><li><p>启用一级缓存来重新编译代码，并运行以下指令以比较两个新的转置核函数的性能。<br> 在Fermi M2090上运行的结果如表4-9所示。<br> $ ./transpose 4<br> $ ./transpose 5</p></li></ul><figure><img src="`+K+'" alt="table4-9" tabindex="0" loading="lazy"><figcaption>table4-9</figcaption></figure><ul><li><p>注：块大小：16×16；矩阵大小：2048×2048；有效带宽的单位：GB/s。</p></li><li><p>此外，通过启用一级缓存，按列加载和按行存储获得了更好的有效带宽和整体执行时间。</p></li></ul><h3 id="_4-4-2-4-对角转置-读取行与读取列" tabindex="-1"><a class="header-anchor" href="#_4-4-2-4-对角转置-读取行与读取列" aria-hidden="true">#</a> 4.4.2.4 对角转置：读取行与读取列</h3><ul><li><p>当启用一个线程块的网格时，线程块会被分配给SM。编程模型抽象可能用一个一维<br> 或二维布局来表示该网格，但是从硬件的角度来看，所有块都是一维的。每个线程块都有<br> 其唯一标识符bid，它可以用网格中的线程块按行优先顺序计算得出：<br> int bid = blockIdx.y * gridDim.x + blockIdx.x;</p></li><li><p>图4-27所示为一个4×4的线程块网格，它包含了每个线程块的ID。</p></li><li><p>当启用一个核函数时，线程块被分配给SM的顺序由块ID来确定。一旦所有的SM都被<br> 完全占用，所有剩余的线程块都保持不变直到当前的执行被完成。一旦一个线程块执行结<br> 束，将为该SM分配另一个线程块。由于线程块完成的速度和顺序是不确定的，随着内核<br> 进程的执行，起初通过bid相连的活跃线程块会变得不太连续了。</p></li><li><p>尽管无法直接调控线程块的顺序，但你可以灵活地使用块坐标blockIdx.x和<br> blockIdx.y。图4-27说明了笛卡尔坐标系下的块坐标。图4-28展示了一个表示blockIdx.x和<br> block-Idx.y的不同方法：使用对角块坐标系。<br><img src="'+V+'" alt="figure4-27" loading="lazy"></p></li></ul><figure><img src="'+Z+`" alt="figure4-28" tabindex="0" loading="lazy"><figcaption>figure4-28</figcaption></figure><ul><li>对角坐标系用于确定一维线程块的ID，但对于数据访问，仍需使用笛卡尔坐标系。因<br> 此，当用对角坐标表示块ID时，需要将对角坐标映射到笛卡尔坐标中，以便可以访问到正<br> 确的数据块。对于一个方阵来说，这个映射可以通过以下方程式计算得出：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>block_x <span class="token operator">=</span> <span class="token punctuation">(</span>blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> blockIdx<span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">%</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
block_y <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>这里，blockIdx.x和blockIdx.y为对角坐标。block_x和block_y是它们对应的笛卡尔坐<br> 标。基于行的矩阵转置核函数使用如下所示的对角坐标。在核函数的起始部分包含了从对<br> 角坐标到笛卡尔坐标的映射计算，然后使用映射的笛卡尔坐标（block_x，block_y）来计<br> 算线程索引（ix，iy）。这个对角转置核函数会影响线程块分配数据块的方式。下面的核<br> 函数使用了对角线程块坐标，它借助合并读取和交叉写入实现了矩阵的转置：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>__global__ <span class="token keyword">void</span> <span class="token function">transposeDiagonalRow</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> 
 <span class="token keyword">int</span> nx<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> blk_y <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> blk_x <span class="token operator">=</span> <span class="token punctuation">(</span>blockIdx<span class="token punctuation">.</span>x<span class="token operator">+</span>blockIdx<span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token operator">%</span>gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blk_x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blk_y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>


使用基于列的对角坐标的核函数如下所示：
__global__ <span class="token keyword">void</span> <span class="token function">transposeDiagonalCol</span><span class="token punctuation">(</span><span class="token keyword">float</span> <span class="token operator">*</span>out<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>in<span class="token punctuation">,</span> <span class="token keyword">const</span> 
 <span class="token keyword">int</span> nx<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">int</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> blk_y <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> blk_x <span class="token operator">=</span> <span class="token punctuation">(</span>blockIdx<span class="token punctuation">.</span>x<span class="token operator">+</span>blockIdx<span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token operator">%</span>gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ix <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> blk_x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> iy <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">*</span> blk_y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span>
 <span class="token keyword">if</span> <span class="token punctuation">(</span>ix <span class="token operator">&lt;</span> nx <span class="token operator">&amp;&amp;</span> iy <span class="token operator">&lt;</span> ny<span class="token punctuation">)</span> <span class="token punctuation">{</span>
 out<span class="token punctuation">[</span>iy<span class="token operator">*</span>nx <span class="token operator">+</span> ix<span class="token punctuation">]</span> <span class="token operator">=</span> in<span class="token punctuation">[</span>ix<span class="token operator">*</span>ny <span class="token operator">+</span> iy<span class="token punctuation">]</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span>
<span class="token punctuation">}</span>


向核函数<span class="token keyword">switch</span>语句中添加以下代码来调用这些核函数：
<span class="token keyword">case</span> <span class="token number">6</span><span class="token operator">:</span>
 kernel <span class="token operator">=</span> <span class="token operator">&amp;</span>transposeDiagonalRow<span class="token punctuation">;</span> 
 kernelName <span class="token operator">=</span> <span class="token string">&quot;DiagonalRow &quot;</span><span class="token punctuation">;</span>
 <span class="token keyword">break</span><span class="token punctuation">;</span>
<span class="token keyword">case</span> <span class="token number">7</span><span class="token operator">:</span>
 kernel <span class="token operator">=</span> <span class="token operator">&amp;</span>transposeDiagonalCol<span class="token punctuation">;</span>
 kernelName <span class="token operator">=</span> <span class="token string">&quot;DiagonalCol &quot;</span><span class="token punctuation">;</span>
 <span class="token keyword">break</span><span class="token punctuation">;</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>启用一级缓存来重新编译代码，并运行以下指令来比较两个转置核函数的性能。结果<br> 总结如表4-10所示。<br> $ ./transpose 6<br> $ ./transpose 7</li></ul><figure><img src="`+q+'" alt="table4-10" tabindex="0" loading="lazy"><figcaption>table4-10</figcaption></figure><ul><li><p>通过使用对角坐标系来修改线程块的执行顺序，这使基于行的核函数性能得到了大大<br> 提升。但是，基于列的核函数在使用笛卡尔块坐标系仍然比使用对角块坐标系表现得更<br> 好。对角核函数的实现可以通过展开块得到更大的提升，但是这种实现不像使用基于笛卡<br> 尔坐标系的核函数那样简单直接。</p></li><li><p>这种性能提升的原因与DRAM的并行访问有关。发送给全局内存的请求由DRAM分区<br> 完成。设备内存中的连续的256字节区域被分配到连续的分区。当使用笛卡尔坐标将线程<br> 块映射到数据块时，全局内存访问可能无法均匀地被分配到整个DRAM从分区中，这时就<br> 可能发生“分区冲突”的现象。发生分区冲突时，内存请求在某些分区中排队等候，而另一<br> 些分区一直未被调用。因为对角坐标映射造成了从线程块到待处理数据块的非线性映射，<br> 所以交叉访问不太可能会落入到一个独立的分区中，并且会带来性能的提升。</p></li><li><p>对最佳性能来说，被所有活跃的线程束并发访问的全局内存应该在分区中被均匀地划<br> 分。图4-29所示为一个简化的可视化模型，它表示了使用笛卡尔坐标表示块ID时的分区冲<br> 突。在这个图中，假设通过两个分区访问全局内存，每个分区的宽度为256个字节，并且<br> 使用一个大小为32×32的线程块启动核函数。如果每个数据块的宽度是128个字节，那么<br> 需要使用两个分区为第0、1.2.3个线程块加载数据。但现实是，只能使用一个分区为第0、<br> 1.2.3个线程块存储数据，因此造成了分区冲突。</p></li><li><p>图4-30借用了图4-29中的简化模型，但这次使用对角坐标来表示块ID。在这种情况<br> 下，需要使用两个分区为第0、1.2.3个线程块加载和存储数据。加载和存储请求在两个分<br> 区之间被均匀分配。这个例子说明了为什么对角核函数能够获得更好的性能。</p></li></ul><figure><img src="'+L+'" alt="figure4-29" tabindex="0" loading="lazy"><figcaption>figure4-29</figcaption></figure><figure><img src="'+W+'" alt="figure4-30" tabindex="0" loading="lazy"><figcaption>figure4-30</figcaption></figure><h3 id="_4-4-2-5-使用瘦块来增加并行性" tabindex="-1"><a class="header-anchor" href="#_4-4-2-5-使用瘦块来增加并行性" aria-hidden="true">#</a> 4.4.2.5 使用瘦块来增加并行性</h3><ul><li>增加并行性最简单的方式是调整块的大小。之前的几节内容已经证明了这种简单的技<br> 术对提高性能的有效性。进一步对使用基于列的NaiveCol核函数的块大小进行试验（通过<br> 核函数switch语句中的case 3进行访问）。块大小的测试结果列于表4-11中。</li><li>表4-11 核函数的有效带宽（启用一级缓存）</li></ul><figure><img src="'+X+'" alt="table4-11" tabindex="0" loading="lazy"><figcaption>table4-11</figcaption></figure><ul><li><p>目前最佳的块大小为（8，32），尽管它与大小为（16，16）的块显示了相同的并行<br> 性，但这种性能的提升是由“瘦的”块（8，32）带来的，如图4-31所示。<br><img src="'+j+`" alt="figure4-31" loading="lazy"></p></li><li><p>通过增加存储在线程块中连续元素的数量，“瘦”块可以提高存储操作的效率（如表4-<br> 12所示）。你可以通过使用nvprof来测量加载和存储的吞吐量以证实这一点。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ nvprof <span class="token operator">--</span>devices <span class="token number">0</span> <span class="token operator">--</span>metrics gld_throughput<span class="token punctuation">,</span>gst_throughput\\
 <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">3</span> <span class="token number">16</span>
$ nvprof <span class="token operator">--</span>devices <span class="token number">0</span> <span class="token operator">--</span>metrics gld_throughput<span class="token punctuation">,</span>gst_throughput\\
 <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">3</span> <span class="token number">8</span> <span class="token number">32</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+J+`" alt="table4-12" tabindex="0" loading="lazy"><figcaption>table4-12</figcaption></figure><ul><li>接下来，尝试用以下指令来比较不同核函数实现的带宽，使用的块大小为（8，32）。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">0</span> <span class="token number">8</span> <span class="token number">32</span>
$ <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">1</span> <span class="token number">8</span> <span class="token number">32</span>
$ <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">2</span> <span class="token number">8</span> <span class="token number">32</span>
$ <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">3</span> <span class="token number">8</span> <span class="token number">32</span>
$ <span class="token punctuation">.</span><span class="token operator">/</span>transpose <span class="token number">4</span> <span class="token number">8</span> <span class="token number">32</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>示例结果如表4-13所示。到目前为止，核函数Unroll4Col性能表现得最好，甚至优于<br> 上限拷贝核函数。有效带宽达到了峰值带宽的60%～80%，这一结果是很令人满意的。</li></ul><figure><img src="`+Q+`" alt="table4-13" tabindex="0" loading="lazy"><figcaption>table4-13</figcaption></figure><h2 id="_4-5-使用统一内存的矩阵加法" tabindex="-1"><a class="header-anchor" href="#_4-5-使用统一内存的矩阵加法" aria-hidden="true">#</a> 4.5 使用统一内存的矩阵加法</h2><ul><li><p>在第2章中你已经学习了如何在GPU中添加两个矩阵。为了简化主机和设备内存空间<br> 的管理，提高这个CUDA程序的可读性和易维护性，可以使用统一内存将以下解决方案添<br> 加到矩阵加法的主函数中：<br> 用托管内存分配来替换主机和设备内存分配，以消除重复指针<br> 删除所有显式的内存副本</p></li><li><p>首先，声明和分配3个托管数组，其中数组A和B用于输入，数组gpuRef用于输出：</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token keyword">float</span> <span class="token operator">*</span>A<span class="token punctuation">,</span> <span class="token operator">*</span>B<span class="token punctuation">,</span> <span class="token operator">*</span>gpuRef<span class="token punctuation">;</span> 
<span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>A<span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>B<span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">cudaMallocManaged</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>gpuRef<span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

然后，使用指向托管内存的指针来初始化主机上的两个输入矩阵：
<span class="token function">initialData</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> nxy<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">initialData</span><span class="token punctuation">(</span>B<span class="token punctuation">,</span> nxy<span class="token punctuation">)</span><span class="token punctuation">;</span>

最后，通过指向托管内存的指针调用矩阵加法核函数：
sumMatrixGPU<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid<span class="token punctuation">,</span> block<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> gpuRef<span class="token punctuation">,</span> nx<span class="token punctuation">,</span> ny<span class="token punctuation">)</span><span class="token punctuation">;</span> 
<span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,44),ys=n("li",null,[n("p",null,[s("因为核函数的启动与主机程序是异步的，并且内存块cudaMemcpy的调用不需要使用"),n("br"),s(" 托管内存，所以在直接访问核函数输出之前，需要在主机端显式地同步。相比于之前未托"),n("br"),s(" 管内存的矩阵加法程序，这里的代码因为使用了统一内存而明显被简化了。")])],-1),_s=n("br",null,null,-1),hs={href:"http://xn--sumMatrixGPUManual-jq53az15mhpl5l2k.cu",target:"_blank",rel:"noopener noreferrer"},xs=n("br",null,null,-1),ws=n("br",null,null,-1),As=n("br",null,null,-1),Ds=n("br",null,null,-1),Ms=l(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ nvcc <span class="token operator">-</span>arch<span class="token operator">=</span>sm_30 sumMatrixGPUManaged<span class="token punctuation">.</span>cu <span class="token operator">-</span>o managed
$ nvcc <span class="token operator">-</span>arch<span class="token operator">=</span>sm_30 sumMatrixGPUManual<span class="token punctuation">.</span>cu <span class="token operator">-</span>o manual
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>如果在一个多GPU设备的系统上进行测试，托管应用需要附加的步骤。因为托管内存<br> 分配对系统中的所有设备是可见的，所以可以限制哪一个设备对应用程序可见，这样托管<br> 内存便可以只分配在一个设备上。为此，设置环境变量CUDA_VISIBLE_DEVICES来使一<br> 个GPU对CUDA应用程序可见：</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>$ <span class="token keyword">export</span> CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span>

首先，运行托管程序：
$ <span class="token punctuation">.</span><span class="token operator">/</span>managed <span class="token number">14</span>

Kepler K40的实测结果如下所示：
$ <span class="token punctuation">.</span><span class="token operator">/</span>managed 
Starting <span class="token keyword">using</span> Device <span class="token number">0</span><span class="token operator">:</span> Tesla K40m
Matrix size<span class="token operator">:</span> nx <span class="token number">16384</span> ny <span class="token number">16384</span>
initialization<span class="token operator">:</span> <span class="token number">5.930170</span> sec
sumMatrix on host<span class="token operator">:</span> <span class="token number">0.504631</span> sec
sumMatrix on gpu <span class="token operator">:</span> <span class="token number">0.025203</span> sec <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>

下一步，运行不使用托管内存的manual程序：
$ <span class="token punctuation">.</span><span class="token operator">/</span>manual <span class="token number">14</span>

Kepler K40的实测结果如下所示：
$ <span class="token punctuation">.</span><span class="token operator">/</span>manual 
Starting <span class="token keyword">using</span> Device <span class="token number">0</span><span class="token operator">:</span> Tesla K40m
Matrix size<span class="token operator">:</span> nx <span class="token number">16384</span> ny <span class="token number">16384</span>
initialization<span class="token operator">:</span> <span class="token number">1.835069</span> sec
sumMatrix on host<span class="token operator">:</span> <span class="token number">0.474370</span> sec
sumMatrix on gpu <span class="token operator">:</span> <span class="token number">0.020226</span> sec <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>

这些结果表明，使用托管内存的核函数速度与显式地在主机和设备之间复制数据几乎
一样快，并且很明显它需要更少的编程工作。
用nvprof跟踪两个程序，如下所示：
$ nvprof <span class="token operator">--</span>profile<span class="token operator">-</span>api<span class="token operator">-</span>trace runtime <span class="token punctuation">.</span><span class="token operator">/</span>managed 
$ nvprof <span class="token operator">--</span>profile<span class="token operator">-</span>api<span class="token operator">-</span>trace runtime <span class="token punctuation">.</span><span class="token operator">/</span>manual
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>Kepler K40的运行结果总结在了表4-14中。影响性能差异的最大因素在于CPU数据的<br> 初始化时间——使用托管内存耗费的时间更长。矩阵最初是在GPU上被分配的，由于矩阵<br> 是用初始值填充的，所以首先会在CPU上引用。这就要求底层系统在初始化之前，将矩阵<br> 中的数据从设备传输到主机中，这是manual版的核函数中不执行的传输。</li></ul><figure><img src="`+Y+`" alt="table4-14" tabindex="0" loading="lazy"><figcaption>table4-14</figcaption></figure><ul><li><p>当执行主机端矩阵求和函数时，整个矩阵都在CPU上了，因此执行时间比非托管内存<br> 要短。接下来，warm-up核函数将整个矩阵迁回设备中，这样当实际的矩阵加法核函数被<br> 启动时，数据已经在GPU上了。如果没有执行warm-up核函数，使用托管内存的核函数明<br> 显运行得更慢。</p></li><li><p>nvvp和nvprof支持检验统一内存的性能。这两种分析器都可以测量系统中每个GPU统<br> 一内存的通信量。默认情况是不执行该功能的。通过以下的nvprof标志启用统一内存相关<br> 指标。</p></li><li><p>$ nvprof --unified-memory-profiling per-process-device ./managed</p></li><li><p>--print-gpu-trace还提供统一内存的行为信息。以下是运行在Kepler K40上的部分结果。</p></li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token operator">==</span><span class="token number">28893</span><span class="token operator">==</span> Unified Memory profiling result<span class="token operator">:</span>
Device <span class="token string">&quot;Tesla K40m (0)&quot;</span>
 Count Avg Min Max
 Host To <span class="token function">Device</span> <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span> <span class="token number">8</span> <span class="token number">1.3422e+08</span> <span class="token number">0</span> <span class="token number">2.68e+08</span>
 Device To <span class="token function">Host</span> <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span> <span class="token number">507</span> <span class="token number">5490570.86</span> <span class="token number">0</span> <span class="token number">4.03e+08</span>
 CPU Page faults <span class="token number">507</span> <span class="token number">48909.01</span> <span class="token number">0</span> <span class="token number">98304</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>注意，在进行设备到主机传输数据时，将CPU的页故障报告给设备。当主机应用程序<br> 引用一个CPU虚拟内存地址而不是物理内存地址时，就会出现页面故障。当CPU需要访问<br> 当前驻留在GPU中的托管内存时，统一内存使用CPU页面故障来触发设备到主机的数据传<br> 输。测试出的页面故障的数量与数据大小密切相关。尝试用一个含有256×256个元素的矩<br> 阵重新运行程序：<br> $ nvprof --unified-memory-profiling per-process-device ./managed 8</li><li>结果如下。注意这时页面故障的数量大大减少。</li></ul><div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token operator">==</span><span class="token number">29464</span><span class="token operator">==</span> Unified Memory profiling result<span class="token operator">:</span>
Device <span class="token string">&quot;Tesla K40m (0)&quot;</span>
 Count Avg Min Max
 Host To <span class="token function">Device</span> <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span> <span class="token number">2</span> <span class="token number">524288.00</span> <span class="token number">0</span> <span class="token number">1048576</span>
 Device To <span class="token function">Host</span> <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span> <span class="token number">9</span> <span class="token number">505628.44</span> <span class="token number">0</span> <span class="token number">1572864</span>
 CPU Page faults <span class="token number">9</span> <span class="token number">123.44</span> <span class="token number">0</span> <span class="token number">384</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>你也可以使用nvvp查看统一内存的行为。如下所示为在使用托管内存的实现中启动nvvp：<br> $ nvvp ./managed</p></li><li><p>在可执行属性标签“创建新会话”中，在“参数”区域输入14来测试一个大型矩阵，然后<br> 选择“下一步”。接着选择“启用统一内存分析”复选框，如图4-32所示。<br><img src="`+nn+'" alt="figure4-32" loading="lazy"></p></li><li><p>图4-33所示为托管程序的时间线。从图中可以看出，主机的页面故障与DtoH数据传输<br> 密切相关。通过将数据传输到GPU，统一内存实现了性能优化。底层系统可以维护主机和<br> 设备之间的一致性，并将数据放在可以对其高效访问的地方。<br><img src="'+sn+'" alt="figure4-33" loading="lazy"></p></li><li><p>图4-34说明了没有使用统一内存的程序时间线。比较图4-33和图4-34，可以看到，显<br> 式地管理数据移动只使用一次设备到主机的传输，而使用统一内存却要使用两次传输。<br><img src="'+an+'" alt="figure4-34" loading="lazy"></p></li><li><p>CUDA 6.0中发布的统一内存是用来提高程序员的编程效率的。底层系统强调性能的<br> 一致性和正确性。结果表明，在CUDA应用中手动优化数据移动的性能比使用统一内存的<br> 性能要更优，可以肯定的是，NVIDIA公司未来计划推出的硬件和软件将支持统一内存的<br> 性能提升和可编程性。</p></li></ul><h2 id="_4-6-总结" tabindex="-1"><a class="header-anchor" href="#_4-6-总结" aria-hidden="true">#</a> 4.6 总结</h2><ul><li><p>CUDA编程模型的一个显著特点是有对程序员直接可用的GPU内存层次结构。这对数<br> 据移动和布局提供了更多的控制，优化了性能并得到了更高的峰值性能。</p></li><li><p>全局内存是最大的、延迟最高的、最常用的内存。对全局内存的请求可以由32个字节<br> 或128个字节的事务来完成。记住这些特点和粒度对于调控应用程序中全局内存的使用是<br> 很重要的。</p></li><li><p>通过本章的示例，我们学习了以下两种提高带宽利用率的方法：<br> 最大化当前并发内存访问的次数<br> 最大化在总线上的全局内存和SM片上内存之间移动字节的利用率</p></li><li><p>为保持有足够多的正在执行的内存操作，可以使用展开技术在每个线程中创建更多的<br> 独立内存请求，或调整网格和线程块的执行配置来体现充分的SM并行性。</p></li><li><p>为了避免在设备内存和片上内存之间有未使用数据的移动，应该努力实现理想的访问<br> 模式：对齐和合并内存访问。</p></li><li><p>对齐内存访问相对容易，但有时合并访问比较有挑战性。一些算法本身就无法合并访<br> 问，或实现起来有一定的困难。</p></li><li><p>改进合并访问的重点在于线程束中的内存访问模式。另一方面，消除分区冲突的重点<br> 则在于所有活跃线程束的访问模式。对角坐标映射是一种通过调整块执行顺序来避免分区<br> 冲突的方法。</p></li><li><p>通过消除重复指针以及在主机和设备之间显式传输数据的需要，统一内存大大简化了<br> CUDA编程。CUDA 6.0中统一内存的实现明显地保持了性能的一致性和优越性。未来硬件<br> 和软件的提升将会提高统一内存的性能。</p></li><li><p>下一章将详细介绍在本章简要提到的两个话题：常量内存和共享内存。</p></li></ul><h2 id="_4-7-习题" tabindex="-1"><a class="header-anchor" href="#_4-7-习题" aria-hidden="true">#</a> 4.7 习题</h2>',13),Ss={href:"http://1.xn--globalVariable-k79vm57csi5e3v9e.cu",target:"_blank",rel:"noopener noreferrer"},Cs=n("br",null,null,-1),Us=n("br",null,null,-1),Ps={href:"http://2.xn--globalVariable-k79vm57csi5e3v9e.cu",target:"_blank",rel:"noopener noreferrer"},Bs=n("br",null,null,-1),zs=n("br",null,null,-1),Is=n("li",null,[n("p",null,[s("3.在memTransfer和pinMemTransfer中比较固定内存和可分页内存拷贝的性能，利用"),n("br"),s(" nvprof并选择不同的内存大小：2M、4M、8M、16M、32M、64M、128M。")])],-1),Es=n("li",null,[n("p",null,[s("4.用相同的例子，比较固定内存空间与可分页内存空间的分配和释放性能，利用CPU"),n("br"),s(" 定时器并选择不同的内存大小：2M、4M、8M、16M、32M、64M、128M。")])],-1),Rs={href:"http://5.xn--sumArrayZerocopy-sg6zn71z.cu",target:"_blank",rel:"noopener noreferrer"},Gs=n("br",null,null,-1),Ts={href:"http://6.xn--sumArrayZerocopyUVA-p854avz90a.cu",target:"_blank",rel:"noopener noreferrer"},Hs=n("br",null,null,-1),$s=n("br",null,null,-1),Os=n("li",null,[n("p",null,[s("7.在偏移量分别为0、4、8、16、32、64、96、128、160、192、224和256的情况下，"),n("br"),s(" 编译readSeg-ment.cu文件并运行以下命令：./iread $OFFSET证明对齐的地址必须是哪个字节的倍数。")])],-1),Fs=n("li",null,[n("p",null,[s("8.对于readSegment.cu参考文件Makefile。在Makefile中禁用一级中缓存并生成可执行"),n("br"),s(" 的iread_l2。在偏移量分别为0、11和128时用以下命令来测试它：./iread_l2 $OFFSET与启用一级缓存的结果相比，看看有什么不同。")])],-1),Ns=n("li",null,[n("p",null,"9.运行以下指令，令其偏移量分别为0、11、128：")],-1),Ks=l(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code>nvprof <span class="token operator">--</span>devices <span class="token number">0</span> \\
 <span class="token operator">--</span>metrics gld_efficiency \\
 <span class="token operator">--</span>metrics gld_throughput \\
 <span class="token punctuation">.</span><span class="token operator">/</span>iread_l2 $OFFSET<span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),Vs={href:"http://10.xn--simpleMathAoS-ot3uf27ct70eed4e.cu",target:"_blank",rel:"noopener noreferrer"},Zs=n("br",null,null,-1),qs=n("li",null,[n("p",null,[s("11.基于练习10，将核函数修改为只读/写变量x。与simpleMathSoA.cu文件比较结果。"),n("br"),s(" 使用合适的nvprof指标来解释这种差异。")])],-1),Ls={href:"http://12.xn--writeSegment-sf7sh15cu16dot2e.cu",target:"_blank",rel:"noopener noreferrer"},Ws=n("br",null,null,-1),Xs=n("br",null,null,-1),js=l("<li><p>13.对readWriteOffset使用展开因子4，并与原程序比较性能，用合适的nvprof指标来解<br> 释这种差异。</p></li><li><p>14.为核函数readWriteOffset和readWriteOffsetUnroll4的执行配置找到最佳的配置，使用<br> 合适的指标来解释为什么这组配置更好。</p></li><li><p>15.参考核函数tranposeUnroll4Row。实现一个新的核函数tranposeRow，让这个核函数<br> 中的每个线程都处理一行中的所有元素。与现有的核函数进行性能比较，并使用合适的指<br> 标来解释其差异。</p></li><li><p>16.参考核函数tranposeUnroll4Row。实现一个新的核函数tranposeUnroll8Row，让这个<br> 核函数中的每个线程都处理8个元素。与现有的核函数进行性能比较，并使用合适的指标<br> 来解释这种差异。</p></li><li><p>17.参考核函数transposeDiagonalCol和tranposeUnroll4Row。实现一个新的核函数<br> transposeDiago-nalColUnroll4，让这个核函数中的每个线程都处理4个元素。与现有的核函<br> 数进行性能比较，并使用合适的指标来解释这种差异。</p></li>",5),Js={href:"http://18.xn--sumArrayZerocpy-tn2z965krn5gqyxa.cu",target:"_blank",rel:"noopener noreferrer"},Qs=n("br",null,null,-1),Ys={href:"http://19.xn--sumMatrixGPUManaged-p816a942mvm9hdy2a.cu",target:"_blank",rel:"noopener noreferrer"},na=n("br",null,null,-1),sa={href:"http://20.xn--sumMatrixGPUManaged-p816a942mvm9hdy2a.cu",target:"_blank",rel:"noopener noreferrer"},aa=n("br",null,null,-1),ta=l(`<div class="language-cpp line-numbers-mode" data-ext="cpp"><pre class="language-cpp"><code><span class="token function">memset</span><span class="token punctuation">(</span>hostRef<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">memset</span><span class="token punctuation">(</span>gpuRef<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> nBytes<span class="token punctuation">)</span><span class="token punctuation">;</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1);function ea(oa,pa){const e=u("router-link"),i=u("CodeTabs"),c=u("ExternalLinkIcon");return k(),d("div",null,[en,on,b(" more "),n("nav",pn,[n("ul",null,[n("li",null,[a(e,{to:"#简单介绍主要是基础"},{default:t(()=>[s("简单介绍主要是基础")]),_:1})]),n("li",null,[a(e,{to:"#第4章-全局内存"},{default:t(()=>[s("第4章 全局内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-cuda内存模型概述"},{default:t(()=>[s("4.1 CUDA内存模型概述")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_4-1-1-内存层次结构的优点"},{default:t(()=>[s("4.1.1 内存层次结构的优点")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-cuda内存模型"},{default:t(()=>[s("4.1.2 CUDA内存模型")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-1-寄存器-重点"},{default:t(()=>[s("4.1.2.1 寄存器[重点]")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-2-本地内存"},{default:t(()=>[s("4.1.2.2 本地内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-3-共享内存"},{default:t(()=>[s("4.1.2.3 共享内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-4-常量内存"},{default:t(()=>[s("4.1.2.4 常量内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-5-纹理内存"},{default:t(()=>[s("4.1.2.5 纹理内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-6-全局内存"},{default:t(()=>[s("4.1.2.6 全局内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-7-gpu缓存"},{default:t(()=>[s("4.1.2.7 GPU缓存")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-8-cuda变量声明总结"},{default:t(()=>[s("4.1.2.8 CUDA变量声明总结")]),_:1})]),n("li",null,[a(e,{to:"#_4-1-2-9-静态全局内存"},{default:t(()=>[s("4.1.2.9 静态全局内存")]),_:1})])])]),n("li",null,[a(e,{to:"#_4-2-内存管理"},{default:t(()=>[s("4.2 内存管理")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_4-2-1-内存分配和释放"},{default:t(()=>[s("4.2.1 内存分配和释放")]),_:1})]),n("li",null,[a(e,{to:"#_4-2-2-内存传输"},{default:t(()=>[s("4.2.2 内存传输")]),_:1})]),n("li",null,[a(e,{to:"#_4-2-3-固定内存"},{default:t(()=>[s("4.2.3 固定内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-2-4-零拷贝内存"},{default:t(()=>[s("4.2.4 零拷贝内存")]),_:1})]),n("li",null,[a(e,{to:"#_4-2-5-统一虚拟寻址"},{default:t(()=>[s("4.2.5 统一虚拟寻址")]),_:1})]),n("li",null,[a(e,{to:"#_4-2-6-统一内存寻址"},{default:t(()=>[s("4.2.6 统一内存寻址")]),_:1})])])]),n("li",null,[a(e,{to:"#_4-3-内存访问模式"},{default:t(()=>[s("4.3 内存访问模式")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_4-3-1-对齐与合并访问"},{default:t(()=>[s("4.3.1 对齐与合并访问")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-2-全局内存读取"},{default:t(()=>[s("4.3.2 全局内存读取")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-2-1-缓存加载"},{default:t(()=>[s("4.3.2.1 缓存加载")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-2-2-没有缓存的加载"},{default:t(()=>[s("4.3.2.2 没有缓存的加载")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-2-3-非对齐读取的示例"},{default:t(()=>[s("4.3.2.3 非对齐读取的示例")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-2-4-只读缓存"},{default:t(()=>[s("4.3.2.4 只读缓存")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-3-全局内存写入"},{default:t(()=>[s("4.3.3 全局内存写入")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-4-结构体数组与数组结构体"},{default:t(()=>[s("4.3.4 结构体数组与数组结构体")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-4-1-示例-使用aos数据布局的简单数学运算"},{default:t(()=>[s("4.3.4.1 示例：使用AoS数据布局的简单数学运算")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-4-2-示例-使用soa数据布局的简单数学运算"},{default:t(()=>[s("4.3.4.2 示例：使用SoA数据布局的简单数学运算")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-5-性能调整"},{default:t(()=>[s("4.3.5 性能调整")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-5-1-展开技术"},{default:t(()=>[s("4.3.5.1 展开技术")]),_:1})]),n("li",null,[a(e,{to:"#_4-3-5-2-增大并行性"},{default:t(()=>[s("4.3.5.2 增大并行性")]),_:1})])])]),n("li",null,[a(e,{to:"#_4-4-核函数可达到的带宽"},{default:t(()=>[s("4.4 核函数可达到的带宽")]),_:1}),n("ul",null,[n("li",null,[a(e,{to:"#_4-4-1-内存带宽"},{default:t(()=>[s("4.4.1 内存带宽")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-矩阵转置问题"},{default:t(()=>[s("4.4.2 矩阵转置问题")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-1-为转置核函数设置性能的上限和下限"},{default:t(()=>[s("4.4.2.1 为转置核函数设置性能的上限和下限")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-2-朴素转置-读取行与读取列"},{default:t(()=>[s("4.4.2.2 朴素转置：读取行与读取列")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-3-展开转置-读取行与读取列"},{default:t(()=>[s("4.4.2.3 展开转置：读取行与读取列")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-4-对角转置-读取行与读取列"},{default:t(()=>[s("4.4.2.4 对角转置：读取行与读取列")]),_:1})]),n("li",null,[a(e,{to:"#_4-4-2-5-使用瘦块来增加并行性"},{default:t(()=>[s("4.4.2.5 使用瘦块来增加并行性")]),_:1})])])]),n("li",null,[a(e,{to:"#_4-5-使用统一内存的矩阵加法"},{default:t(()=>[s("4.5 使用统一内存的矩阵加法")]),_:1})]),n("li",null,[a(e,{to:"#_4-6-总结"},{default:t(()=>[s("4.6 总结")]),_:1})]),n("li",null,[a(e,{to:"#_4-7-习题"},{default:t(()=>[s("4.7 习题")]),_:1})])])]),cn,n("details",ln,[un,a(i,{id:"466",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[rn]),tab1:t(({value:o,isActive:p})=>[kn]),_:1})]),dn,n("details",bn,[mn,a(i,{id:"580",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[vn]),tab1:t(({value:o,isActive:p})=>[fn]),_:1})]),gn,n("ul",null,[yn,n("li",null,[s("你可以试着在文件memTransfer.cu中用固定主机内存替换可分页内存，或者你可以从"),_n,n("a",hn,[s("Wrox.com中下载文件pinMemTransfer.cu"),a(c)]),s("。")]),n("li",null,[s("使用nvcc编译代码并通过nvprof运行："),xn,s(" $ nvcc -O3 "),n("a",wn,[s("pinMemTransfer.cu"),a(c)]),s(" -o pinMemTransfer"),An,s(" $ nvprof ./pinMemTransfer")]),Dn]),Mn,n("ul",null,[Sn,n("li",null,[n("p",null,[s("代码清单4-3 用零拷贝内存加总数组（"),n("a",Cn,[s("sumArrayZerocopy.cu"),a(c)]),s("）（仅列出主函数）")])])]),n("details",Un,[Pn,a(i,{id:"756",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[Bn]),tab1:t(({value:o,isActive:p})=>[zn]),_:1})]),In,n("ul",null,[n("li",null,[n("p",null,[s("注意，从cudaHostAlloc函数返回的指针被直接传递给核函数。使用UVA更新的代码可"),En,s(" 以在wrox.com上的sumArrayZerocpyUVA.cu中找到。可以用以下命令进行编译："),Rn,s(" $ nvcc -O3 -arch=sm_20 "),n("a",Gn,[s("sumArrayZerocpyUVA.cu"),a(c)]),s(" -o sumArrayZerocpyUVA")])]),Tn]),Hn,n("ul",null,[$n,n("li",null,[n("p",null,[s("代码清单4-4 使用偏移量读取内存（"),n("a",On,[s("readSegment.cu"),a(c)]),s("）（仅列出主函数）")])])]),n("details",Fn,[Nn,a(i,{id:"1173",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[Kn]),tab1:t(({value:o,isActive:p})=>[Vn]),_:1})]),Zn,n("ul",null,[qn,n("li",null,[s("使用下列指令编译代码并运行。"),Ln,s(" $ nvcc -O3 -arch=sm_20 "),n("a",Wn,[s("writeSegment.cu"),a(c)]),s(" -o writeSegment")]),Xn]),jn,n("ul",null,[Jn,n("li",null,[n("p",null,[s("代码清单4-5 使用AoS数据布局的简单数学运算（"),n("a",Qn,[s("simpleMethAoS.cu"),a(c)]),s("）（仅列出主函"),Yn,s(" 数）")])])]),n("details",ns,[ss,a(i,{id:"1423",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[as]),tab1:t(({value:o,isActive:p})=>[ts]),_:1})]),es,n("ul",null,[n("li",null,[n("p",null,[s("SoA测试的主函数与之前的simpleMathAoS.cu示例中的主函数非常相似。你可以从"),os,s(" Wrox.com中的simpleMathSoA.cu下载完整的示例代码。用以下指令编译该文件并进行测"),ps,s(" 试。"),cs,s(" $ nvcc -O3 -arch=sm_20 "),n("a",ls,[s("simpleMathSoA.cu"),a(c)]),s(" -o simpleSoA"),is,s(" $ ./simpleSoA")])]),us]),rs,n("ul",null,[ks,n("li",null,[n("p",null,[s("代码清单4-6 矩阵的转置（"),n("a",ds,[s("transpose.cu"),a(c)]),s("）（只列出了只要功能）")])])]),n("details",bs,[ms,a(i,{id:"1730",data:[{id:"源代码"},{id:"编译运行"}],"tab-id":"shell"},{title0:t(({value:o,isActive:p})=>[s("源代码")]),title1:t(({value:o,isActive:p})=>[s("编译运行")]),tab0:t(({value:o,isActive:p})=>[vs]),tab1:t(({value:o,isActive:p})=>[fs]),_:1})]),gs,n("ul",null,[ys,n("li",null,[n("p",null,[s("你可以从Wrox.com中的sumMatrixGPUManaged.cu下载完整的示例代码。你也可以找到"),_s,n("a",hs,[s("其对应的sumMatrixGPUManual.cu"),a(c)]),s("，它使用相同的矩阵加法核函数但不使用托管内存。相"),xs,s(" 反，它显式地将数据拷贝到设备中或将数据拷贝出设备。这两种核函数都需预先运行一个"),ws,s(" warm-up核函数，以避免核函数启动的系统开销，并获得更准确的计时结果。使用CUDA"),As,s(" 6.0和Kepler或更新的GPU，编译这两种核函数，一个被命名为managed，另一个被命名为"),Ds,s(" manual。")])])]),Ms,n("ul",null,[n("li",null,[n("p",null,[n("a",Ss,[s("1.参考文件globalVariable.cu"),a(c)]),s("。静态声明一个大小为5的全局浮点数组，用相同的值3.14"),Cs,s(" 初始化该全局数组。修改核函数，令每个线程都用相同的线程索引更改数组中的元素值。"),Us,s(" 将该值与线程索引相乘。用5个线程调用核函数。")])]),n("li",null,[n("p",null,[n("a",Ps,[s("2.参考文件globalVariable.cu"),a(c)]),s("。使用数据传输函数cudaMemcpy（）替换下列符号拷贝函"),Bs,s(" 数：cuda-MemcpyToSymbol（）、cudaMemcpyFromSymbol（）。需要使用"),zs,s(" cudaGetSymbolAddress（）获取全局变量地址。")])]),Is,Es,n("li",null,[n("p",null,[n("a",Rs,[s("5.修改sumArrayZerocopy.cu"),a(c)]),s("，用一个偏移量访问数组A、B、C。比较启用一级缓存和"),Gs,s(" 禁用一级缓存的性能差异。如果你的GPU不支持配置一级缓存，推测预期结果。")])]),n("li",null,[n("p",null,[n("a",Ts,[s("6.修改sumArrayZerocopyUVA.cu"),a(c)]),s("，用一个偏移量访问数组A、B、C。比较启用一级缓"),Hs,s(" 存和禁用一级缓存的性能差异。如果你的GPU不支持配置一级缓存，解释一下有一级缓存"),$s,s(" 和没有一级缓存的情况下预期的结果。")])]),Os,Fs,Ns]),Ks,n("ul",null,[n("li",null,[n("p",null,[n("a",Vs,[s("10.参考文件simpleMathAoS.cu"),a(c)]),s("。将innerStruct定义为struct__align__（8）innerStruct，"),Zs,s(" 对齐到8个字节。使用nvprof比较性能，并解释使用nvprof指标的差异。")])]),qs,n("li",null,[n("p",null,[n("a",Ls,[s("12.参考文件writeSegment.cu"),a(c)]),s("。写一个新的核函数readWriteOffset代替读和写，并使用"),Ws,s(" 偏移量32，33、64、65、128和129运行该函数。比较readOffset和writeOffset的性能差异，"),Xs,s(" 并解释这种差异。")])]),js,n("li",null,[n("p",null,[n("a",Js,[s("18.参考程序sumArrayZerocpy.cu"),a(c)]),s("，使用统一内存实现数组加法。使用nvprof比较"),Qs,s(" sumArrays和sumArraysZeroCopy的性能。")])]),n("li",null,[n("p",null,[n("a",Ys,[s("19.参考程序sumMatrixGPUManaged.cu"),a(c)]),s("。如果删除warmup核函数，性能将如何变化？"),na,s(" 并用nvprof和nvvp评估其性能。")])]),n("li",null,[n("p",null,[n("a",sa,[s("20.参考程序sumMatrixGPUManaged.cu"),a(c)]),s("。移除下列的memsets会对性能有影响吗？如果"),aa,s(" 有，请用nv-prof或nvvp检验性能。")])])]),ta])}const ia=r(tn,[["render",ea],["__file","D-第四章.html.vue"]]);export{ia as default};
